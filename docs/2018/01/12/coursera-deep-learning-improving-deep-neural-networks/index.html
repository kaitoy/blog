<!DOCTYPE html>
<html lang="en-us">
    <head>
         

<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-6244473643910448",
    enable_page_level_ads: true
  });
</script>

<meta name="google-site-verification" content="9qs7VjxtSrYMqw5OElxCdKv_gnssSRi6acB2iYlZnGA" />
<meta property="og:url" content="https://www.kaitoy.xyz/2018/01/12/coursera-deep-learning-improving-deep-neural-networks/">
<meta property="og:site_name" content="To Be Decided">
<meta name="twitter:card" content="summary"></meta>
<link rel="canonical" href="https://www.kaitoy.xyz/2018/01/12/coursera-deep-learning-improving-deep-neural-networks/">



  <meta property="og:type" content="article">
  <meta property="og:title" content="CourseraのDeep Learning SpecializationのImproving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了した | To Be Decided">
  <title>CourseraのDeep Learning SpecializationのImproving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了した | To Be Decided</title>
  <meta property="og:description" content="CourseraのDeep Learning SpecializationのNeural Networks and Deep Learningコースを修了したのに続き、Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了した。










このコースは、ディープニューラルネットワークのチューニングなどについて学べる3週間のコース。
今のところ全部英語。

2018/1/5に始めて、1/12に完了。
8日間かかった。
修了したらまたCertifacateもらえた。">
  <meta name="description" content="CourseraのDeep Learning SpecializationのNeural Networks and Deep Learningコースを修了したのに続き、Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了した。










このコースは、ディープニューラルネットワークのチューニングなどについて学べる3週間のコース。
今のところ全部英語。

2018/1/5に始めて、1/12に完了。
8日間かかった。
修了したらまたCertifacateもらえた。">
  <meta property="og:image" content="https://www.kaitoy.xyz/images/deeplearning.ai.png">



        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        
        <style>

    html body {
        font-family: 'Noto Sans JP', sans-serif;
        background-color: #fefefe;
    }

    :root {
        --accent: #fa1e44;
        --border-width:  5px ;
    }

</style>


<link rel="stylesheet" href="https://www.kaitoy.xyz/css/main.css">






<script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js"></script>
<script>
  var webFontConfig = {
    google: {
      families: ['Noto Sans JP:400,700:japanese'],
      active: function() {
        sessionStorage.fonts = true;
      }
    },
    timeout: 3000
  };
  WebFont.load(webFontConfig);
</script>





<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
 






<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>








<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>
 <meta name="generator" content="Hugo 0.55.1" />
        
        
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-65248565-1"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments)};
          gtag('js', new Date());

          gtag('config', 'UA-65248565-1');
        </script>
        
    </head>

    

    <body>
         
<div id="fb-root"></div>
<script async defer crossorigin="anonymous" src="https://connect.facebook.net/ja_JP/sdk.js#xfbml=1&version=v3.2"></script>

        <nav class="navbar navbar-default navbar-fixed-top">

            <div class="container">

                <div class="navbar-header">

                    <a class="navbar-brand visible-xs" href="#">CourseraのDeep Learning SpecializationのImproving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了した</a>

                    <button class="navbar-toggle" data-target=".navbar-collapse" data-toggle="collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>

                </div>

                <div class="collapse navbar-collapse">

                    
                        <ul class="nav navbar-nav">
                            
                                <li><a href="/">Home</a></li>
                            
                                <li><a href="/post/">Posts</a></li>
                            
                                <li><a href="/tags/">Tags</a></li>
                            
                                <li><a href="/about/">About</a></li>
                            
                        </ul>
                    

                    
                        <ul class="nav navbar-nav navbar-right">
                            
                                <li class="navbar-icon"><a href="mailto:kaitoy@pcap4j.org"><i class="fa fa-envelope-o"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://github.com/kaitoy"><i class="fa fa-github"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://www.linkedin.com/in/kaito-yamada-8558b913a"><i class="fa fa-linkedin"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://www.facebook.com/yamada.kaito.90"><i class="fa fa-facebook-square"></i></a></li>
                            
                        </ul>
                    

                </div>

            </div>

        </nav>


<main>

    <div class="single-post">
        <div class="item">

    
    
    

    
    

    <h4 class="post-title"><a href="/2018/01/12/coursera-deep-learning-improving-deep-neural-networks/">CourseraのDeep Learning SpecializationのImproving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了した</a></h4>
    <h5>Fri, Jan 12, 2018</h5>
    
    <a href="https://www.kaitoy.xyz/tags/coursera"><kbd class="item-tag">coursera</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/machine-learning"><kbd class="item-tag">machine learning</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/deep-learning"><kbd class="item-tag">deep learning</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/neural-network"><kbd class="item-tag">neural network</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/tensorflow"><kbd class="item-tag">tensorflow</kbd></a>
    

</div>


        <div class="cover">
            <a href="/2018/01/12/coursera-deep-learning-improving-deep-neural-networks/">
                <img src="https://www.kaitoy.xyz/images/deeplearning.ai.png" alt="CourseraのDeep Learning SpecializationのImproving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了した">
            </a>
        </div>

        

        <br> <div class="text-justify"><p><a href="https://www.kaitoy.xyz/2018/01/05/coursera-deep-learning-neural-networks-and-deep-learning/">CourseraのDeep Learning SpecializationのNeural Networks and Deep Learningコースを修了した</a>のに続き、<a href="https://www.coursera.org/learn/deep-neural-network">Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコース</a>を修了した。</p>





<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-6244473643910448"
     data-ad-slot="1845600530"
     data-ad-format="auto"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>


<p>このコースは、ディープニューラルネットワークのチューニングなどについて学べる3週間のコース。
今のところ全部英語。</p>

<p>2018/1/5に始めて、1/12に完了。
8日間かかった。
修了したらまた<a href="https://www.coursera.org/account/accomplishments/certificate/5VS9EJJ6TJ3A">Certifacate</a>もらえた。</p>

<p>以下、3週分の内容をメモ程度に書いておく。</p>

<ul>
<li><p>1週目</p>

<p>OverfittingやUnderfittingを防ぐテクニックについて。</p>

<ul>
<li><p>動画</p>

<ul>
<li><p>データ分割</p>

<p>深層学習のモデル構築は、検討(Idea)、実装(Code)、検証(Experiment)というサイクルの繰り返し(Iterative process)。
取り組む課題、課題のドメイン、データ量、マシンの構成などにより、ハイパーパラメータは変わるので、経験をもって事前に予測することは無理なので、サイクルをどれだけ速く回せるかが鍵。</p>

<p>データは、訓練データ(Training set)、Devデータ(Development set))(a.k.a. Cross-validation set)、テストデータ(Test set)に分割する。
訓練データで学習し、Devデータでハイパーパラメータを評価し、テストデータで最終的な評価と性能見積をする。
テストデータは無くてもいい。</p>

<p>サンプル数が1万くらいだった時代は、6:2:2くらいで分割してたけど、近年は数百万とかのデータを扱い、交差検証データやテストデータの割合はもっと小さくするのがトレンド。
98:1:1など。</p>

<p>Devデータとテストデータは同じようなものを使うべき。
訓練データは必ずしも同様でなくてもいい。訓練データは沢山要るので、別のデータソースからとることもある。</p></li>

<li><p>バイアス vs バリアンス</p>

<p>でかいネットワークで正則化して大量データで学習させるのが吉。</p></li>

<li><p>正則化</p>

<p>過学習(Overfitting)を防ぐため、コスト関数を正則化(Regularization)すべし。</p>

<p>ロジスティック回帰ではL2ノルム(L2 norm)を使ったL2正則化が一般的。
L1正則化はあまり使われない。
L1正則化をすると、wがスパース行列になってモデルを圧縮できると言われることがあるが、経験上その効果はほとんどない。</p>

<p>正則化パラメータλはハイパーパラメータで、Devデータで評価する。</p>

<p>ニューラルネットワークでは正則化項にフロベニウスノルム(Frobenius norm)を使う。</p></li>

<li><p>Dropout(Inverted Dropout)</p>

<p>ランダムにノードを無効化しながら学習することで過学習を防ぐ。
画像処理の分野では、特徴量の数が多く学習データが少ない傾向があるので、ほぼ常に使われる。</p>

<p>コスト関数が計算できなくなるのが欠点。
計算する必要があるときにはDropoutを無効化する。</p></li>

<li><p>データ拡張(Data augmentation)</p>

<p>データを加工して増やせば、高バリアンス対策になる。</p></li>

<li><p>早期終了(Early stopping)</p>

<p>過学習するまえに学習を止めるテクニック。
訓練データとDevデータについてコストをプロットして、Devデータのものが上がる前に止める。</p>

<p>これは、直交化(Orthogonalization)の原則、つまり一度に一つのことを考慮すべきという原則に反していて、コストを最小化するという問題と、過学習を避けるという問題に同時に対処することになるので微妙。</p>

<p>普通は代わりにL2正則化使えばいいけど、λを最適化する手間を省きたいときには選択肢になりうる、というか実現場ではちょくちょく選択肢になる。</p></li>

<li><p>訓練データの正規化(Normalization)</p>

<p>訓練データの各特徴量について平均を0にして分散を1にすると学習が速くなる。</p>

<p>訓練データを正規化したらテストデータも正規化する。
その際、正規化パラメータは訓練データのものを使う。</p></li>

<li><p>勾配消失(Vanishing gradient)、勾配爆発(Exploding gradient)</p>

<p>ニューラルネットワークの層が深くなると、層の出力や勾配が指数関数的に大きくなったり小さくなったりして、学習が難しくなる問題。
長年ディープニューラルネットワークの発展を妨げてきた問題。</p>

<p>パラメータのランダム初期化をすると防げる。
ガウス分布で作ったパラメータに特定の値を掛けてを分散が1/n(ReLUの時は2/n)になるように調整して、活性化関数に入力する値を約1に抑える。
掛ける値は活性化関数ごとにだいたい決まっていて((e.g. Xavier Initialization)、その値をハイパーパラメータとして調整するのはそれほど優先度は高くない。</p></li>

<li><p>Gradient checking</p>

<p>順伝播・逆伝播が正確に実装できているかを、数値計算手法で概算した勾配と逆伝播で出した勾配を比べてチェックするテクニック。
計算コストが高くなるので、デバッグ時にのみ使う。</p>

<p>Dropoutしてるときには使えない。</p></li>
</ul></li>

<li><p>プログラミング課題</p>

<ul>
<li><p>初期化</p>

<p>ゼロ初期化、大きい値でのランダム初期化、He初期化(Xavier初期化っぽいやつ)を実装して性能を比べる。</p></li>

<li><p>正則化</p>

<p>正則化無し、L2正則化、Dropoutの実装と比較。</p></li>

<li><p>Gradient checking</p>

<p>Gradient checkingの実装と、その結果を利用した逆伝播のデバッグ。</p></li>
</ul></li>

<li><p>Yoshua Bengioへのインタビュー</p></li>
</ul></li>

<li><p>2週目</p>

<p>学習を速くするテクニックについて。</p>

<ul>
<li><p>動画</p>

<ul>
<li><p>ミニバッチ勾配降下法(Mini-batch gradient descent)</p>

<p>普通の勾配降下法(i.e. バッチ勾配降下法)よりかなり速いので、大規模データでよく使われるテクニック。
学習回数に対するコストのプロットはノイジーになる。</p>

<p>ミニバッチサイズというハイパーパラメータが増える。
ミニバッチサイズがmならバッチ勾配降下法、1なら確率的勾配降下法(Stochastic gradient descent)になる。</p>

<p>ミニバッチ勾配降下法と確率的勾配降下法は収束しない。</p>

<p>バッチ勾配降下法は遅すぎる。
確率的勾配降下法はベクトル化の恩恵がなくなるという欠点がある。
ので、適当なミニバッチサイズにするのがよく、それが一番速い。</p>

<p>2000個くらいのデータならバッチ勾配降下法。
それより多ければ、64～512位のミニバッチサイズがいい。
メモリ効率を考えると2の累乗数がいいし、CPU/GPUメモリサイズに乗るサイズにすべし。</p></li>

<li><p>指数加重移動平均 (EWMA: Exponentially Weighted Moving Average)</p>

<p>ノイズのあるデータから、よりスムーズなプロットを書く手法。
過去数日の平均をもとにプロットする。</p>

<p>この手法だと、最初の方のデータが不当に小さくなってしまう。
これが問題になるなら、バイアス補正(Bias correction)をかける。</p></li>

<li><p>モーメンタム(Momentum)付き勾配降下法</p>

<p>パラメータを更新するときに、勾配そのままではなく、勾配の指数加重移動平均を使う手法。
勾配降下を滑らかに速くできる。
慣性(勢い)をつけて走り抜ける感じ。</p>

<p>指数加重移動平均を計算するときのβが新たなハイパーパラメータになる。
普通0.9。この場合バイアス補正はそんなに効果ないので普通かけない。</p></li>

<li><p>RMSprop</p>

<p>パラメータを更新するときに、勾配そのままではなく、勾配の二乗平均平方根(RMS: Root Mean Square)を使う手法。
学習率を上げつつ、勾配降下を滑らかに速くできる。</p>

<p>二乗平均平方根を計算するときのβと、ゼロ除算を防ぐためのεが新たなハイパーパラメータになる。
提唱者は、<code>β=0.999</code>、<code>ε=10^-8</code>を推奨しているし、これらをチューニングすることはあまりない。</p>

<p>Couseraが広めたことで、よく使われるようになった。</p></li>

<li><p>Adam(Adaptive Moment Estimation)</p>

<p>モーメンタムとRMSpropとバイアス補正を組み合わせた最適化アルゴリズム。
これをミニバッチ勾配降下法と一緒に使えばだいたい上手くいく。</p></li>

<li><p>学習率減衰(Learning rate decay)</p>

<p>ミニバッチ勾配降下を収束させるために、学習率を徐々に小さくする手法。
エポックごとに学習率を下げる。</p>

<p>学習率αが、α0と 減衰率(Decay rate)とエポック番号から計算されるようになるので、α0と減衰率がハイパーパラメータ。</p>

<p>学習率の計算方法にはいくつかある。
指数関数的に下げたり、階段状に下げたり。</p>

<p>Andrew先生はあまり使わない手法。
学習率をよくチューニングすれば十分。</p></li>

<li><p>局所最適解(Local optima)</p>

<p>かつて、勾配が0になる点は、コストの谷、つまり局所最適解だと考えられていて、そこに嵌ることが問題だった。</p>

<p>けど、ディープニューラルネットワークでは多くは尾根的なもの。
鞍の上みたいな部分なので鞍点(Saddle point)と呼ばれる。
特徴量が沢山あるので、ちょっと動かすとどれかの勾配は負になる。</p>

<p>よって局所最適解はあまり恐れなくていい。
代わりに、鞍点の台地みたいな部分では勾配が小さいので学習効率が悪くなる。
ここを勢いよく抜けたいので、モーメンタムやRMSpropやAdamが有効になる。</p></li>
</ul></li>

<li><p>プログラミング課題</p>

<ul>
<li><p>ミニバッチ勾配降下法実装</p>

<p>訓練データをシャッフルして、ミニバッチサイズに分割して(余りは余りでミニバッチにする)、forループで回す。</p></li>

<li><p>モーメンタムとAdam実装</p>

<p>単なるミニバッチ勾配降下法とモーメンタム付きとAdamを比較。</p></li>
</ul></li>

<li><p>Yuanqing Linへのインタビュー</p>

<p>中国の国立深層学習研究所のトップ。</p></li>
</ul></li>

<li><p>2週目</p>

<p>ハイパーパラメータのチューニング方法、バッチ正規化、ソフトマックス回帰、TensorFlow。</p>

<ul>
<li><p>動画</p>

<ul>
<li><p>ハイパーパラメータのチューニング</p>

<p>一番重要なのは学習率。
次はモーメンタムのβとかミニバッチサイズとか隠れ層のノード数。
その次がレイヤ数とか学習率減衰率。</p>

<p>チューニングの際は、かつてはグリッドサーチ(Grid search)がよく使われたけど、これはハイパーパラメータが多くなるとつらい。
ランダムサーチ(Randomized search)がより効率的。</p>

<p>グリッドサーチだとあるパラメータを固定して別のパラメータを変化させるけど、変化させたパラメータがどうでもいいものだった場合、その試行がほとんど無駄になるので。</p>

<p>粗くランダムサーチして当たりをつけ、範囲を絞って細かいランダムサーチする。</p>

<p>ランダムといってもいろいろあって、ユニット数なんかは一様にランダムでいいけど、学習率なんかはlogスケールの上でランダムにしたほうがいい。</p>

<p>実運用では、計算リソースが少ない場合に採るパンダアプローチと、潤沢なリソースで複数のモデルを同時に訓練するキャビアアプローチがある。</p></li>

<li><p>バッチ正規化(Batch normalization)</p>

<p>深層学習の実用化において最も重要なアルゴリズムの一つ。
ハイパーパラメータの選定を簡単にして、ディープニューラルネットワークの訓練を簡単にする。</p>

<p>バッチ正規化では、各層の入力を正規化する。
ミニバッチごとに平均を0、分散を1に正規化した後、βとγというパラメータでそれぞれを調整する。
aよりz(i.e. 活性化関数適用前)を正規化するのが普通。</p>

<p>(ハイパーではない)パラメータとしてβとγが層ごとに増える。
これらもWとともに学習する。
βがbの役割をするので、bはいらなくなる。</p>

<p>バッチ正規化は共変量シフト(Covariate shift)という問題に対応するもの。
共変量シフトは、訓練した後で入力の分散が変わると、また訓練しなおさないといけないという問題。
ニューラルネットワークの内部では、前のほうの層のWが学習を進めるたびに変わり、その層の出力が変わる。
つまり後のほうの層への入力が変わるので、後のほうの層の学習が進みにくい。
バッチ正規化は、この後のほうの層への入力の分散を一定範囲に抑えることで、後のほうの層の学習を効率化する。</p>

<p>Dropoutと同様な論理(ノードへの依存が分散される)で正則化の効果もややある。</p>

<p>訓練が終わったら、最後のミニバッチの平均μと分散σ^2を保存しておいて、予測時に使う。
μとσ^2は訓練データ全体から再計算してもよさそうだけど、普通はやらない。</p></li>

<li><p>ソフトマックス回帰(Softmax regression)</p>

<p>ニューラルネットワークで多値分類(Multi-class classification)するアルゴリズム。
出力層(ソフトマックス層)のノード数をクラス数にして、活性化関数にソフトマックス関数を使う。
出力層の各ノードは、サンプルが各クラスに属する確率を出力する。</p></li>

<li><p>TensorFlow</p>

<p>ディープラーニングフレームワークはいろいろある: Caffe/Caffe2、CNTK、DL2J、Keras、Lasagne、mxnet、PaddlePaddle、TensorFlow、Theano、Torch。
プログラミングしやすいこと、訓練性能がいいこと、オープンであることが重要。</p></li>

<li><p>プログラミング課題</p>

<ul>
<li><p>TensorFlowの基本</p>

<p>TensorFlowでのプログラムはだいたい以下のような手順で書く。</p>

<ol>
<li>テンソル(tensor)をつくる。これはまだ評価されない。</li>
<li>テンソル間の計算式(計算グラフ)を書く。</li>
<li>テンソルを初期化する。</li>
<li>セッションを作る。</li>
<li>セッションを実行する。ここで計算が実行される。</li>
</ol>

<p>後で(セッション実行時に)値を入れたい場合はプレースホルダ(placeholder)を使う。</p></li>

<li><p>TensorFlowでのニューラルネットワーク実装</p>

<p>画像を読み込んで多クラス分類するNNを作る。
以下、今回使った関数の一部。</p>

<ul>
<li>シグモイド関数: <code>tf.sigmoid(x)</code></li>
<li>エントロピーコスト: <code>tf.nn.sigmoid_cross_entropy_with_logits(logits, labels)</code></li>
<li>One-hotエンコーディング: <code>tf.one_hot(labels, depth, axis)</code></li>
<li>最急降下法: <code>tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)</code></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></div>

        <section class="share-buttons">
          <div class="share-button">
            <div class="fb-share-button" data-href="https://www.kaitoy.xyz/2018/01/12/coursera-deep-learning-improving-deep-neural-networks/" data-layout="box_count" data-size="small"><a target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.kaitoy.xyz%2f2018%2f01%2f12%2fcoursera-deep-learning-improving-deep-neural-networks%2f&amp;src=sdkpreparse" class="fb-xfbml-parse-ignore">Share</a></div>
          </div>
          <div class="share-button">
            <a class="twitter-share-button"
              href="https://twitter.com/intent/tweet?text=Coursera%e3%81%aeDeep%20Learning%20Specialization%e3%81%aeImproving%20Deep%20Neural%20Networks%3a%20Hyperparameter%20tuning%2c%20Regularization%20and%20Optimization%e3%82%b3%e3%83%bc%e3%82%b9%e3%82%92%e4%bf%ae%e4%ba%86%e3%81%97%e3%81%9f&amp;url=https%3a%2f%2fwww.kaitoy.xyz%2f2018%2f01%2f12%2fcoursera-deep-learning-improving-deep-neural-networks%2f"
              data-size="large">
            Tweet</a>
          </div>
          <div class="share-button">
            <a href="http://b.hatena.ne.jp/entry/" class="hatena-bookmark-button" data-hatena-bookmark-layout="vertical-normal" data-hatena-bookmark-lang="ja" title="このエントリーをはてなブックマークに追加"><img src="https://b.st-hatena.com/images/v4/public/entry-button/button-only@2x.png" alt="このエントリーをはてなブックマークに追加" width="20" height="20" style="border: none;" /></a><script type="text/javascript" src="https://b.st-hatena.com/js/bookmark_button.js" charset="utf-8" async="async"></script>
          </div>
          <div class="share-button">
            <a data-pocket-label="pocket" data-pocket-count="vertical" class="pocket-btn" data-lang="en"></a>
            <script type="text/javascript">!function(d,i){if(!d.getElementById(i)){var j=d.createElement("script");j.id=i;j.src="https://widgets.getpocket.com/v1/j/btn.js?v=1";var w=d.getElementById(i);d.body.appendChild(j);}}(document,"pocket-btn-js");</script>
          </div>
        </section>
    </div>

    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <ins class="adsbygoogle"
         style="display:block"
         data-ad-client="ca-pub-6244473643910448"
         data-ad-slot="1845600530"
         data-ad-format="auto"></ins>
    <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
    </script>

    <section class="tbd-pagination">
      <div class="row">
        <div class="col-sm-6">
          <div class="prev">
            
            <a href="https://www.kaitoy.xyz/2018/01/05/coursera-deep-learning-neural-networks-and-deep-learning/">
              <i class="fa fa-angle-left"></i>
              CourseraのDeep Learning SpecializationのNeural Networks and Deep Learningコースを修了した
            </a>
            
          </div>
        </div>
        <div class="col-sm-6">
          <div class="next text-right">
            
            <a href="https://www.kaitoy.xyz/2018/01/16/coursera-deep-learning-ml-strategy/">
              CourseraのDeep Learning SpecializationのStructuring Machine Learning Projectsコースを修了した
              <i class="fa fa-angle-right"></i>
            </a>
            
          </div>
        </div>
      </div>
    </section>

    
    

    

        <h4 class="page-header">Related</h4>

        <div class="related-links">
           <div class="item">

    
    
    

    
    

    <h4 class="post-title"><a href="/2018/03/25/hello-world-to-ml-with-keras/">機械学習のHello World: MNISTの分類モデルをKerasで作ってみた</a></h4>
    <h5>Sun, Mar 25, 2018</h5>
    
    <a href="https://www.kaitoy.xyz/tags/machine-learning"><kbd class="item-tag">machine learning</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/deep-learning"><kbd class="item-tag">deep learning</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/neural-network"><kbd class="item-tag">neural network</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/tensorflow"><kbd class="item-tag">tensorflow</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/keras"><kbd class="item-tag">keras</kbd></a>
    

</div>
  <div class="item">

    
    
    

    
    

    <h4 class="post-title"><a href="/2018/03/04/open-data/">オープンデータメモ</a></h4>
    <h5>Sun, Mar 4, 2018</h5>
    
    <a href="https://www.kaitoy.xyz/tags/machine-learning"><kbd class="item-tag">machine learning</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/deep-learning"><kbd class="item-tag">deep learning</kbd></a>
    

</div>
  <div class="item">

    
    
    

    
    

    <h4 class="post-title"><a href="/2018/02/27/coursera-deep-learning-sequence-models/">CourseraのDeep Learning SpecializationのSequence Modelsコースを修了した</a></h4>
    <h5>Tue, Feb 27, 2018</h5>
    
    <a href="https://www.kaitoy.xyz/tags/coursera"><kbd class="item-tag">coursera</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/machine-learning"><kbd class="item-tag">machine learning</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/deep-learning"><kbd class="item-tag">deep learning</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/neural-network"><kbd class="item-tag">neural network</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/rnn"><kbd class="item-tag">rnn</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/nlp"><kbd class="item-tag">nlp</kbd></a>
    

</div>
  <div class="item">

    
    
    

    
    

    <h4 class="post-title"><a href="/2018/02/06/coursera-deep-learning-convolutional-neural-networks/">CourseraのDeep Learning SpecializationのConvolutional Neural Networksコースを修了した</a></h4>
    <h5>Tue, Feb 6, 2018</h5>
    
    <a href="https://www.kaitoy.xyz/tags/coursera"><kbd class="item-tag">coursera</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/machine-learning"><kbd class="item-tag">machine learning</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/deep-learning"><kbd class="item-tag">deep learning</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/neural-network"><kbd class="item-tag">neural network</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/cnn"><kbd class="item-tag">cnn</kbd></a>
    

</div>
  <div class="item">

    
    
    

    
    

    <h4 class="post-title"><a href="/2018/01/16/coursera-deep-learning-ml-strategy/">CourseraのDeep Learning SpecializationのStructuring Machine Learning Projectsコースを修了した</a></h4>
    <h5>Tue, Jan 16, 2018</h5>
    
    <a href="https://www.kaitoy.xyz/tags/coursera"><kbd class="item-tag">coursera</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/machine-learning"><kbd class="item-tag">machine learning</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/deep-learning"><kbd class="item-tag">deep learning</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/neural-network"><kbd class="item-tag">neural network</kbd></a>
    

</div>
  <div class="item">

    
    
    

    
    

    <h4 class="post-title"><a href="/2018/01/05/coursera-deep-learning-neural-networks-and-deep-learning/">CourseraのDeep Learning SpecializationのNeural Networks and Deep Learningコースを修了した</a></h4>
    <h5>Fri, Jan 5, 2018</h5>
    
    <a href="https://www.kaitoy.xyz/tags/coursera"><kbd class="item-tag">coursera</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/machine-learning"><kbd class="item-tag">machine learning</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/deep-learning"><kbd class="item-tag">deep learning</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/neural-network"><kbd class="item-tag">neural network</kbd></a>
    

</div>
  <div class="item">

    
    
    

    
    

    <h4 class="post-title"><a href="/2017/12/22/coursera-machine-learning/">CourseraのMachine Learningコースを修了した</a></h4>
    <h5>Fri, Dec 22, 2017</h5>
    
    <a href="https://www.kaitoy.xyz/tags/coursera"><kbd class="item-tag">coursera</kbd></a>
    
    <a href="https://www.kaitoy.xyz/tags/machine-learning"><kbd class="item-tag">machine learning</kbd></a>
    

</div>
 
        </div>
    

    

        <h4 class="page-header">Comments</h4>

        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "kaitoy-tobedecided" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    

</main>

        <footer>

            <p class="copyright text-muted">&copy; 2015 Kaito Yamada. Powered by <a href="https://gohugo.io">Hugo</a> and <a href="https://github.com/calintat/minimal">Minimal</a></p>

        </footer>

        <script>window.twttr = (function(d, s, id) {
          var js, fjs = d.getElementsByTagName(s)[0],
            t = window.twttr || {};
          if (d.getElementById(id)) return t;
          js = d.createElement(s);
          js.id = id;
          js.src = "https://platform.twitter.com/widgets.js";
          fjs.parentNode.insertBefore(js, fjs);

          t._e = [];
          t.ready = function(f) {
            t._e.push(f);
          };

          return t;
        }(document, "script", "twitter-wjs"));</script>

    </body>

</html>


<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on To Be Decided</title>
    <link>https://www.kaitoy.xyz/post/</link>
    <description>Recent content in Posts on To Be Decided</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© 2015 Kaito Yamada</copyright>
    <lastBuildDate>Sun, 23 Jun 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://www.kaitoy.xyz/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Kubernetesのエコシステム ー Kubernetesクラスタ構築編</title>
      <link>https://www.kaitoy.xyz/2019/06/23/k8s-ecosystem-preparing-k8s-cluster/</link>
      <pubDate>Sun, 23 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.kaitoy.xyz/2019/06/23/k8s-ecosystem-preparing-k8s-cluster/</guid>
      <description>前回に続いて、Kubernetesのエコシステムをまとめていく。
今回はKubernetesのクラスタの構築手段について書く。
   (adsbygoogle = window.adsbygoogle || []).push({});  Kubernetesのアーキテクチャ とりあえず、Kubernetesのクラスタとは何かについて書く。
Kubernetesはざっくりマスタコンポーネントとノードコンポーネントに分けられる。 これらは同一または別個のマシンにデプロイできて、HTTPS通信を介して一つのクラスタを構成する。
マスタコンポーネントをデプロイしたマシン群はコントロールプレーンと呼ばれることがある。 ノードコンポーネントをデプロイした1マシンは単にノードと呼ばれるが、ワーカノードと呼ぶことも多い。 ノードは1クラスタに複数あるのが普通で、数百ノード～数千ノードという規模の構成も可能。
Kubernetesクラスタに対してアプリのリソース(e.g. コンテナとかサービスとか永続化ボリューム)を定義してやると、Kubernetesがそれに従ってリソースを準備してアプリをデプロイしてくれる。 アプリを構成する各Podをどのノード上で動かすかはコントロールプレーンが動的に決める。
 マスタコンポーネント
Kubernetesクラスタやその上で動くアプリケーションの構成を管理するコンポーネント群で、それぞれがGo製の単一バイナリのサーバアプリケーション。 以下の4つのコンポーネントがある。(実際にはもう一つあるけど、マネージドKubernetesを提供するクラウドプロバイダが使うやつなので普通は気にしなくていい。)
 kube-apiserver
HTTPSのポートを開けて、REST API(とProtocol BuffersのAPI)でアプリのリソースを定義するためのKubernetes APIを提供するサーバ。 ユーザはkubectlというコマンドラインツールや任意のHTTPクライアントを使ってこれと話し、アプリのリソース定義を投入する。
全てのリソースがwatch operation (aka. watch list)をサポートしていて、クライアントがリソースごとの定義や状態の更新通知をリアルタイムに受けられるようになっている。 (因みにwatch APIというのもあるが、Kubernetes 1.12で非推奨になった。)
 etcd
kube-apiserverが受け取ったリソース定義、アプリケーションの状態、Kubernetesクラスタ内で発生したイベントなどの情報を永続化するデータベース。 データベースのアーキテクチャは分散キーバリューストア。 kube-apiserverだけが参照、更新する。
実はKubernetesとは別のプロジェクトのもので、etcdはetcdでKubernetesとは別にクラスタを組める。
gRPCのAPIでデータの変更をwatchできるので、Kubernetesはこれを利用してリアクティブな感じに動く。
 kube-scheduler
kube-apiserverのwatch operationを使ってPod定義の作成をwatchして、各Podをどのノードにデプロイするかを決める。 決めるときには、Podの要求リソースとか、ノードや他Podとの親和性(i.e. Affinity)定義を鑑みてくれる。 この、Podをデプロイするノードを決める処理をスケジューリングという。
 kube-controller-manager
kube-apiserverのwatch operationを使って色んなリソース定義をwatchする。 kube-controller-managerはそのプロセス内部でリソース種別毎にgoroutineでコントローラを実行し、リソース定義と実際のリソース状態を比較し、その差を埋めるように処理をするのが主な役割。
  コンポーネント間はHTTPSやgRPCで通信するので、ネットワークさえつながっていればそれぞれがどこで動いていてもいいんだけど、普通はetcd以外の3つは同一マシンで動かして、冗長化構成にするときもその単位でする。
 ノードコンポーネント
ノードのマシンやPodやコンテナを管理・監視するコンポーネント群で、それぞれがGo製の単一バイナリのサーバアプリケーション。 以下の2つのコンポーネントがある。
 kubelet
kube-apiserverのwatch operationを使ってPod定義(など?</description>
    </item>
    
    <item>
      <title>Kubernetesのエコシステム ー コンテナランタイム編</title>
      <link>https://www.kaitoy.xyz/2019/06/15/k8s-ecosystem-container-runtimes/</link>
      <pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.kaitoy.xyz/2019/06/15/k8s-ecosystem-container-runtimes/</guid>
      <description>Kubernetesを仕事で使い始めて1年たったので、これまで使ったり見聞きして気になったKubernetesまわりのエコシステムについていくつかの記事にまとめていきたい。 第一弾はコンテナランタイム編。
   (adsbygoogle = window.adsbygoogle || []).push({});  Kubernetes KubernetesはGoogleが2014年6月に発表したコンテナオーケストレーションツール。 単一または複数の物理・仮想マシンでクラスタを構築し、その上でコンテナを起動して、関連するリソース(e.g. 永続化ボリューム)とともに管理できる。 コンテナやそのリソースやそれら間の関連を宣言的に管理できるのと、拡張性が高くていろんなものと連携して一元管理できるのが特長。
Kuberの部分はUserの要領で読んで、Kuとneにアクセントをつけて、クーバーネティスみたいに読むとネイティブっぽい。
コンテナ コンテナは一言で言えば軽量な仮想環境。 軽量というのは、ハードウェアをエミュレートしたりするハイパバイザの上で動く仮想マシンに比べて起動時間やリソース消費が少ないという意味。
より詳しく言えばコンテナは、OSのカーネルの機能で論理的に隔離された環境を作り、そこでアプリケーションプロセスを動かす技術。 ホストOSとカーネルを共有するので軽量にできるわけだけど、仮想マシンと比較してホストとの分離が弱いのが欠点と言えば欠点。
中国語だと容器と書く。 容器…
Pod Kubernetes上でのワークロードの最小単位はPodと呼ばれる。 PodはKubernetes上の仮想マシンのような概念で、Podごとに論理リソースが隔離され、IPアドレスが割り当てられ、その中で一つないし複数のコンテナが動く。 Pod自体もサンドボックスコンテナという特殊なコンテナによって構成される。 隔離された論理リソース空間はサンドボックスコンテナ(pauseコンテナ)によって保持されるので、アプリケーションを動かすコンテナが再起動されたりしても論理リソース空間を維持できるという寸法。
コンテナランタイム コンテナを実行するためのプログラムはコンテナランタイムと呼ばれる。
実はKubernetes自身はコンテナを起動する機能は持ってない。 ので、kubeletというコンポーネントがコンテナランタイムのAPIやコマンドを叩いてコンテナを起動する。
kubeletとコンテナランタイムとの関係は歴史的なものもあって結構複雑。 以下、いろいろあるコンテナランタイムを概ね時系列順に紹介する。
Docker Dockerは、2013年3月に生まれたもっとも古くもっともよく知られたコンテナランタイム。 Linuxカーネルの機能であるnamespacesで論理リソースを隔離し、cgroupsでハードウェアリソースを隔離し、Overlay Filesystemでファイルシステムを隔離し、その中でプロセスを起動する。 さらにSELinuxでファイルアクセスを制限したり、seccompでシステムコールを制限したり、AppArmorでcapabilityを制限したりもできる。
Dockerと一言で言っても、実際にコンテナを実行する低レベルなコンテナランタイムと、それを使ってコンテナを起動して管理するDocker Daemon (dockerd)と、そのデーモンにリクエストを送るためのDocker Clientに分けられて、さらに時代とともに機能が細分化されていくつかのモジュールで構成されるようになっている。
初期のKubernetes(1.2まで?)はコンテナランタイムはDocker一択で、kubeletからdockerdの内部APIを呼ぶような密連携をしていたそうな。
rkt rktはCoreOS社(現Red Hat)が2014年12月に発表したコンテナランタイム。 当時はRocketと表記してロケットと読んでいたが、商標上の問題があったのかすぐにrktになった。
当時クローズドな性質だったDockerに対抗してか、rktは発表当初からその仕様をApp Container (aka. appc)として公開し、オープン感を出していた。 appcにはコンテナイメージ形式やコンテナランタイムなどの仕様が定められ、いくつかの3rdパーティ実装も生まれた。
rktはコンテナを様々な隔離レベルで起動できるのが特徴で、ホストと同じnamespaceで起動して緩く隔離したり、KVMの仮想マシン上で起動して厳密に隔離したりできる。
もう一つの特徴として、Podをネイティブサポートしていることがある。 つまりrktでは、Kubernetes無しでもコンテナはPod内で実行される。
Windows Containers Windowsにもコンテナ対応が。
2015年4月にMicrosoftからWindows Containersが発表され、2015年8月にWindows Server 2016のプレビュー版で実際に触れるようになった。 Windows ContainersはWindows上でWindowsとLinuxのコンテナを動かせる機能で、Dockerのクライアントから操作できる。 (初期はPowerShellのコマンドレットでも操作できたけど、こちらのインターフェースはすぐに廃止された。)
その後2017年1月、Windows Containersを実現するための機能を提供するサービスであるHost Compute Service (HCS)が発表された。 この頃はWindows版DockerはこのHCSにべったりな実装だった。</description>
    </item>
    
    <item>
      <title>アジャイル開発の真髄 ― DRY</title>
      <link>https://www.kaitoy.xyz/2019/04/11/essence-of-agile-dry/</link>
      <pubDate>Thu, 11 Apr 2019 23:34:53 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2019/04/11/essence-of-agile-dry/</guid>
      <description>今携わっているプロジェクトではScrumで開発していて、私自身2年ほどスクラムマスタを経験した。 うちの会社はかなり保守的で、ごく最近までウォータフォールで開発するのがあたりまえだったので、そこから文化を変え、マインドシフトし、アジャイルなプロセスに順応していくにはそれなりに苦労があった。 今でも、アジャイルに慣れていないエンジニアがアジャイルなチームに入ってくると、やはりいろいろな違いに戸惑っているように見えるし、こちらとしても期待するアウトプットがなかなか出てこなくて困ることが多い。
私はスクラムマスタというロールを任されてはいるが、どちらかと言えばテックリードやアーキテクト的な役割に期待されている気がしていて、そっちに力が入ってしまうのが実情。 そんな状況なので、コードレビューには結構時間を割いているんだけど、アジャイルなエンジニアとそうでないエンジニアが書くコードにはなんだかとても重大な差異があるような気がずっとしていた。 で、最近それを説明できるまでに考えがまとまってきたので、ここに書き残しておく。
   (adsbygoogle = window.adsbygoogle || []).push({});  ウォータフォールの問題 ウォータフォール開発は以下のような特徴がある。
 綿密な計画を事前に立てて、計画通りに開発を進めることを重視する。 機能設計 ⇒ 詳細設計 ⇒ コーディング ⇒ テストといった感じに、全体の計画を工程でフェーズ分けして、手戻りなく、一方向に進むことを重視する。 各フェーズでは全開発項目を並行して進め、ソフトウェア全体の整合性を取ることを重視する。 各フェーズで、包括的で完成されたアウトプットを作ることを重視する。アウトプットは各フェーズで詳細にレビューする。 綿密な設計ドキュメントを整備し、適時アップデートしていくことを重視する。理想的には、コーディングは設計ドキュメントをプログラミング言語に射影するだけの単純作業になる。  これはこれで、バグの少ないソフトウェアを、大人数で、非属人的に、低リスクで開発するのに向いた開発手法ではある。 しかし、設計ドキュメントの整備にかなりの時間がかかり、開発工程全体が長くなりがちなのと、動くソフトウェアが出来てくるのが開発終盤になってしまうのが欠点。 ソフトウェアがビジネスにおいて果たす役割がかなり大きくなった昨今では、ウォータフォール開発はビジネスのスピードや変化についていけない時代遅れの手法という評価になった。 代わりに台頭したのがアジャイル開発だ。
現代では欧米のソフトウェアプロジェクトの9割以上がアジャイルなプロセスを採用しているというのをHPEのカンファレンスで聞いた覚えがある。 いまだにウォータフォールを採用するのは、例えばNASAのロケット制御ソフトウェアみたいな、バグがあると数十億ドルと世界トップレベルの人材が吹っ飛ぶようなソフトウェアの開発プロジェクトだ。 (因みにNASAのコーディング規約は、再起関数禁止、静的に回数が決まらないループ禁止など、常軌を逸した厳しさ。)
アジャイル開発のDRY ウォータフォールとアジャイルの違いは大量にあるが、パッと見て分かりやすい違いは設計ドキュメントの有無であろう。 アジャイルな開発では設計ドキュメントを排除するのが基本。

ウォータフォールなプロセス: 
ウォータフォール開発では、こんな感じにいろんなドキュメントを作ることになる。 加えて、ソースコードには大量のコメントを書くのが良しとされていた。(うちの会社だけ?) また、テストフェーズでは、何をどう操作して何をどう確認するという詳細なチェックリストを作り、経験の有無にかかわらずだれでも機械的に同じテストができるようになっているのが理想とされていた。(うちの会社だけ?)
沢山設計ドキュメントを書いても、結局顧客が手にするのはソフトウェアだけ。(ユーザマニュアルは別として。) 顧客が目にする唯一の「真実 (Truth)」は、インストールされて手元で動いているソフトウェアだ。
この「真実」の挙動や構造を説明するもの、つまり「真実の情報源 (Source of Truth)」はどこにあるか。 機能設計書や詳細設計書は言うまでもなく「真実の情報源」。 ソースコードのコメントも、どのような処理をしているのかを説明するものが多いだろうから、「真実の情報源」と言える。 ソースコード自体も、「真実」の挙動を決定づけるものなので「真実の情報源」だ。 テスト設計書やチェックリストも、「何をしたとき、どうなる」ということを記述しているので、「真実の情報源」になる。 このようにウォータフォール開発では、唯一の「真実」を様々な形で何度も表現することになるので、開発工数が膨らむことになる。
アジャイル開発では、これをプロセスにおけるDRY (Don’t Repeat Yourself)と断罪する。 つまり、「真実の情報源」が複数になることを悪として、「真実の単一情報源 (Single Source of Truth)」にしなさいと言う。 これには、ソフトウェアの挙動を変更したいときに修正すべき「情報源」を少なくして、修正工数を下げつつ、「情報源」間のずれによるバグを防ぐ狙いがある。
アジャイル開発プロセスにおける真実の単一情報源 アジャイル開発プロセスにおける「真実の単一情報源」とは何か。 それは、ウォータフォール開発における「真実の情報源」から何を削減するかという話になるわけだけど、結論は明らかだ。 絶対に捨てられないものが一つだけあるので、それ以外を排除することになる。 残すのは当然ソースコード(自動テストのコードも含む)。 ソースコードが無ければソフトウェアは動き得ないので。</description>
    </item>
    
    <item>
      <title>ズンドコキヨシ with Kubernetes Operator - KubebuilderでKubernetes Operatorを作ってみた</title>
      <link>https://www.kaitoy.xyz/2019/03/08/k8s-zundoko-operator/</link>
      <pubDate>Fri, 08 Mar 2019 17:29:16 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2019/03/08/k8s-zundoko-operator/</guid>
      <description>Javaの講義、試験が「自作関数を作り記述しなさい」って問題だったから
「ズン」「ドコ」のいずれかをランダムで出力し続けて「ズン」「ズン」「ズン」「ズン」「ドコ」の配列が出たら「キ・ヨ・シ！」って出力した後終了って関数作ったら満点で単位貰ってた
&amp;mdash; てくも (@kumiromilk) 2016年3月9日 
久しぶりにズンドコしたくなったので、Kubebuilderを使って、KubernetesのOperatorとして動くZundoko Operatorを作ってみた。
   (adsbygoogle = window.adsbygoogle || []).push({});  Kubernetes Operatorとは KubernetesのOperatorというのはCoreOS社(現Red Hat)によって提唱された概念(実装パターン)で、KubernetesのAPIで登録されるKubernetesオブジェクトの内容に従って何らかの処理をするController (e.g. Deployment Controller)の一種。
Controllerが汎用的なのに対して、特定のアプリケーションに特化しているのが特徴。 アプリケーションごとの細かな設定をKubernetesオブジェクトで表現するために、KubernetesのAPIを拡張する。
APIを拡張するにはAPI Aggregationを使う方法とCustom Resource Definition (CRD)を使う方法がある。 API Aggregationは、Kubernetesオブジェクトをetcd以外で管理したり、WebSocketを使ったり、Kubernetesクラスタ外のAPIサーバを使う場合など、特殊な場合にも対応できる高度なやりかたで、大抵のユースケースではCRDで事足りる。 Operatorも普通はCRDを使う。(というかCRDを使うのがOperatorという人もいる。)
CRDとは KubernetesのAPIを簡単に拡張できる仕組みで、Kubernetesオブジェクト(リソース)を定義するKubernetesオブジェクト。
YAMLで、定義したいリソースの名前や型やバリデーションなんかを書いてkubectl applyすれば、そのリソースをKubernetesのREST APIとかkubectlで作成したり取得したりできるようになる。
Operatorの仕組み Operatorは、CRDで定義されたリソース(など)の作成、更新、削除を監視(watch)して、リソースの内容に応じた何らかの処理をするReconciliationループを回すPod。 普通、リソースはOperatorの管理対象のアプリケーションの状態を表す。 で、Operatorはリソースの内容とアプリケーションの状態が同じになるように、Reconciliationループ内でDeploymentを作ったりアプリケーションのAPIを叩いたりする。
ユーザとしては、アプリケーションの構成や設定をKubernetesのAPIで宣言的に統一的に管理できるようになって幸せになれる。
Operator作成ツール Operatorを作るツールとして以下がある。
   ツール Operator SDK Kubebuilder Metacontroller     開発元 Kubernetesコミュニティ製 CoreOS社製 GKEチーム製   GitHubスター数 1459 1009 506   開発言語 Go、Ansible、Helm Go 任意   特徴 プロジェクトテンプレート生成、ビルド、デプロイをするCLIツール。AnsibleでもOperatorを書けるのが面白い。Operator FrameworkとしてLifecycle Managerなどが提供されていたり、OperatorHub.</description>
    </item>
    
    <item>
      <title>React &#43; Reduxアプリケーションプロジェクトのテンプレートを作る ― その11: FlowからTypeScriptへ移行</title>
      <link>https://www.kaitoy.xyz/2018/11/26/creating-react-redux-app-from-scratch-11/</link>
      <pubDate>Mon, 26 Nov 2018 16:09:14 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/11/26/creating-react-redux-app-from-scratch-11/</guid>
      <description>ReactとReduxを学ぶために、開発環境というかプロジェクトテンプレートをスクラッチから作っている。 (最終的な成果はGitHubに置いた。)
前回はCode Splitting、Flow、Jest、Enzymeをセットアップした。
前回でこのシリーズを終わりにするつもりだったけど、型システムをFlowからTypeScriptに移行したのでそれについて書く。
   (adsbygoogle = window.adsbygoogle || []).push({});  TypeScript TypeScriptはMicrosoft製のAltJS。 もともとはCoffeeScriptのように言語の機能面(e.g. class構文やアロー関数)を補強しつつ、静的型付けをサポートする言語だったが、最近はECMAScriptが前者をカバーしてるので、後者を主な目的として使う人が多い。
2012年に誕生した言語で、同様に静的型付けをサポートするFlowよりも2歳ほど年上。
TypeScript vs Flow 個人的には、静的型付けだけを目的にするならAltJSである必要はなく、静的型付けだけを補完するFlowのほうが筋がいいような気がする。 TypeScriptはECMAScriptの進化に追従すべく、追加される機能や構文をサポートするためのエンハンスを繰り返しているが、そこはBabelに任せて静的型付けに注力したらいいような。
とはいえ、以下のような点を鑑み、結局TypeScriptを選択した。
 TypeScriptの方が人気  GitHubのプロジェクトのスター数はTypeScriptが4万超えでFlowが2万弱。 観測している限り、FlowからTypeScriptへ移行したというのは聞くが、逆は聞かない。 人気があるということはコミュニティやエコシステムが大きいということ。  TypeScriptがノってる  BabelやCreate React AppがTypeScriptをサポートして来ていて、なんだか時流にのっている。  Flowは型定義ファイルの管理方法が微妙  Flowはflow-typedという専用のツールを使ってファイルをダウンロードし、ダウンロードしたものをGitとかのVCSでバージョン管理するというやりかた。 TypeScriptはnpmで管理されてるので、Yarnでダウンロードもバージョン管理もできる。VCSのリポジトリに自前のコードしか入れないで済むのもいい。  TypeScriptの方が型定義ファイルが沢山提供されてる  Flowの10倍くらいある。  TypeScriptの方がエラーメッセージが分かりやすい  というのをどこかで聞いた。  Flowの方が段階的に型を導入できる、というのは昔の話  今はTypeScriptもオプションによって段階的に導入できるというのが定評。 そもそも最初から型付けするならどうでもいい。  Flowの方が厳密な型チェックしてくれる、というのも昔の話  TypeScriptが追い付いてきて、今はほぼ同程度らしい。  TypeScript+VSCodeの開発体験が最高すぎるらしい  どっちもMicrosoft製なので。  TypeScriptの方がドキュメントが充実してる TypeScriptの方が、いざというときにソースが読みやすい  TypeScriptはTypeScriptで実装されてて、FlowはOCamlで実装されてる。   参考:</description>
    </item>
    
    <item>
      <title>React &#43; Reduxアプリケーションプロジェクトのテンプレートを作る ― その10: Code Splitting、Flow、Jest、Enzyme</title>
      <link>https://www.kaitoy.xyz/2018/11/07/creating-react-redux-app-from-scratch-10/</link>
      <pubDate>Wed, 07 Nov 2018 23:41:30 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/11/07/creating-react-redux-app-from-scratch-10/</guid>
      <description>ReactとReduxを学ぶために、開発環境というかプロジェクトテンプレートをスクラッチから作っている。 (最終的な成果はGitHubに置いた。)
前回はReact Routerをセットアップした。
今回は残りの要素をまとめてかたづける。
(2018/11/21更新)
   (adsbygoogle = window.adsbygoogle || []).push({});  Code Splitting webpackでリソースをバンドルすると、一回の通信でアプリの要素全てをロードできるので効率いいような気がするけど、アプリの規模が大きくなってくるとバンドルサイズが大きくなって、初期ロード時間が長くなり、つまり初期画面の表示に時間がかかるようになってしまう。 そもそも、いつもアプリの全画面をみるとは限らないので、いつもアプリの全要素をロードするのは無駄。
そんな問題に対応する技術がCode Splitting。 バンドルを分割し、(理想的には)必要な時に必要な分だけロードする技術。
Code Splittingのやりかたはいくつかあるが、ダイナミックインポートとReact.lazyとReact Suspenseとwebpackのプリフェッチディレクティブを使ったやつを、フォントモジュールに適用してみる。
src/components/App.jsx:
-import React from &amp;#39;react&amp;#39;; +import React, { Suspense } from &amp;#39;react&amp;#39;;  import { Route, Redirect } from &amp;#39;react-router-dom&amp;#39;; import Home from &amp;#39;./Home&amp;#39;; -import Fonts from &amp;#39;../fonts&amp;#39;;  +const Fonts = React.lazy(() =&amp;gt; import(/* webpackPrefetch: true */ &amp;#39;../fonts&amp;#39;));  const App = () =&amp;gt; ( &amp;lt;div&amp;gt; &amp;lt;Route exact path=&amp;#34;/&amp;#34; render={() =&amp;gt; &amp;lt;Redirect to=&amp;#34;/home&amp;#34; /&amp;gt;} /&amp;gt; &amp;lt;Route exact path=&amp;#34;/home&amp;#34; component={Home} /&amp;gt; - &amp;lt;Fonts /&amp;gt; + &amp;lt;Suspense fallback={&amp;lt;div /&amp;gt;}&amp;gt; + &amp;lt;Fonts /&amp;gt; + &amp;lt;/Suspense&amp;gt;  &amp;lt;/div&amp;gt; ); export default App;  コード変更はこれだけ。</description>
    </item>
    
    <item>
      <title>React &#43; Reduxアプリケーションプロジェクトのテンプレートを作る ― その9: React Router</title>
      <link>https://www.kaitoy.xyz/2018/11/02/creating-react-redux-app-from-scratch-09/</link>
      <pubDate>Fri, 02 Nov 2018 13:45:56 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/11/02/creating-react-redux-app-from-scratch-09/</guid>
      <description>ReactとReduxを学ぶために、開発環境というかプロジェクトテンプレートをスクラッチから作っている。 (最終的な成果はGitHubに置いた。)
前回はRedux Sagaをセットアップした。
(2018/11/21更新)
   (adsbygoogle = window.adsbygoogle || []).push({});  フロントエンドのルーティング Webアプリケーションにおけるルーティングとは、クライアントがリクエストしたURLに対して、返すべきリソースを選択する処理。 昔はバックエンド(i.e. サーバサイド)でやってたけど、バックエンドでリソースを返すということは、ページ遷移が発生するということなので、ネイティブアプリケーションに比べてUXが落ちてしまう。
一方、ページ遷移を発生させないようにAjaxでサーバとやりとりしつつ、ちまちまDOMをいじるのは大変。 DOMをごっそり書き換えて、ページ遷移なしに画面を切り替えることはできるけど、ナイーブにやると以下のような問題がある。
 URLと画面の紐づけがなく、URLを指定して直接開けない ブラウザの進む、戻るが使えない 宣言的に書けない  こういった問題に対応するため、フロントエンドでのルーティング技術が生まれた。
フロントエンドのルーティングでは、URLが変わってもリクエストはサーバに飛ばない。 代わりに、フロントエンドフレームワークがそのURLを見て、適切な画面を選んでレンダリングする。
ハッシュベースのルーティング URLが変わってもリクエストがサーバに飛ばないとは何事か。
それを実現するやりかたは2通りある。 古くはハッシュ(#、フラグメント識別子)をつかったやり方。
例えば、http://example.com/でUIをサーブしているとすると、http://example.com/#fooとか、http://example.com/#barで別々のページの状態を表現する。 ハッシュ以降が変わってもブラウザがサーバにリクエストを投げることはないので、クライアント側でハンドリングできる。 (因みに、ハッシュを含んだURLをブラウザのアドレスバーに入れても、ハッシュを除いたURLでリクエストが送られる。この挙動の根拠となる規格はRFCなどを調べても見つからなかったけど…)
ハッシュの書き換えは、JavaScriptで以下のようにしてできる。
location.hash = newHash;  こういう処理を、例えばWeb UIのボタンをクリックしたときなんかに実行してURLを変えて、その上で画面を更新してやればいい。
そのあと、ブラウザの戻るボタンなんかを押されると書き換える前のURLにもどるわけだけど、これを検知するためにsetInterval()とかで定期的にlocation.hashを監視してたりした。
History APIによるルーティング ハッシュベースのルーティングは見るからにしょぼい。 URLのハッシュ以降しか使えないのもしょぼいし、内部の処理も泥臭い。
これが、HTML 5でHistory APIがでて変わった。 History APIはJavaScriptのAPIで、ブラウザの履歴を操作できる。
const state = { hoge: &amp;#34;hogeee&amp;#34; }; history.pushState(state, &amp;#34;&amp;#34;, &amp;#34;/foo/bar&amp;#34;);  こんな感じのを実行すると、URLが/foo/barに変わる。(が、もちろんサーバにはリクエストは飛ばない。) で、ブラウザの戻るボタンを押すと、popstateイベントが発生するので、それにイベントハンドラを登録しておけば、もとのURLに戻った時にも適時画面を書き換えられる。 popstateイベントからは、pushState()に渡したstateオブジェクトを取得できる。

ところで、ブラウザのアドレスバーに/foo/barを直打ちするとどうなるかというと、普通にWebサーバを設定しておくと、/foo/bar/index.htmlを返そうとして、無いので404エラーになっちゃう。 ので、サーバ設定では、どのURLも同じリソース(e.g. /index.</description>
    </item>
    
    <item>
      <title>React &#43; Reduxアプリケーションプロジェクトのテンプレートを作る ― その8: Redux-Saga</title>
      <link>https://www.kaitoy.xyz/2018/10/07/creating-react-redux-app-from-scratch-08/</link>
      <pubDate>Sun, 07 Oct 2018 13:26:22 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/10/07/creating-react-redux-app-from-scratch-08/</guid>
      <description>ReactとReduxを学ぶために、開発環境というかプロジェクトテンプレートをスクラッチから作っている。 (最終的な成果はGitHubに置いた。)
前回はReact Reduxをセットアップした。
(2018/11/21更新)
   (adsbygoogle = window.adsbygoogle || []).push({});  ReduxのMiddleware Redux単体では同期的なデータフローしか実装できない。 つまり、Actionを発生させたら、即座にディスパッチされ、stateが更新される。 一方、非同期なフローとは、REST APIを呼んでその結果でstateを更新するような処理。 REST API呼び出しが非同期なわけだが、これをReduxのピュアなフローのどこで実行するのかというと、Middlewareで実行する。
MiddlewareはStoreのdispatch()をラップして、Actionをトラップして副作用を含む任意の処理をするための機能。 Middlewareの仕組みについてはこの記事が分かりやすい。
Middlewareには例えば、発生したActionの内容と、それによるstateの変化をログに出力するredux-loggerがある。 デバッグに有用そうなので入れておく。
yarn add redux-logger v3.0.6が入った。
Middlewareは、ReduxのapplyMiddleware()というAPIを使って、createStore()実行時に適用できる。
src/configureStore.js:
-import { createStore } from &amp;#39;redux&amp;#39;; +import { createStore, applyMiddleware } from &amp;#39;redux&amp;#39;; +import { logger } from &amp;#39;redux-logger&amp;#39;;  import rootReducer from &amp;#39;./reducers/rootReducer&amp;#39;; export default function configureStore(initialState = {}) { + const middlewares = []; + if (process.</description>
    </item>
    
    <item>
      <title>React &#43; Reduxアプリケーションプロジェクトのテンプレートを作る ― その7: React Redux</title>
      <link>https://www.kaitoy.xyz/2018/10/01/creating-react-redux-app-from-scratch-07/</link>
      <pubDate>Mon, 01 Oct 2018 07:54:53 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/10/01/creating-react-redux-app-from-scratch-07/</guid>
      <description>ReactとReduxを学ぶために、開発環境というかプロジェクトテンプレートをスクラッチから作っている。 (最終的な成果はGitHubに置いた。)
前回はReduxをセットアップした。
(2018/11/21更新)
   (adsbygoogle = window.adsbygoogle || []).push({});  React Redux 前回はReduxをセットアップして、ActionをStoreにディスパッチしてstateを更新できるようになった。 今回はこれをReactにつなぐ。
使うのはReact Redux。
yarn add react-redux v5.1.1が入った。
Presentational Components と Container Components React Reduxの使い方を理解するには、Presentational Components と Container Components という概念を知らないといけない。 これはReactコンポーネントを役割別に分ける考え方で、それぞれ以下のような特徴をもつ。
    Presentational Components Container Components     主な役割 DOMをレンダリングする データを取得したりstateを更新したりする(Reduxとつなぐ)   Reduxとの関連 無し 有り   データの読み込み propsから読む Reduxのstateオブジェクトから読む   データの更新 propsで渡されたコールバックを呼ぶ ReduxのActionをディスパッチする   作り方 自前で書く React Reduxで生成する</description>
    </item>
    
    <item>
      <title>React &#43; Reduxアプリケーションプロジェクトのテンプレートを作る ― その6: Redux</title>
      <link>https://www.kaitoy.xyz/2018/09/26/creating-react-redux-app-from-scratch-06/</link>
      <pubDate>Wed, 26 Sep 2018 23:03:04 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/09/26/creating-react-redux-app-from-scratch-06/</guid>
      <description>ReactとReduxを学ぶために、開発環境というかプロジェクトテンプレートをスクラッチから作っている。 (最終的な成果はGitHubに置いた。)
前回はMaterial-UIをセットアップした。
(2018/11/21更新)
   (adsbygoogle = window.adsbygoogle || []).push({});  Reactの状態管理 Reactによるプログラミングをするとき、小さいUIコンポーネントをたくさん作って、それらを組み合わせてVirtual DOMツリーを作っておいて、そこにpropsをほうりこんでレンダリングする、という感じになる。 また、レンダリングした後はコンポーネントのstateをいじって状態を変化させる。
このpropsやstateの扱いをReactの状態管理という。 propsやstateを適当にアドホックに設定してると、結局jQuery使ってるのとそんなに変わらなくなって辛くなるので、Reactの開発元であるFacebookはFluxというアーキテクチャを提案している。

Fluxでは、単一の(またはドメイン毎くらいの単位の)オブジェクトでアプリケーション全体の状態(state)を表し、これをStoreに保持する。 ReactはStoreが保持するstateを受け取り、それをもとにViewをレンダリングする。 Viewに対するユーザの操作(など)はActionというオブジェクトで表現され、Dispatcherに渡され、Dispatcherに登録されたcallbackを通してstateを変化させる。
データが常に一方向に流れて見通しがよく、各コンポーネントの独立性が高いのが特徴。 各コンポーネントは、受け取ったデータをピュアに処理すればよく、リアクティブにファンクショナルに実装できる。
Redux Fluxの実装、というか発展形がRedux。
ReduxではFluxのDispatcher辺りがReducerに置き換わっている。 ReducerはActionと現在のstateから次のstateを計算する純粋関数。
また、ReduxからはViewが切り離されていて、Actionによってstateを更新する状態管理ライブラリの役割に徹している。 ReactコンポーネントのイベントハンドラからActionオブジェクトを生成したり、更新したstateをReactに渡したりするつなぎ目は、別途React Reduxというライブラリが担当する。
ReduxとReact Reduxについては、Qiitaの「たぶんこれが一番分かりやすいと思います React + Redux のフロー図解」という記事が分かりやすい。
今回はReduxを導入する。
yarn add redux Redux v4.0.1が入った。
以降、現時点で唯一のUIコンポーネントであるHOGEボタンの状態管理を実装してみる。
Action まずActionを実装する。
Actionオブジェクトはどんな形式でもいいけど、普通はFlux Standard Action(FSA)にする。 FSAは以下のプロパティを持つプレーンオブジェクト。
 type: Action種別を示す文字列定数。必須。 payload: Actionの情報を示す任意の型の値。任意。 error: Actionがエラーを表すものかを示す boolean プロパティ。エラーなら true にして、payload にエラーオブジェクトをセットする。任意。 meta: その他の情報を入れる任意の型の値。任意。  Actionのコードは、Actionのtypeに入れる値を定義するactionTypes.jsと、Action Creator(i.e. Actionオブジェクトを生成する関数)を定義するactions.jsからなり、ともにsrc/actions/に置く。</description>
    </item>
    
    <item>
      <title>React &#43; Reduxアプリケーションプロジェクトのテンプレートを作る ― その5: Material-UIとWebフォント</title>
      <link>https://www.kaitoy.xyz/2018/09/06/creating-react-redux-app-from-scratch-05/</link>
      <pubDate>Thu, 06 Sep 2018 23:33:31 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/09/06/creating-react-redux-app-from-scratch-05/</guid>
      <description>ReactとReduxを学ぶために、開発環境というかプロジェクトテンプレートをスクラッチから作っている。 (最終的な成果はGitHubに置いた。)
前回はCSS周りの処理系をセットアップした。
(2018/11/21更新)
   (adsbygoogle = window.adsbygoogle || []).push({});  既成Reactコンポーネント 前回まででHTMLもCSSもReactコンポーネント単位で書けるようになったんだけど、実際、自分で1からコンポーネントを書くのは、特にデザインセンスがない人にとっては辛い。 かっこいいUIコンポーネントを作りたいならデザイナーの協力が必要だけど、個人の開発などそれができない状況も多い。
という問題を抱えた人たち向けなのかはわからないが、既成のReactコンポーネントセットが色々OSSで提供されている。
 Material-UI: GoogleのマテリアルデザインのReact実装。 Semantic UI React: Semantic UIのReactバインディング。 antd: Ant DesignのReact実装。 Blueprint: 複雑でデータ量の多いUI向けに作られたReact UIツールキット。 React-Bootstrap: BootstrapのReactバインディング。現時点ではv4未対応。 Grommet: HPEによるエンタープライズレディなデザインシステム。 Office UI Fabric React: OfficeなどのMicrosoft製品に使われているReactコンポーネントセット。  
今回はこの中でも圧倒的に人気なMaterial-UIを導入する。
Material-UI Material-UIは簡単に使える。 とりあえずコアパッケージをインストールする。
yarn add @material-ui/core v3.5.1が入った。

あとはパッケージに入っている色々なコンポーネントをMaterial-UIのドキュメント見ながら使えばいいだけ。
src/components/App.jsx:
import React from &amp;#39;react&amp;#39;; import styled from &amp;#39;styled-components&amp;#39;; +import Button from &amp;#39;@material-ui/core/Button&amp;#39;;  const Wrapper = styled.</description>
    </item>
    
    <item>
      <title>React &#43; Reduxアプリケーションプロジェクトのテンプレートを作る ― その4: CSS ModulesとPostCSSとstylelintとstyled-components</title>
      <link>https://www.kaitoy.xyz/2018/08/29/creating-react-redux-app-from-scratch-04/</link>
      <pubDate>Wed, 29 Aug 2018 23:50:53 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/08/29/creating-react-redux-app-from-scratch-04/</guid>
      <description>ReactとReduxを学ぶために、開発環境というかプロジェクトテンプレートをスクラッチから作っている。 (最終的な成果はGitHubに置いた。)
前回はPrettierとESLintをセットアップした。
(2018/11/21更新)
   (adsbygoogle = window.adsbygoogle || []).push({});  CSS 前回までで作った環境で、Reactを使ってHTMLのDOMツリーを構築することができるようになったが、これは基本的にUIに表示する情報の構造しか定義しない。 UIの見た目(スタイル)を決めるのはCSSなので、それをアプリに組み込むことを考えないといけない。
組み込み方には現時点で大きく3通りある。
CSSを別途設計する 一つ目はCSSを別途設計する方法。
Reactコンポーネントからレンダリングされる要素にclassが付くようにしておいて、設計したCSSをbundle.jsとは別途読み込んでスタイルを適用することにはる。
この場合、CSSのスタイル定義はすべてグローバルなので、設計効率やメンテナンス効率を維持しつつ、各コンポーネントに意図したスタイルが適用されるようにするため、テクニックを凝らしてCSSクラスを設計する必要がある。 例えばBEM (2009年3月誕生)、OOCSS (2009年3月誕生)、SMACSS (2011年9月誕生)、FLOCSS (2014年4月誕生)など。
CSS自体は、素のCSSを書くことはあまりなく、普通はSassなどのAltCSSやPostCSSを使って書く。
さらに、stylelintでリンティングすることで、CSSの品質を上げられる。 リンティングルールは、stylelintプロジェクトから提供されているstylelint-config-recommendedかstylelint-config-standardを使えば十分。 後者がGoogleやAirbnbのCSSスタイルガイドを反映していていい感じ。
書いたCSSは、webpackのcss-loaderで読み込める。 webpackはJavaScriptのimport &#39;./App.css&#39;;みたいなコードを見つけると、css-loaderに処理を渡す。 css-loaderは、import文で指定されたCSSファイルだけでなく、@importやurl()で定義される依存関係をたどって関連するCSSを一通り読み込む。
読み込んだCSSは、webpackのstyle-loaderを使ってDOMに適用できる。 style-loaderは、読み込んだCSSを&amp;lt;style&amp;gt;タグで囲ってHTMLのヘッダに挿入してくれる。

CSSの処理にはPostCSSを使うとして、プロジェクトに以下のパッケージを追加する。 (PostCSSについてはQiitaの記事が参考になった。)
 css-loader: CSSを読み込むためのwebpackのローダ。 style-loader: CSSをDOMに追加するためのwebpackのローダ。 postcss-loader: PostCSSを実行するためのwebpackのローダ。 postcss-preset-env: CSSのエッジな機能を使うためのPostCSSプラグイン。 autoprefixer: CSSプロパティにベンダプレフィックスを追加してくれるPostCSSプラグイン。 postcss-flexbugs-fixes: Flexboxのバグを修正してくれるPostCSSプラグイン。 cssnano: CSSをミニファイしてくれるPostCSSプラグイン。 stylelint: CSSのリンタ。 stylelint-config-standard: stylelintのルール設定集。 stylelint-config-prettier: Prettierが施すコード整形とコンフリクトするルールを無効にするstylelintルール設定集。  yarn add -D css-loader style-loader postcss-loader postcss-preset-env autoprefixer postcss-flexbugs-fixes cssnano stylelint stylelint-config-standard stylelint-config-prettier</description>
    </item>
    
    <item>
      <title>React &#43; Reduxアプリケーションプロジェクトのテンプレートを作る ― その3: PrettierとESLint</title>
      <link>https://www.kaitoy.xyz/2018/08/23/creating-react-redux-app-from-scratch-03/</link>
      <pubDate>Thu, 23 Aug 2018 00:19:09 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/08/23/creating-react-redux-app-from-scratch-03/</guid>
      <description>ReactとReduxを学ぶために、開発環境というかプロジェクトテンプレートをスクラッチから作っている。 (最終的な成果はGitHubに置いた。)
前回はReactをセットアップした。
(2018/11/21更新)
   (adsbygoogle = window.adsbygoogle || []).push({});  フォーマッタとリンタ プロジェクトにフォーマッタとリンタを導入する。
フォーマッタはソースの体裁を整えるツール。 フォーマッタを使うことで体裁が統一され、ソースが読みやすくなり、品質向上につながる。
リンタはソースを静的解析して、潜在的なバグ、構造的な問題、体裁の問題を検出して警告してくれるツール。 フォーマッタは体裁だけ整えるのに対し、リンタは論理構造にも制約を課せるので、コーディングスタイルがより統一できたり、ミスをしやすい論理構造が無くなったりして、品質向上につながる。
JavaScriptのような動的型付け言語では、実行時まで顕在化しないバグを作りこみやすく、また実行時エラーの原因解析が静的型付け言語に比べて難しいので、フォーマッタとリンタでプログラム実行前に問題をできるだけ取り除いておくのが重要。 またチーム開発では、コードレビューでコーディンスタイルを見る必要がなくなり、効率化につながる。
Prettier フォーマッタにはPrettierを使う。
Prettierは2017年1月にリリースされた新しいツール。 構文解析をしてASTを構築し、そこからフォーマット済みコードを出力するので、従来のツールよりも厳密な整形(e.g. 行の最大長を考慮した整形)ができる。
また、opinionated(独断的)であることも特徴で、Prettierプロジェクトが推奨するフォーマットをほぼ強制し、設定がほとんどない。 このため導入が簡単だけど、かゆいところに手が届かないこともある。
JavaScriptの他、JSX、CSS、Markdown、GraphQLのフォーマットにも対応している。

まずプロジェクトにインストールする。
yarn add -D prettier v1.15.2が入った。

設定はprettier.config.jsというファイルを書いてプロジェクトルートに置けばいい。
prettier.config.js:
module.exports = { printWidth: 100, // 行の最大長  tabWidth: 2, // インデント長  singleQuote: true, // 文字列をシングルクオートで囲う  trailingComma: &amp;#39;all&amp;#39;, // オブジェクトのプロパティとか関数の引数を複数行で書いたときに、全行の末尾にカンマをつける };  
また、フォーマット対象外のファイルを指定するファイルである.prettierignoreをプロジェクトルートに置く。
.prettierignore:
node_modules/ dist/ node_modulesはnpmパッケージが入るディレクトリ。 実際はnode_modulesはデフォルトで無視されるから書かなくていいんだけど。</description>
    </item>
    
    <item>
      <title>React &#43; Reduxアプリケーションプロジェクトのテンプレートを作る ― その2: React</title>
      <link>https://www.kaitoy.xyz/2018/08/22/creating-react-redux-app-from-scratch-02/</link>
      <pubDate>Wed, 22 Aug 2018 08:21:28 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/08/22/creating-react-redux-app-from-scratch-02/</guid>
      <description>ReactとReduxを学ぶために、開発環境というかプロジェクトテンプレートをスクラッチから作っている。 (最終的な成果はGitHubに置いた。)
前回はNode.jsとYarnとBabelとwebpackをセットアップした。
(2018/11/21更新)
   (adsbygoogle = window.adsbygoogle || []).push({});  Reactとは 以前にも同じような事を書いたけど、改めてReactについて書く。 ちょっとコーディングの詳細にも触れながら。
ReactはViewを記述するためのライブラリで、特徴はVirtual DOMとJSX。
Virtual DOM Virtual DOMはDOMを仮想化するもので、JavaScriptからVirtual DOMでUIを記述してやると、それが実DOMに効率的に反映されるようになっている。
JSX Virtual DOMはJSXというHTMLみたいな言語で記述できる。
import React from &amp;#39;react&amp;#39;; import ReactDOM from &amp;#39;react-dom&amp;#39;; ReactDOM.render( &amp;lt;h1&amp;gt;Hello, world!&amp;lt;/h1&amp;gt;, document.getElementById(&amp;#39;root&amp;#39;) ); こんな風に書くと、idがrootであるHTML要素の中に、&amp;lt;h1&amp;gt;Hello, world!&amp;lt;/h1&amp;gt;がレンダリングされる。 上記コードの&amp;lt;h1&amp;gt;Hello, world!&amp;lt;/h1&amp;gt;の部分がJSX。
コンポーネント JSXではコンポーネントを定義して新たなタグとして使うことができるので、再利用できるコンポーネントを作って、それらを組み合わせてUIを構築することで、効率的な開発ができる。
import React from &amp;#39;react&amp;#39;; import ReactDOM from &amp;#39;react-dom&amp;#39;; // Welcomeコンポーネントの定義 function Welcome() { return &amp;lt;h1&amp;gt;Hello, World&amp;lt;/h1&amp;gt;; } // Welcomeコンポーネントのレンダリング ReactDOM.render( &amp;lt;Welcome /&amp;gt;, document.getElementById(&amp;#39;root&amp;#39;) ); 上記コードではコンポーネントをfunctionで定義しているが、アロー関数で書いても全く一緒。</description>
    </item>
    
    <item>
      <title>React &#43; Reduxアプリケーションプロジェクトのテンプレートを作る ― その1: Node.jsとYarnとBabelとwebpack</title>
      <link>https://www.kaitoy.xyz/2018/08/19/creating-react-redux-app-from-scratch-01/</link>
      <pubDate>Sun, 19 Aug 2018 15:27:19 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/08/19/creating-react-redux-app-from-scratch-01/</guid>
      <description>昔、Dojo Toolkitを使ってFlashなUIをJavaScriptに書き換えた時以来、仕事でWeb UIを触ることはなかったんだけど、最近になってWeb UIを書かなければいけなくなるような気がして再学習を始めた。
題材はReact (とRedux)。 今一番人気のフロントエンドフレームワークで、昔触ったこともあるので。
前回の記事でReactが生まれた経緯を学んだので、今回から実習に入る。
(2018/11/21更新)
   (adsbygoogle = window.adsbygoogle || []).push({});  プロジェクト作成 ちょっとCreate React Appを触ってみたけど使わないことにした。 すぐ開発始められるのはよかったんだけど、裏でなにが起こっているかわからな過ぎて肌に合わないし、使うライブラリが結構固定されちゃいそうだったし、トラブルシュート(特にライブラリのバグを踏んだ時)が大変そうだったので。
代わりに、公式で紹介されているブログ記事であるCreating a React App… From Scratch.を見ながら、スクラッチからプロジェクトを作ることにした。
環境はWindows 10 Home。
最終的な成果はGitHubに置いた。
Node.jsインストール なにはともあれNode.js。
Node.jsのバージョン管理には以前はnodist使っていたんだけど、こいつは2年ほど前に開発が止まっているので、代わりにnvm for Windowsを入れた。
nvm installで任意のバージョンのNode.jsをインストール出来て、nvm useで使うNode.jsのバージョンを切り替えられる。
今回使うNode.jsのバージョンは、現時点でLTS版の最新である8.11.4にする。
C:\&amp;gt;nvm install 8.11.4 Downloading node.js version 8.11.4 (64-bit)... Complete Creating C:\Users\kaitoy\AppData\Roaming\nvm\temp Downloading npm version 5.6.0... Complete Installing npm v5.6.0... Installation complete. If you want to use this version, type nvm use 8.</description>
    </item>
    
    <item>
      <title>Webアプリケーションフロントエンド年代記 - 2018年夏編</title>
      <link>https://www.kaitoy.xyz/2018/08/16/chronicle-of-frontend-2018/</link>
      <pubDate>Thu, 16 Aug 2018 23:44:39 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/08/16/chronicle-of-frontend-2018/</guid>
      <description>Webアプリケーションの、主にフロントエンド周りに関連する歴史をまとめた。
   (adsbygoogle = window.adsbygoogle || []).push({});  静的サイト まずは原初の話から。
1990年代前半、まだWebアプリケーションという言葉が無かった時代。 静的にHTMLファイルを配信するだけのWebサイト(静的サイト)だけがあった。 静的サイトでは、HTTPサーバーに複数のHTMLファイルを置いておいて、クライアントのHTTPリクエストのURLのパスによって配信するHTMLファイルを変える。
例えば、HTTPサーバーをhttpdで立てて、ドキュメントルートを/var/www/htmlに設定して、以下のようにファイルを配置したとする。

/var/www/html/ | +-- index.html | +-- sub/ | +-- hoge.html 
この場合、ブラウザでhttp://&amp;lt;HTTPサーバアドレス&amp;gt;/index.htmlにアクセスすれば/var/www/html/index.htmlが配信されてレンダリングされて表示される。 http://&amp;lt;HTTPサーバアドレス&amp;gt;/sub/hoge.htmlにアクセスすれば/var/www/html/sub/hoge.htmlが配信される。
古のWebサイトは、こんな感じにコンテンツごとにHTMLファイルを書いてサーバに置いておいて、その間にリンクを張って辿れるようにすることで構成されていた。
まあ今も大体そんな感じだけど。
動的HTML生成 (プログラムでHTMLを書き出す) 静的サイトだと表現できることが非常に限られるので、クライアントからのリクエストの内容をサーバが解釈し、DBの情報やなんかをもとにサーバ側でHTMLドキュメントを動的に生成し、クライアントに返す、ということをするようになった。
原始的(1990年代中盤から後半)には、プログラム中で一連のHTMLドキュメントを出力する方法がとられた。
public void doGet( HttpServletRequest request, HttpServletResponse response ) throws IOException, ServletException { response.setContentType(&amp;#34;text/html;&amp;#34;); PrintWriter out = response.getWriter(); out.println(&amp;#34;&amp;lt;html&amp;gt;&amp;#34;); out.println(&amp;#34; &amp;lt;head&amp;gt;&amp;#34;); out.println(&amp;#34; &amp;lt;title&amp;gt;Hoge&amp;lt;/title&amp;gt;&amp;#34;); out.println(&amp;#34; &amp;lt;/head&amp;gt;&amp;#34;); out.println(&amp;#34; &amp;lt;body&amp;gt;&amp;#34;); out.println(new java.util.Date()); out.println(&amp;#34; &amp;lt;/body&amp;gt;&amp;#34;); out.println(&amp;#34;&amp;lt;/html&amp;gt;&amp;#34;); } 使われた技術は、CGI (Perl)とか、Java Servletとか。 Jakarta ECSなんてのもあった。</description>
    </item>
    
    <item>
      <title>PackerでESXiにVMを自動構築</title>
      <link>https://www.kaitoy.xyz/2018/06/30/packer-esxi/</link>
      <pubDate>Sat, 30 Jun 2018 16:56:34 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/06/30/packer-esxi/</guid>
      <description>前回「Packer + Ansible on Windows 10でKubernetes 1.10のクラスタ on VirtualBoxを全自動構築」で、やったことをESXiでやっただけ。
書いたコードはGitHubに置いてある。
   (adsbygoogle = window.adsbygoogle || []).push({});  前回との違い VirtualBoxとESXiとで変えないといけない部分は、主にPackerのbuilderの定義。 前回はvirtualbox-isoだったけど、今回はvmware-isoを使う。 それに伴ってパラメータが結構違ってくる。
いっこトリッキーだったのが、cdrom_adapter_typeにideを明示的に指定しておかないと、CDロムドライブがSCSIになって、OSのインストールメディアのマウントか読み取り辺りでエラーになってしまったところ。 環境によっては指定しないでいいかも。
また、&amp;quot;vnc_disable_password&amp;quot;: &amp;quot;true&amp;quot;をbuilderに指定しておかないと、Packerが「Error handshaking with VNC: no suitable auth schemes found. server supported: []byte{0x1}」というエラーを出す。
あとは、Nested Virtualizationでやった(下記)ので、すごく遅くて、色々タイムアウトを伸ばしたりしてやる必要があった。
ESXi環境 ESXi(というかVMware vSphere Hypervisor)は、現時点での最新の6.7を使用。 自前のWindows 10 HomeのノートPC上で動くVMware Player 12で作ったVMにESXiをインストールして環境を作った。
(因みにVirtualBoxにもインストールしてみたESXi上ではVM作成できなかった。VirtualBoxは今の時点でNested Virtualization未サポートで、サポートする予定もない模様。)
Packerから操作するには、以下の設定をする必要がある。
 静的IPアドレスを設定。Packerからの接続先に指定するので。 SSH有効化。PackerがSSHで接続するので。  因みにSSHクライアントでESXiにつなぐときは、チャレンジ/レスポンス認証になる。  GuestIPHack の有効化  ESXiにSSHでログインして「esxcli system settings advanced set -o /Net/GuestIPHack -i 1」  ファイアウォール設定でVNCポート(TCP5900番台)を開ける。 これをしないとPackerが「Starting HTTP server on port xxxx」でハングする。 けどこれが一筋縄ではいかない。この記事にあるように、/etc/vmware/firewall/service.</description>
    </item>
    
    <item>
      <title>Packer &#43; Ansible on Windows 10でKubernetes 1.10のクラスタ on VirtualBoxを全自動構築</title>
      <link>https://www.kaitoy.xyz/2018/06/17/packer-k8s/</link>
      <pubDate>Sun, 17 Jun 2018 23:22:33 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/06/17/packer-k8s/</guid>
      <description>「Kubernetes 1.10のクラスタを全手動で構築するのをAnsibleで全自動化した」の続きで、さらにPackerを組み合わせて、VM作成まで自動化した話。
AnsibleをWindows(MSYS2)で動かした話でもある。
書いたPackerテンプレートはGitHubに置いた。
   (adsbygoogle = window.adsbygoogle || []).push({});  Packerとは Packerは、様々な種類のVMを構築できるツール。 VagrantとかTerraformとかを開発しているHashiCorpが開発している。
テンプレートと呼ばれるビルド定義をJSONファイルに書いて、ビルド、プロビジョニング、ポストプロセスを実行して、アーティファクトと呼ばれるビルドの成果物を生成する。
ビルドのステップでは、VMを作成して、ハードウェア構成を設定したり、OSをインストールしたりする。
以下のような環境でVMを作れる。
 VirtualBox Hyper-V VMware Workstation VMware vSphere Hypervisor Docker AWS EC2  
プロビジョニングのステップでは、ビルドで作ったVMのOS上で指定された操作を実行し、ソフトウェアのインストールなどのセットアップ処理をする。
プロビジョニングには以下のようなツールを使える。
 Shell PowerShell Ansible Chef Puppet  プロビジョニングが終わるとアーティファクト(VMイメージファイルや、AWS EC2のAMI IDとか)が出力される。

ポストプロセスのステップでは、アーティファクトを入力として何らかの処理をして、最終的なアーティファクトを生成する。
ポストプロセスでは以下のような処理を実行できる。
 アーカイブ VagrantBox生成 AWS EC2へのインポート Docker push  
PackerはGoで書かれていてビルド済みのバイナリが配布されているので、ダウンロードページから落として PATHの通ったところに置くだけでインストールできる。

今回はPacker 1.2.4のWindows版をインストールした。
Packerのテンプレート概要 Packerのテンプレートにはビルド、プロビジョニング、ポストプロセスの定義を複数かけて、複数環境のVM生成を1ファイルで定義できる。
テンプレートには以下のプロパティを書く。
 builders: ビルドの定義のリスト。 description: テンプレートの説明。 min_packer_version: Packer の最低バージョン指定。 post-processors: ポストプロセスの定義のリスト。 provisioners: プロビジョニングの定義のリスト。 variables: テンプレート内で使う変数の定義。 _comment: コメントなどを書くためのプロパティ。実際はアンダースコアで始まればなんでもいい。JSON オブジェクトのルートレベルのみで使える。  これらのうち、必須なのはbuildersだけ。</description>
    </item>
    
    <item>
      <title>Kubernetes 1.10のクラスタを全手動で構築するのをAnsibleで全自動化した</title>
      <link>https://www.kaitoy.xyz/2018/06/03/build-k8s-cluster-by-ansible/</link>
      <pubDate>Sun, 03 Jun 2018 17:14:07 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/06/03/build-k8s-cluster-by-ansible/</guid>
      <description>「Kubernetes 1.10をスクラッチから全手動で構築」、「Kubernetes 1.10のクラスタにWeave Netをデプロイする」、「Kubernetes 1.10のkubeletの起動オプションをKubelet ConfigファイルとPodSecurityPolicyで置き換える」のまとめとして、Kubernetes 1.10のクラスタを構築するAnsible Playbookを書いた。
書いたものはGitHubに置いた。
   (adsbygoogle = window.adsbygoogle || []).push({});  Ansibleとは Ansibleは、Ansible社が開発したOSSのIT自動化ツール。 Ansible社は2015年10月にRedHatが買収したので、現在はRedHatが開発している。 似たようなツールにPuppetやChefがあるが、最近はAnsibleが最も支持されている気がする。
構成管理ツールと紹介されることが多い気がするが、2014年末位からはIT自動化ツールを自称していて、構成管理は実現するユースケースの一つという位置づけになっているので、そろそろ認識を改めてあげたい。
ユースケースは以下のようなもの。
 プロビジョニング (ベアメタル、VM、クラウドインスタンス) 構成管理 アプリケーションデプロイメント CI/CD セキュリティ・コンプライアンス管理 オーケストレーション  
以下のような特徴を持つ。
 Python(とPowerShell)で作られてる。  昔はPython 2じゃないと動かなかったけど、2.2からPython 3でも動くようになった。  YAMLで書いた定義(Playbook)に従って処理を実行する。 シンプルで簡便であることを売りにしている。  多数のモジュールがビルトインされていて、様々な操作を簡潔な定義で宣言的に実行できる。  エージェントレスで、SSH(等)で対象のサーバにつないで処理を実行する。 処理を冪等にできるような仕組みが備わっていて、特にビルトインモジュールを活用すると簡単に冪等性を持たせられる。  
Pythonで書かれているのでどこでも動くかと思いきや、fcntlとかgrpやらUnix特有のモジュールを使っているため、WindowsのPythonでは動かない。
MSYS2とかWSLでは動く模様。 (Git Bashでは動かない…)

今回使ったのは最新版の2.5.3。
Ansibleインストール AnsibleはYUMとかpipとかでインストールできる。
今回はOracle Linux 7.4で動かすため、以下のようにインストールした。
 AnsibleのYUMリポジトリ追加
以下の内容を/etc/yum.repos.d/の適当な.repoファイルに書く。
[ansible] name=Ansible baseurl=http://releases.ansible.com/ansible/rpm/release/epel-7-x86_64/ gpgcheck=0 enabled=1 依存するPythonパッケージのYUMリポジトリを有効化</description>
    </item>
    
    <item>
      <title>Kubernetes 1.10のkubeletの起動オプションをKubelet ConfigファイルとPodSecurityPolicyで置き換える</title>
      <link>https://www.kaitoy.xyz/2018/05/05/kubernetes-kubelet-config-and-pod-sec-policy/</link>
      <pubDate>Sat, 05 May 2018 21:54:30 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/05/05/kubernetes-kubelet-config-and-pod-sec-policy/</guid>
      <description>「Kubernetes 1.10をスクラッチから全手動で構築」、「Kubernetes 1.10のクラスタにWeave Netをデプロイする」の続き。
kubeletの起動オプションの代わりに、Kubelet ConfigファイルとPodSecurityPolicyを使うように変更した話。
ついでにkube-proxyとkube-schedulerもConfigファイルを使うようにした。
   (adsbygoogle = window.adsbygoogle || []).push({});  Kubelet Configファイル journalctl -u kubeletすると、以下の警告が出ている。
Apr 28 15:31:39 k8s-master kubelet[1370]: Flag --address has been deprecated, This parameter should be set via the config file specified by the Kubelet&amp;#39;s - -config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information. Apr 28 15:31:40 k8s-master kubelet[1370]: Flag --pod-manifest-path has been deprecated, This parameter should be set via the config file specified by the K ubelet&amp;#39;s --config flag.</description>
    </item>
    
    <item>
      <title>Kubernetes 1.10のクラスタにWeave Netをデプロイする</title>
      <link>https://www.kaitoy.xyz/2018/05/04/kubernetes-with-weave-net/</link>
      <pubDate>Fri, 04 May 2018 11:14:33 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/05/04/kubernetes-with-weave-net/</guid>
      <description>「Kubernetes 1.10をスクラッチから全手動で構築」で、Kubernetes 1.10のクラスタに、ネットワークプロバイダとしてflannelをデプロイしたけど、flannelはNetwork Policyをサポートしていないので、代わりにWeave Netをデプロイしてみた話。
   (adsbygoogle = window.adsbygoogle || []).push({});  Weave Netにした理由 Network Policyをサポートしているネットワークプロバイダには現時点で以下のものがある。
 Calico Cilium Kube-router Romana Weave Net  このなかで、よく名前を聞くのがCalicoとWeave Net。 GitHubのスター数が圧倒的に多いのがWeave Net。 性能が比較的いいのがWeave Net。
ということでWeave Netにした。
Weave Netデプロイ 以下を参考に設定してデプロイする。
 https://www.weave.works/docs/net/latest/kubernetes/kube-addon/ https://www.weave.works/docs/net/latest/install/installing-weave/ https://github.com/weaveworks/weave/blob/master/prog/weave-kube/launch.sh  Kubernetesマニフェスト Weave NetをKubernetesクラスタにデプロイするためのマニフェストは、GitHub Releasesかhttps://cloud.weave.worksからダウンロードできる。 今回は後者にする。
https://cloud.weave.worksを使う場合、Kubernetesのバージョンなどのパラメータはクエリストリングで指定できる。 主なパラメータは以下。
 k8s-version: Kubernetesのバージョン。指定しないとlatest。 password-secret: ノード間のWeave Net通信の暗号化に使うパスワードを保持するSecret名。指定しないと平文。(参考: https://www.weave.works/docs/net/latest/tasks/manage/security-untrusted-networks/) IPALLOC_RANGE: Podに割り当てるIPアドレスの範囲。指定しないと10.32.0.0/12。 CHECKPOINT_DISABLE: Weave Netのアップデートを定期的にチェックする機能の無効化オプション。 WEAVE_MTU: MTUを指定するオプション。デフォルトで1376バイト。  
WEAVE_MTUはとりあえずデフォルトにしておいて、IPALLOC_RANGEもデフォルトにして、通信暗号化して、CHECKPOINT_DISABLEをtrueにするとすると、マニフェストは以下のようにダウンロードできる。
# curl -fsSLo weave-daemonset.</description>
    </item>
    
    <item>
      <title>Kubernetes 1.10をスクラッチから全手動で構築</title>
      <link>https://www.kaitoy.xyz/2018/04/17/kubernetes110-from-scratch/</link>
      <pubDate>Tue, 17 Apr 2018 00:31:48 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/04/17/kubernetes110-from-scratch/</guid>
      <description>Oracle Linux 7.4.0のVMでKubernetes 1.10.0のクラスタをスクラッチから全手動で作った。 参考にしたのは主に以下。
 https://nixaid.com/deploying-kubernetes-cluster-from-scratch/ https://github.com/kubernetes/kubeadm/blob/master/docs/design/design_v1.10.md https://kubernetes.io/docs/getting-started-guides/scratch/ https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/ https://ulam.io/blog/kubernetes-scratch/ https://docs.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/creating-a-linux-master  (2019/1/17追記: クラスタ全手動構築手順はKubernetes 1.13になってもほとんど変わっていない。ユニットファイルに指定するオプションが多少減ったりしたくらい。 また、ホストがRHELでもほとんど変わらない。インストールするDockerがDocker-CE(もしくはRedhatのやつ)に変わるくらいで、あとはkubeletの--cgroup-driverをsystemdにしないといけなかったかも。)    (adsbygoogle = window.adsbygoogle || []).push({});  構成  マシン: Windows 10 Homeのラップトップの上のVMware PlayerのVM  CPU: 2コア メモリ: 4GB NIF: NATのを一つ  OS: Oracle Linux 7.4.0  Minimalインストール IPアドレス: 192.168.171.200、静的割り当て ホスト名: k8s-master (hostsで解決)  Docker: Oracle Container Runtime for Docker (docker-engine) 17.06.2 Kubernetes: バージョン1.10.0  単一ノード 全コンポーネント(kubelet、kube-proxy、kube-apiserver、kube-controller-manager、kube-scheduler、etcd)をsystemdで起動 (i.e. 非コンテナ)  kubeletとkube-proxy以外は非rootユーザ kubeletは現時点でrootで動かす必要がある kube-proxyはiptableいじったりする都合上rootで動かす必要があるっぽい。  コンポーネント間通信とkubectlの通信をTLSで暗号化  TLS 1.</description>
    </item>
    
    <item>
      <title>Skaffoldを触ってみた</title>
      <link>https://www.kaitoy.xyz/2018/04/01/hello-skaffold/</link>
      <pubDate>Sun, 01 Apr 2018 09:59:43 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/04/01/hello-skaffold/</guid>
      <description>Skaffoldを試してみた話。
   (adsbygoogle = window.adsbygoogle || []).push({});  Skaffoldとは Googleが開発している、Kubernetesアプリケーションを快適に開発するためのツール。 アプリケーションのソースを監視し、変更が入ると、自動的にコンテナイメージをビルドしてKubernetesクラスタにデプロイしてくれる。
2018/3/16に発表された新しいツールで、触った感じではまだこれからといった感じだった。
Goで書かれていて、Linux、OS X、Windows用のバイナリが提供されている。
似たツールにはMicrosoftのDraftがある。
また、Gitのコミットを自動デプロイしてくれるものに、Gitkube、Jenkins X (エックス)がある。
Windows版を試す 自PCがWindowsなのでWindows版を試す。 会社で使ってるのもWindowsだし。
Skaffoldを使うには、Skaffoldの実行バイナリ、Kubernetesクラスタ、そのクラスタをコンテクストに設定したkubectl、Dockerが必要。
まずWindows版Skaffoldをインストールする。 GitHubのリリースページからWindowsバイナリをダウンロードして、skaffold.exeにリネームしてPATHの通ったところに置くだけ。 Skaffoldのバージョンは0.3.0。

Kubernetesクラスタは、Windows 10 Home上にminikube 0.22.2で作ったKubernetes 1.7.0のクラスタ。 minikubeは以前インストールしたものを使う。
minikubeを起動。
&amp;gt; minikube start --kubernetes-version v1.7.0 kubectlもminikubeと一緒にインストール済み。

Dockerについては、デーモンはminikube上のを使えばいいとして、クライアント(Docker Client)はskaffoldコマンドから実行するのでWindows上にないとだめはなず。
WindowsでDockerと言えば今ならDocker for Windowsだけど、これはWindows 10 Proじゃないと使えなかったはずなので、Docker Toolboxでクライアントをいれた。
このクライアントはデフォルトではローカルのデーモンを見てるので、minikubeのデーモンを見させる。 そのための設定はminikubeのコマンドで分かるようになっている。
&amp;gt; minikube docker-env SET DOCKER_TLS_VERIFY=1 SET DOCKER_HOST=tcp://192.168.99.100:2376 SET DOCKER_CERT_PATH=C:\Users\kaitoy\.minikube\certs SET DOCKER_API_VERSION=1.23 REM Run this command to configure your shell: REM @FOR /f &amp;#34;tokens=*&amp;#34; %i IN (&amp;#39;minikube docker-env&amp;#39;) DO @%i これに従って以下のコマンドを実行するとクライアントの設定完了。</description>
    </item>
    
    <item>
      <title>機械学習のHello World: MNISTの分類モデルをKerasで作ってみた</title>
      <link>https://www.kaitoy.xyz/2018/03/25/hello-world-to-ml-with-keras/</link>
      <pubDate>Sun, 25 Mar 2018 22:43:27 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/03/25/hello-world-to-ml-with-keras/</guid>
      <description>機械学習のHello WorldとしてよくやられるMNISTの分類モデルをKeras on TensorFlowで作ってみた話。
   (adsbygoogle = window.adsbygoogle || []).push({});  MNISTとは 手書き数字画像のラベル付きデータセット。 6万個の訓練データと1万個のテストデータからなる。 CC BY-SA 3.0で配布されているっぽい。
一つの画像は28×28ピクセルの白黒画像で、0から9のアラビア数字が書かれている。
画像とラベルがそれぞれ独特な形式でアーカイブされていて、画像一つ、ラベル一つ取り出すのも一苦労する。
Kerasとは Pythonのニューラルネットワークライブラリ。 バックエンドとしてTensorFlowかCNTKかTheanoを使う。 今回はTensorFlowを使った。
やったこと KerasのMNISTのAPIとかコードサンプルとかがあけどこれらはスルー。
MNISTのサイトにあるデータセットをダウンロードしてきて、サイトに書いてあるデータ形式の説明を見ながらサンプルを取り出すコードを書いた。 で、KerasでVGGっぽいCNNを書いて、学習させてモデルをダンプして、ダンプしたモデルをロードしてテストデータで評価するコードを書いた。 コードはGitHubに。
ネットワークアーキテクチャ 入力画像のサイズに合わせてVGGを小さくした感じのCNNを作った。
VGGは2014年に発表されたアーキテクチャで、各層に同じフィルタを使い、フィルタ数を線形増加させるシンプルな構造でありながら、性能がよく、今でもよく使われるっぽい。
VGGを図にすると以下の構造。
実際はバッチ正規化とかDropoutもやるのかも。 プーリング層は数えないで16層なので、VGG-16とも呼ばれる。 パラメータ数は1億3800万個くらいで、結構深めなアーキテクチャ。

VGG-16は244×244×3の画像を入力して1000クラスに分類するのに対し、MNISTは28×28×1を入れて10クラスに分類するので、以下のような7層版を作った。
これでパラメータ数は27万個くらい。 訓練データのサンプル数が6万個なので、パラメータ数が大分多い感じではある。
コードは以下。
inputs: Tensor = Input(shape=(IMAGE_NUM_ROWS, IMAGE_NUM_COLS, 1)) x: Tensor = Conv2D(filters=8, kernel_size=(2, 2), padding=&amp;#39;same&amp;#39;, activation=&amp;#39;relu&amp;#39;)(inputs) x = Conv2D(filters=8, kernel_size=(2, 2), padding=&amp;#39;same&amp;#39;, activation=&amp;#39;relu&amp;#39;)(x) x = MaxPooling2D(pool_size=(2, 2))(x) x = Conv2D(filters=16, kernel_size=(2, 2), padding=&amp;#39;same&amp;#39;, activation=&amp;#39;relu&amp;#39;)(x) x = Conv2D(filters=16, kernel_size=(2, 2), padding=&amp;#39;same&amp;#39;, activation=&amp;#39;relu&amp;#39;)(x) x = Conv2D(filters=16, kernel_size=(2, 2), padding=&amp;#39;same&amp;#39;, activation=&amp;#39;relu&amp;#39;)(x) x = MaxPooling2D(pool_size=(2, 2))(x) x = Flatten()(x) x = Dense(units=256, activation=&amp;#39;relu&amp;#39;)(x) x = Dense(units=256, activation=&amp;#39;relu&amp;#39;)(x) predictions: Tensor = Dense(NUM_CLASSES, activation=&amp;#39;softmax&amp;#39;)(x) model: Model = Model(inputs=inputs, outputs=predictions) model.</description>
    </item>
    
    <item>
      <title>オープンデータメモ</title>
      <link>https://www.kaitoy.xyz/2018/03/04/open-data/</link>
      <pubDate>Sun, 04 Mar 2018 16:22:46 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/03/04/open-data/</guid>
      <description>機械学習の勉強に使えそうなオープンデータのメモ。
   (adsbygoogle = window.adsbygoogle || []).push({});   テキスト  WordNet: 英語の語彙データベース。名詞、動詞、形容詞、副詞ごとに階層的にグルーピングされたDBが提供されている。ライセンスはWordNet Licenseで、著作権表示さえしておけば、目的の制限なく、使用、複製、改変、再配布を無料でできる。 日本語ワードネット: 日本語版WordNet。ライセンスはJapanese WordNetライセンスで、著作権表示さえしておけば、目的の制限なく、使用、複製、改変、再配布を無料でできる。  画像  ImageNet: WordNetの名詞の階層構造に従ってラベル付けされた1400万個以上の画像データ。バウンディングボックスも付いてる。画像はFlickrとかに上がっているもので、そこから自分で無料でダウンロードできる。非商用(研究か教育)目的ならImageNetのサイトから画像をダウンロードできる。 Open Images: 900万個の画像に数千クラスのラベルとバウンディングボックスを付けたデータ。ライセンスはCreative Commons BY 4.0。 MNIST: 手書き数字のラベル付きデータセット。訓練データとテストデータ合わせて7万個。機械学習のHello Worldに使われる。  動画  YouTube-8M: 800万個のYouTube動画を4800クラスでラベル付けしたデータ。ライセンスはCreative Commons BY 4.0。 YouTube-Bounding Boxes: 24万個のYouTube動画に23クラスのラベルと560万個のバウンディングボックスを付けたデータ。ライセンスはCreative Commons BY 4.0。 Atomic Visual Actions(AVA): 5.76万個のYouTube動画を、80種の動作についてラベル付けしたデータ。ライセンスはCreative Commons BY 4.0。  音声  Speech Commands Datase: 6.5万個の1秒音声データで、30種の言葉を数千人が発音してる。ライセンスはCreative Commons BY 4.0。 AudioSet: 200万個の10秒音声データで、527クラスでラベル付けされてる。ライセンスはCreative Commons BY 4.0。  データカタログサイト  DATA GO JP: 日本政府が公開してる公共データ集。 UCI Machine Learning Repository: 現時点で426のデータセットが配布されている。有名なアヤメのデータセットのソースはここ。  単語ベクトル  HR領域の単語ベクトル: 約9.</description>
    </item>
    
    <item>
      <title>CourseraのDeep Learning SpecializationのSequence Modelsコースを修了した</title>
      <link>https://www.kaitoy.xyz/2018/02/27/coursera-deep-learning-sequence-models/</link>
      <pubDate>Tue, 27 Feb 2018 00:49:05 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/02/27/coursera-deep-learning-sequence-models/</guid>
      <description>CourseraのDeep Learning SpecializationのConvolutional Neural Networksコースを修了したのに続き、Sequence Modelsコースを修了した。
   (adsbygoogle = window.adsbygoogle || []).push({});  このコースは、RNNの原理、代表的なアーキテクチャ、自然言語処理などについて学べる3週間のコース。 生成モデルが色々出てきて面白い。 動画は今のところ全部英語。
2018/2/6に始めて、2/27に完了。 22日間かかった。 修了したらまたCertifacateもらえた。
また、これでDeep Learning Specializationのすべてのコースを修了したので、全部まとめたCertifacateももらえた。 結局2ヶ月ほどかかり、1万円以上課金してしまった…
以下、3週分の内容をメモ程度に書いておく。
 1週目
連続データを扱うシーケンス(Sequence)モデルについて学ぶ。 RNN、LSTM、GRU、BRNN。
 動画
 再帰型ニューラルネットワーク(Recurrent Neural Network)
シーケンスモデルにはRNNなどがあって、音声認識(Speech recognition)や自然言語処理(Natural language processing)に使われる。 音楽生成(Music generation)、感情分類(Sentiment classification)、DNA解析(DNA sequence analysis)、動画行動認識(Video Activity Recognition)、固有表現抽出(Named entity recognition)なんてのも。
入力だけが連続データだったり、出力だけが連続データだったり、両方だったり。
自然言語処理では、ボキャブラリ(Vocabulary)を使って単語をone hotベクトルにして扱う。 ボキャブラリは普通5万次元くらいのベクトル。 ボキャブラリにない単語はそれ用(unknown)の次元に割り当てる。
入力や出力の次元がサンプルごとに違うので、普通のNNは使えない。 また、普通のNNだと、文のある個所から学んだ特徴を他の箇所と共有しない。 また、普通のNNだと、入力サイズが大きすぎて、パラメータが多くなりすぎる。 RNNはこうした問題を持たない。
RNNは、最初の単語xを受け取り、層で計算し、最初の出力yとアクティベーションaを出し、そのaと次のxを同じ層で受け取り、次のyとaをだす、ということを繰り返す。 xにかける重みをWax、aにかける重みをWaa、yにかける重みをWyaと呼ぶ。 あとaとyを計算するときに足すバイアスがあって、それぞれba、by。 あるxの計算をするときに、その前のxも使うので、連続データ処理に向いてる。 けど、後のxを考慮しないところが欠点。 この欠点に対処したのがBRNN(Bidirectional RNN)。
RNNのaの活性化関数にはtanhがよく使われる。 ReLUもあり。 yには二値分類だったらシグモイドだし、そうでなければソフトマックスとか。
損失関数は普通に交差エントロピーでいいけど、yがベクトルなので、その各要素について交差エントロピーを計算して、足し合わせたものが損失になる。 ここから逆伝播するんだけど、その際に連続データを過去にさかのぼるので、時をかける逆伝播(BPTT: Backpropagation through time)と呼ばれる。</description>
    </item>
    
    <item>
      <title>CourseraのDeep Learning SpecializationのConvolutional Neural Networksコースを修了した</title>
      <link>https://www.kaitoy.xyz/2018/02/06/coursera-deep-learning-convolutional-neural-networks/</link>
      <pubDate>Tue, 06 Feb 2018 00:37:11 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/02/06/coursera-deep-learning-convolutional-neural-networks/</guid>
      <description>CourseraのDeep Learning SpecializationのStructuring Machine Learning Projectsコースを修了したのに続き、Convolutional Neural Networksコースを修了した。
   (adsbygoogle = window.adsbygoogle || []).push({});  このコースは、CNNの原理、代表的なアーキテクチャ、応用などについて学べる4週間のコース。 動画は今のところ全部英語。 プログラミング課題は初のKeras。
このコースは結構難しくて、特に3週目と4週目は理解に苦しんだ。 というか理解しきれなかったような。 けどNST面白かった。
2018/1/16に始めて、2/6に完了。 22日間かかった。 修了したらまたCertifacateもらえた。
以下、4週分の内容をメモ程度に書いておく。
 1週目
畳み込みニューラルネットワーク(CNN: Convolutional neural network)の基本。
 動画
 畳み込み計算
画像認識でよく使われるNNのアーキテクチャ。
低層ではエッジを検出し、層が進むにつれて複雑な特徴を学習する。
画像を特定の行列(普通は奇数の正方行列。3×3が多い。)で畳み込むことで、特定の方向のエッジを検出できる。 この行列をフィルタ(filter)という。カーネルと呼ばれることもある。 例えば縦なら以下。
[[1, 0, -1], [1, 0, -1], [1, 0, -1]] 縦でもいろいろフィルタはあって、以下はSobelフィルタというもの。
[[1, 0, -1], [2, 0, -2], [1, 0, -1]] 以下はScharrフィルタ。
[[ 3, 0, -3], [10, 0, -10], [ 3, 0, -3]] 縦のフィルタを90度回転すると横のフィルタになる。</description>
    </item>
    
    <item>
      <title>CourseraのDeep Learning SpecializationのStructuring Machine Learning Projectsコースを修了した</title>
      <link>https://www.kaitoy.xyz/2018/01/16/coursera-deep-learning-ml-strategy/</link>
      <pubDate>Tue, 16 Jan 2018 07:56:43 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/01/16/coursera-deep-learning-ml-strategy/</guid>
      <description>CourseraのDeep Learning SpecializationのImproving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了したのに続き、Structuring Machine Learning Projectsコースを修了した。
   (adsbygoogle = window.adsbygoogle || []).push({});  このコースは、深層学習プロジェクトの進め方のコツや問題への対処方法などについて学べる2週間のコース。 今回はプログラミング課題がない。 動画は今のところ全部英語。
ちょっと動画編集ミスが多かった。 同じことを二回言ったり、無音無絵の時間があったり、マイクテストしてたり。
2018/1/13に始めて、1/15に完了。 3日間かかった。 修了したらまたCertifacateもらえた。
以下、2週分の内容をメモ程度に書いておく。
 1週目
モデルの改善をするのに、データを増やしたりハイパーパラメータを変えたり色々な手法がある。 一つを試すのに下手すると数か月とかかかるので、効率よく手法の取捨選択し、モデルを改善していくための戦略について学ぶ。
 動画
 直交化(Orthogonalization)
一つの要素で複数の制御をしようとすると難しいので、一つの制御だけするようにする。 具体的には、以下のことを別々に扱う。
 訓練データに目標の精度までフィットさせる。 devデータに目標の精度までフィットさせる。 テストデータに目標の精度までフィットさせる。 現実のデータでうまく動くようにする。  それぞれの目的について、チューニングすべき要素は別々になる。
早期終了は直行化の原則に反しているので、ほかの方法があるならそっちをやったほうがいい。
 指標(Goal)の設定
モデルの改善はイテレーティブなプロセスなので、サイクルを速く回したい。 そのため、モデルを評価する単一の数値があるといい。 F1スコアとか。平均とか
単一の指標にまとめるのがむずいときもある。 精度と速さとか。 そんなときは一つ以外の指標を足切りだけに使う。 ある閾値以上の速さが出てるもののなかで精度をくらべるなど。
 データの分け方
devデータとテストデータの分布(と評価指標)は同じ感じにしないといけない。 そのために、いったん全データをシャッフルしてから分割する。 訓練データの分布は異なってても問題ない。
訓練:テスト = 70:30とか、訓練:dev:テスト = 60:20:20とかいう比率は、1万くらいのデータなら適当。 けど100万くらいなら、98:1:1くらいが妥当。</description>
    </item>
    
    <item>
      <title>CourseraのDeep Learning SpecializationのImproving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了した</title>
      <link>https://www.kaitoy.xyz/2018/01/12/coursera-deep-learning-improving-deep-neural-networks/</link>
      <pubDate>Fri, 12 Jan 2018 23:41:57 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/01/12/coursera-deep-learning-improving-deep-neural-networks/</guid>
      <description>CourseraのDeep Learning SpecializationのNeural Networks and Deep Learningコースを修了したのに続き、Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了した。
   (adsbygoogle = window.adsbygoogle || []).push({});  このコースは、ディープニューラルネットワークのチューニングなどについて学べる3週間のコース。 今のところ全部英語。
2018/1/5に始めて、1/12に完了。 8日間かかった。 修了したらまたCertifacateもらえた。
以下、3週分の内容をメモ程度に書いておく。
 1週目
OverfittingやUnderfittingを防ぐテクニックについて。
 動画
 データ分割
深層学習のモデル構築は、検討(Idea)、実装(Code)、検証(Experiment)というサイクルの繰り返し(Iterative process)。 取り組む課題、課題のドメイン、データ量、マシンの構成などにより、ハイパーパラメータは変わるので、経験をもって事前に予測することは無理なので、サイクルをどれだけ速く回せるかが鍵。
データは、訓練データ(Training set)、Devデータ(Development set))(a.k.a. Cross-validation set)、テストデータ(Test set)に分割する。 訓練データで学習し、Devデータでハイパーパラメータを評価し、テストデータで最終的な評価と性能見積をする。 テストデータは無くてもいい。
サンプル数が1万くらいだった時代は、6:2:2くらいで分割してたけど、近年は数百万とかのデータを扱い、交差検証データやテストデータの割合はもっと小さくするのがトレンド。 98:1:1など。
Devデータとテストデータは同じようなものを使うべき。 訓練データは必ずしも同様でなくてもいい。訓練データは沢山要るので、別のデータソースからとることもある。
 バイアス vs バリアンス
でかいネットワークで正則化して大量データで学習させるのが吉。
 正則化
過学習(Overfitting)を防ぐため、コスト関数を正則化(Regularization)すべし。
ロジスティック回帰ではL2ノルム(L2 norm)を使ったL2正則化が一般的。 L1正則化はあまり使われない。 L1正則化をすると、wがスパース行列になってモデルを圧縮できると言われることがあるが、経験上その効果はほとんどない。
正則化パラメータλはハイパーパラメータで、Devデータで評価する。
ニューラルネットワークでは正則化項にフロベニウスノルム(Frobenius norm)を使う。
 Dropout(Inverted Dropout)</description>
    </item>
    
    <item>
      <title>CourseraのDeep Learning SpecializationのNeural Networks and Deep Learningコースを修了した</title>
      <link>https://www.kaitoy.xyz/2018/01/05/coursera-deep-learning-neural-networks-and-deep-learning/</link>
      <pubDate>Fri, 05 Jan 2018 15:20:23 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/01/05/coursera-deep-learning-neural-networks-and-deep-learning/</guid>
      <description>CourseraのMachine Learningコースに続いて、同じくAndrew先生によるDeep Learning Specializationを受講中。
これは深層学習の基本を学べるもので、以下の5つのコースからなる。
 Neural Networks and Deep Learning Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization Structuring Machine Learning Projects Convolutional Neural Networks Sequence Models  この内、最初のNeural Networks and Deep Learningを修了したので、記念にブログしておく。
   (adsbygoogle = window.adsbygoogle || []).push({});  Deep Learning Specializationとは Deep Learning SpecializationはCoursera Specializationのひとつ。 Coursera Specializationはサブスクリプションモデルで、つまりあるSpecializationのサブスクリプションを購入すると、受講完了するまで毎月定額の料金を支払うことになる。
Deep Learning Specializationは月$49で、5コース合わせて16週分の内容。 最初の7日間はトライアルで無料なので、この間に全部終わらせられればタダ。 無理だけど。
Deep Learning Specializationでは、PythonとTensorFlowでディープニューラルネットワーク、CNN、RNN、LSTM、Adam、Dropout、バッチ正規化、Xavier/He initializationなどを学べる。 Machine Learningコースと同じく、5分～15分くらいの動画による講義と、小テストと、プログラミング課題から構成されている。
プログラミング課題は、coursera hubという、ホステッドJupyter Notebookで解いて提出できるので楽。
Neural Networks and Deep Learningコースとは ディープニューラルネットワークの仕組みを学んで実装する4週間のコース。 また、深層学習の偉い人へのインタビューを見れる。 Machine Learningコースと被っている内容が少なくなく、かなり楽だったが、結構ペースが速いので、Machine Learningコースをやっていなかったら辛かったと思う。</description>
    </item>
    
    <item>
      <title>CourseraのMachine Learningコースを修了した</title>
      <link>https://www.kaitoy.xyz/2017/12/22/coursera-machine-learning/</link>
      <pubDate>Fri, 22 Dec 2017 10:20:44 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2017/12/22/coursera-machine-learning/</guid>
      <description>機械学習の入門教材として有名なCourseraのMachine Learningコースを修了した記念日記。
   (adsbygoogle = window.adsbygoogle || []).push({});  Courseraとは Courseraは、2012年にスタンフォード大学のコンピュータ工学科の2人の教授によって設立されたサービスで、世界トップクラスの大学の講座をオンラインで受けることができるもの。 東京大学とも提携している。
講座の一部は無料で受けることができる。
CourseraのMachine Learningコースとは Machine Learningコースは、Courseraの設立者の一人であるAndrew Ngによる、機械学習の基礎から実践まで浅く広く(?)学べる世界的に有名な講座。 Andrew先生は一時期Googleで働き、Google BrainというDeep Learningのプロジェクトをリードしていたこともある機械学習のエキスパートで、さらにスタンフォードの教授だっただけあって教え方が非常にうまくてわかりやすい。
この講座は主に、5分～15分くらいの動画による講義と、小テストと、プログラミング課題から構成されている。 1週間分の内容が、1.5時間分くらいの動画と、15分くらいでできる小テストと、2、3時間で終わるプログラミング課題で、全体で11週間分やれば修了できる。 1、10、11週目はプログラミング課題が無くてすぐ終わる一方、3～5週目辺りは結構ハード。
私は2017/10/30に始めて、2017/12/19に完了したので、ちょうど50日かかったことになる。
動画は当然英語だが、有志により英語や日本語の字幕が付けられてるので聞き取れなくても問題はあまりない。 ただ、1～4週目くらいまでは、日本語の字幕がずれている動画が少なくなく、それらは英語の字幕でみる必要がある。 1つだけ英語の字幕もダメなものがあって、それだけは字幕なしで見た。
プログラミング課題は、Octaveというオープンソースの数値解析言語で解く。 聞いたことない言語だったが、MATLABとの互換性維持を重視して開発されている言語なので、まあ覚えておいて損はない。 Octaveグラフ描画APIは、MATLABのグラフ描画APIをまねていて、MATLABのグラフ描画APIは、Pythonでよく使われるグラフ描画ライブラリであるMatplotlibがまねていて、つまりOctaveやってるとMatplotlibの勉強にもなる。

以下、11週間分の内容を、キーワードレベルで書いておく。

 1週目
 機械学習の概要
背景、歴史、活用例。 教師あり学習(Supervised learning) vs 教師なし(Unsupervised learning)。
 線形単回帰(Linear regression with one variable)
仮説関数(Hypothesis)、目的関数(Cost function)、平均二乗誤差(Mean squared error)、最小二乗法(Least squares method)、最急降下法(Gradient descent)。
 行列
行列(Matrix)とベクトル(Vector)。 行列演算。 逆行列(Inverse)、転置行列(Transpose)
  2週目
 線形重回帰(Linear regression with multiple variables)</description>
    </item>
    
    <item>
      <title>Kubernetes 1.8のアクセス制御について。あとDashboard。</title>
      <link>https://www.kaitoy.xyz/2017/10/31/retry-dashboard-on-k8s-cluster-by-kubeadm/</link>
      <pubDate>Tue, 31 Oct 2017 16:57:04 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2017/10/31/retry-dashboard-on-k8s-cluster-by-kubeadm/</guid>
      <description>「Kubernetes1.8のクラスタを構築する。kubeadmで。」で、Dashboardがうまく動かない問題が発生したんだけど、それを解決した話。
   (adsbygoogle = window.adsbygoogle || []).push({});  問題の現象 kubeadmでKubernetesクラスタを組んで、自前のアプリ(Goslings)のデプロイまではうまくできたんだけど、Dashboardをデプロイしたら動かず、Web UIにkubectl proxy経由でつないでもタイムアウトしてしまった。
対策 なんとなく、クラスタ内部での名前解決にはkube-dnsによるDNSサービスが使われているっぽいので、/etc/hostsに余計な事書いたのがいけなかったと思った。
ので、/etc/hostsからk8s-masterとk8s-nodeのエントリを削除してから、kubeadm initからやり直してみた。
結果 したらちゃんと動いた。
VMのホストでkubectl proxyして、
C:\Users\kaitoy\Desktop&amp;gt;kubectl proxy Starting to serve on 127.0.0.1:8001 http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/にブラウザでつないだらサインイン画面が表示された。

Dashboardのサインイン処理はKubernetes(というかkube-apiserver)のそれに移譲している。 Dashboardはそこで認証されたユーザでクラスタのリソースにアクセスし、情報を取得して表示する。多分。
Dashboardへのサインイン方法はいくつかあるが、それらを理解するにはKubernetesのアクセス制御について学ぶことを推奨とあったのでちょっとKubernetesのドキュメントを読んだ。
Kubernetesのアクセス制御 Kubernetesクラスタのエンドポイントはkube-apiserverであり、クラスタのリソースへのアクセス制御もkube-apiserverがやる。 クライアントとkube-apiserverとのTLSセッションが確立した後、HTTP層のデータを見てアクセス制御をするんだけど、その処理はAuthentication(認証)、Authorization(認可)、Admission(許可)の三段階からなる。
Authentication 第一段階がAuthentication。 ここでは、kube-apiserverに仕込まれたAuthenticatorモジュールがユーザ認証をする。
Kubernetesが認証するユーザには、Kubernetesが管理するService Accountと、クラスタ外部で管理される通常ユーザの二通りがある。 Service AccountはPodがkube-apiserverと話すためのユーザで、通常ユーザは主に人がkubectlとかでkube-apiserverと話すためのユーザ。(匿名で話すこともできる。) 前者はServiceAccountオブジェクトで定義されるけど、後者用のオブジェクトはない。
ServiceAccountはNamespaceと関連付き(つまりnamespace毎にユニーク)、Secretに紐づく。 Secretオブジェクトはクレデンシャルのセットを定義し、Podにマウントされる。 ServiceAccountとSecretは、ふつうは自動で作られ、Podに割り当てられる。
kube-apiserverには一つ以上のAuthenticatorモジュールを設定できて、どれかで認証できれば次の段階に進める。 認証失敗するとHTTPステータスコード401が返る。
Authenticatorモジュールには以下のようなものがある。
 クライアント証明書: X.509のディジタル証明書を使うモジュール。kube-apiserver起動時に--client-ca-fileオプションで証明書ファイルを渡してやると有効になる。証明書のCommon Nameがユーザ名になり、Organizationがグループになる。クライアント側は、その証明書と対応する秘密鍵をクレデンシャルとして指定する。 Bearer Token: 無記名トークンを使うモジュール。kube-apiserver起動時に--token-auth-fileオプションでトークン情報を渡してやると有効になる。トークン情報はCSVで、「token,user,uid,&amp;quot;group1,group2,group3&amp;quot;」という形式で書く。クライアント側は、トークン文字列をクレデンシャルとして指定する。 ベーシック認証: ユーザ名とパスワードで認証するモジュール。kube-apiserver起動時に--basic-auth-fileオプションでユーザ名とパスワードのリストを渡してやると有効になる。このリストはCSVで、「password,user,uid,&amp;quot;group1,group2,group3&amp;quot;」という形式で書く。クライアント側は、ユーザ名とパスワードをクレデンシャルとして指定する。HTTPクライアントの時はAuthorizationヘッダが使える。 Service Account Token: Service Accountを署名付きBearer Tokenで認証するモジュール。デフォルトで有効になる。  このあたり、Qiitaの「kubernetesがサポートする認証方法の全パターンを動かす」という記事をみると理解が深まる。
Authorization Authenticationをパスすると、クライアントのユーザ(とグループ)が認証され、第二段階のAuthorizationモジュールの処理に移る。 ここでは、リクエストの内容(操作対象、操作種別(メソッド)等)を見て、それがユーザに許されたものなら認可する。 何を許すかは事前にクラスタにポリシーを定義しておく。</description>
    </item>
    
    <item>
      <title>Kubernetes1.8のクラスタを構築する。kubeadmで。</title>
      <link>https://www.kaitoy.xyz/2017/10/21/build-kubernetes-cluster-by-kubeadm/</link>
      <pubDate>Sat, 21 Oct 2017 10:42:46 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2017/10/21/build-kubernetes-cluster-by-kubeadm/</guid>
      <description>「Kubernetes 1.8が出たので、Minikubeを触ってみる」でMinikubeをやったんだけど、もう一歩ステップアップすべく、kubeadmでKubernetesクラスタを組んでみた話。
   (adsbygoogle = window.adsbygoogle || []).push({});  kubeadmとは kubeadm(キューブアダム)はKubernetesに含まれるコマンドで、Kubernetesクラスタを簡単に構築するツール。 Kubernetes 1.4で追加され、Kubernetes 1.8の時点でまだベータで、本番環境には使わないでとなっている。 Qiitaの「kubeadmが何をやっているのかみてみた」という記事が、中でどんな動作をしてるかを解説していて参考になる。
コマンドの使用感からすると、DockerのSwarmモードでのクラスタ構築の容易さをKubernetesに取り込むことを目指して開発されている気がした。
ネットで見かけた評判だと、確かに簡単にクラスタ構築できて素晴らしいけど、TLSの証明書生成など、細かく制御できなくて困るところがあって、やはり本番に使えるレベルではないとのこと。
まあとにかく試してみる価値はあろう。
kubeadmインストール Kubernetesのドキュメントに従ってkubeadmをインストールする。 バージョンは最新版の1.8.1。
VM作成 kubeadmのサポートOSは、Ubuntu 16.04+、Debian 9、CentOS 7、RHEL 7、Fedora 25/26、HypriotOS v1.0.1+となっている。 慣れているCentOS 7を使うことにする。 (HypriotOSってなんだろう?)
自前のノートPCのWindows 10 x64 Home Edition上のVMware Player 12のVMにCentOS 7を入れた。 メモリは1GB以上が要件なので、味を付けて1.4GBで。 VM間で通信できることって要件があったけど、インターネット接続も必要なはずなので、NICはNATのやつで。
このVMはMasterになる。
OS設定 Kubernetesが使うポートをいろいろ開けなければいけないんだけど、めんどいのでfirewalldを無効にする。
# systemctl stop firewalld # systemctl disable firewalld Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service. Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service. 
なんとなくIPアドレスをDHCPから静的割り当てに。(192.168.171.200)
# nmcli c modify ens33 ipv4.method manual # nmcli c modify ens33 ipv4.</description>
    </item>
    
    <item>
      <title>Kubernetesのチュートリアルをやる</title>
      <link>https://www.kaitoy.xyz/2017/10/11/goslings-on-kubernetes-cont/</link>
      <pubDate>Wed, 11 Oct 2017 23:48:40 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2017/10/11/goslings-on-kubernetes-cont/</guid>
      <description>「Kubernetes 1.8が出たので、Minikubeを触ってみる」の続き。 Minikubeのセットアップまではできたので、Kubernetes Basicsというチュートリアルをやりながら、Goslingsをデプロイする。
   (adsbygoogle = window.adsbygoogle || []).push({});  Kubernetes Basics - 概要 Kubernetes Basicsは、公式のチュートリアルで、Kubernetesクラスタのオーケストレーションの基本を学ぶことができるもの。 以下の6つのモジュールからなる。
 Kubernetesクラスタを作る アプリをデプロイする アプリを調査する アプリを公開する アプリをスケールする アプリをアップデートする  チュートリアルで使うのはMinikubeだけど、自分でセットアップする必要はない。 Katacodaという、ブラウザ上でIT技術を学べるプラットフォームがあり、Kubernetes Basicsはそれを利用して、ブラウザ上のターミナルからホステッドMinikubeを操作できるようにしている。
が、前回の記事で自PC上にMinikubeをセットアップしたので、そちらを使うことにする。

Kubernetes Basics - モジュール 1: Kubernetesクラスタを作る Minikubeを起動してkubectlでクラスタの状態をみるだけのモジュール。
これは前回の記事でカバーしている。
Kubernetes Basics - モジュール 2: アプリをデプロイする アプリ(i.e. コンテナ)をデプロイするにはDeploymentオブジェクトを作る。 MasterはDeploymentのspecに従って各ノードにアプリのインスタンスをスケジューリングする。 Deploymentは、アプリが落ちたら再起動してくれる、つまりself-healingも実現する。
Deploymentオブジェクトを作るコマンドはkubectl run &amp;lt;オブジェクト名&amp;gt; --image=&amp;lt;Dockerイメージ名&amp;gt;。 Goslingsをこれでデプロイする。
Goslingsコンテナは3つの引数を受け取り、指定したポートでWebサーバを起動する。 --portオプションでそのポートをexposeするようにして、--の後にコンテナに渡す引数を記述する。
C:\Users\kaitoy&amp;gt;kubectl run goslings --image=kaitoy/goslings:latest --port 8080 -- 8080 /tmp https://github.com/kaitoy/ deployment &amp;#34;goslings&amp;#34; created C:\Users\kaitoy&amp;gt;kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE goslings 1 1 1 1 27s デプロイできた。 裏でPodも作られていて、アプリが起動されている。</description>
    </item>
    
    <item>
      <title>Kubernetes 1.8が出たので、Minikubeを触ってみる</title>
      <link>https://www.kaitoy.xyz/2017/10/10/goslings-on-kubernetes/</link>
      <pubDate>Tue, 10 Oct 2017 00:10:59 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2017/10/10/goslings-on-kubernetes/</guid>
      <description>Kubernetes1.8のリリースが話題になっていたので、ちょっと触って見たという話。 (1.8を触ったとは言っていない。)
具体的には、Kubernetes Basicsというチュートリアルをやりながら、MinikubeにGoslingsをデプロイしたんだけど、この記事ではMinikubeをセットアップしたところまで。
   (adsbygoogle = window.adsbygoogle || []).push({});  Kubernetesとは KubernetesはOSSのコンテナオーケストレーションツール。 英語だとクーバネティスみたいに発音する。 Googleが自身のコンテナ技術であるBorgの運用で培ったノウハウを活かして開発したもの。 2014年ころに開発が始まり、2015年夏にv1がリリースされたということで、かなり新しいツール。 よく比べられるものにはDockerのSwarmモードやApache Mesosがあるが、何が違うのかは調べてないので知らない。 ただ、Dockerコンテナ管理ツールとしてはKubernetesが一番勢いがある雰囲気を感じる。
(2017/10/18追記: DockerがKubernetesとの統合を発表した。KubernetesはDockerネイティブなツールになり、Dockerとともにインストールされ、Docker ComposeのConposeファイルでデプロイできるようになったりする。Kubernetesの大勝利っぽい。)
Kubernetesを使うと、複数の物理マシンからなるHAクラスタ(Kubernetesクラスタ)を構成し、その上にコンテナをデプロイして管理できる。 Kubernetesクラスタは、一組のMasterコンポーネント群(a.k.a. Kubernetes Control Plane、または単にMaster)と一つ以上のNode(昔はMinionと呼ばれてたもの)で構成される。 Nodeは、Masterの管理下でコンテナを実行する機能を備えた、一台のVMや物理マシン。 MasterはNode上で動き、クラスタを管理し、コンテナのスケジューリング、状態管理、スケーリング、アップデートなどを担う。
Kubernetesのアーキテクチャを図にすると以下の感じ。 矢印の向きとかはちょっと間違ってるかも。
ごちゃごちゃするので省いたけど、図の下部のNode内のコンポーネントは、他のNode内でも動いている。

Masterにはkube-apiserverが含まれていて、Kubernetes APIというREST APIを公開する。 このAPIを通してKubernetesオブジェクトを定義したりすることで、宣言的にコンテナの管理ができる仕組み。 ユーザは普通、kubectl(キューブシーティーエル)というコマンドでkube-apiserverとやり取りする。
KubernetesオブジェクトはMasterのetcdによって分散キーバリューストアに永続化され、そのストアをkube-controller-managerとkube-schedulerが(kube-apiserver経由で)watchしてて、変更に応じた処理をする。
kube-controller-managerは、ノードの管理や、オブジェクトのライフサイクルの管理や、コンテナのスケーリングなど、クラスタレベルの機能を実行する。 (よくわからない。)
kube-schedulerは、コンテナを実行するホストを選出し、コンテナのスケジューリングをする。

一方、各Nodeでは、kubelet(キューブレット)というMasterのエージェントになるプロセスが動く。
kubeletはkube-apiserverからの指示で、コンテナイメージを取得してコンテナを起動したり監視したり止めたりする。
kubeletがコンテナを扱うためのコンテナランタイムは、普通はDockerだけど、rktとかcri-oとかfraktiとかも使える。runcやRailCarはどうなんだろう。
コンテナはデフォルトではクラスタ内のプライベートネットワークにつながるので、そこで動いているアプリにユーザからアクセスするには、何らかの形でトラフィックを中継してやる必要がある。 これをするのがkube-proxy。 ロードバランシングもしてくれる。
Kubernetesオブジェクトとは Kubernetesオブジェクトは、Kubernetesクラスタ上で機能する構成要素を表現するもの。 オブジェクトはspecとstatusを持ち、オブジェクトに期待する状態やふるまい(spec)を定義しておくと、Kubernetesが実際の状態(status)をそれに合わせてくれる。 宣言的。
オブジェクトには以下のようなものがある。
 Pod
デプロイの最小単位。 一つ(またはリソースを共有する複数)のコンテナと、ストレージ、ネットワークなどを内包する。 一つのPodには一つのIPアドレスが付く。
kubeletはPodの定義に従ってコンテナを起動する。
因みに、etcd以外のMasterコンポーネントもPodとしてデプロイされる。
 Service
Podの論理グループ。 PodのIPアドレスは外部に公開されないので、外とのやり取りをするためにServiceがある。 kube-proxyはこいつの定義に従って働く。
Serviceには複数のEndpoint(i.e. Pod等)が紐づき、外部からのトラフィックをラウンドロビンでルーティングするので、冗長化やロードバランサ的な働きもする。 ServiceはPodを抽象化するので、Podが死んだり入れ替わったりしても外に影響が見えにくくなる。</description>
    </item>
    
    <item>
      <title>スタートアップはReactを使うべきではない (BSD &#43; patentsライセンスを考慮して) — もし、いつか大企業に買収されたいと望むなら</title>
      <link>https://www.kaitoy.xyz/2017/08/25/if-youre-a-startup-you-should-not-use-react-reflecting-on-the-bsd-patents-license/</link>
      <pubDate>Fri, 25 Aug 2017 00:29:39 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2017/08/25/if-youre-a-startup-you-should-not-use-react-reflecting-on-the-bsd-patents-license/</guid>
      <description>このエントリでは、Raúl Kripalaniによる記事、If you’re a startup, you should not use React (reflecting on the BSD + patents license)を紹介する。 (Raúlから和訳と転載の許可は得た。) 以下はその全文の和訳だが、意訳超訳が混じっているので、もとのニュアンスを知りたければ元記事を読んでもいいし、読まなくてもいい。

2017/9/23追記: React、Jest、Flow、Immutable.jsがMITにリライセンスされるというアナウンスがFacebookからあった。 コミュニティの大勝利だ。

   (adsbygoogle = window.adsbygoogle || []).push({});  現在オープンソースコミュニティで起こっていることには落胆させられる。 特に、オープンソースのおかげで多くのスタートアップやビジネスが存在することを認識したときは。 独占的なソフトウェアのために法外なライセンス料を払わなければならないとしたら、それらは存続できない。
オープンソースとは、より良いソフトウェアをみんなで構築するためのコミュニティをつくることだ。 それを、— Facebookが意図しているような — 人々の権利を交換するための市場として決して使用すべきではない。
Facebookは「BSD + patents」というライセンスモデルを推進している。 広く人気のあるReactを含む、すべてのプロジェクトで。
基本的に、「BSD + patents」はコードが(誰でも参照し利用できるように)公開されていることを意味するが、しかしそれは常にFacebookの著作物でもある。 そして彼らは、君がFacebookを特許侵害で訴えないで 仲良くやっている限り、君に特許ライセンスを与える。
Facebookを訴えた瞬間、Reactの他、君の使っているあらゆるFacebookの「オープンソース」技術の特許権は自動的に取り消されてしまう。
アディオス、バイバイ、どこかへ行ってしまう。
(https://github.com/facebook/react/blob/b8ba8c83f318b84e42933f6928f231dc0918f864/PATENTS)

この問題は、Apache Software Foundationによって衆目にさらされることとなった。
この制限は広大で、残忍だ。

・・・

その知的財産がReactを使用しているドメインと関連しているかどうかは関係ない。
君がReactを使うなら、Facebookが保持する特許に逆らうことはできない。 いつまでも。

言い換えれば、代償。

Facebook、それが君らの考えるオープンソースなのか?</description>
    </item>
    
    <item>
      <title>WebdriverIOとChromeのヘッドレスモードで自動ブラウザテストするDockerイメージ: webdriverio-chrome</title>
      <link>https://www.kaitoy.xyz/2017/08/14/webdriverio-chrome/</link>
      <pubDate>Mon, 14 Aug 2017 10:53:17 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2017/08/14/webdriverio-chrome/</guid>
      <description>「2017年夏、ブラウザテストフレームワーク」の続き。 ServiceNowアプリケーションのブラウザテストをしたくて色々調べている。 前回は、フレームワークにWebdriverIOを使うと決めたところまで書いた。
今回、最終的に、WebdriverIO、WDIO、selenium-standalone、Jasmineと、Chromeのヘッドレスモードを使って、Dockerコンテナ(Alpine Linux)上でテストスクリプトを実行して、ServiceNowのログイン画面のスクリーンショットが取れるところまでできた。
そのコンテナイメージのDockerfileはGitHubに置いた。
   (adsbygoogle = window.adsbygoogle || []).push({});  とりあえずAlpine Linux テスト環境の作成は自宅でやってるけど、DockerイメージにしてDocker Hubとかに上げておけば、社内でダウンロードしてそのまま再現できる。 ダウンロードに係る社内手続きも、Dockerイメージだけに対してやればいいので、中に何を詰め込んでも、後でライブラリとか追加しても、一回こっきりで済む。
というわけでWebdriverIO環境をDockerコンテナとしてつくることにする。 とりあえず、自PC(Windows 10 Home x64)に入ってるVMware Workstation Player 12.5.5でCentOS 7 x64のVMを作り、そこにDockerをインストールした。
次に、そのDockerを使って、WebdriverIO環境のベースにするAlpine Linuxをpullする。
$ docker pull alpine:edge 
Alpine LinuxはBusyBoxとmusl libcで構成された軽量な Linuxディストリビューション。 2016年2月にすべてのオフィシャルDockerイメージがAlpine Linuxベースになるというアナウンスがあったし、他にそれっぽいものもなかったので、これをベースに環境を作ることにした。 glibcじゃないのがちょっと気になるけど、まあ問題ないか。
現在、Chrome 59のAlpine Linuxパッケージはedgeブランチ(i.e. 開発ブランチ)でしか作られていない。 pullするタグをedgeにしたのはそのため。 (因みに現時点でAlpine Linuxのlatestは3.6。)
で、起動。
$ docker run -it alpine:edge sh Chrome(Chromium)インストール まずはChrome(がAlpine Linuxパッケージにはないので、実際にはChromium)と、ついでにChromeDriverをインストールする。 Alpine Linux独自のパッケージマネージャーであるapkを使う。
コンテナ内:
# apk add --update chromium chromium-chromedriver # chromium-browser -version Chromium 59.</description>
    </item>
    
    <item>
      <title>2017年夏、ブラウザテストフレームワーク</title>
      <link>https://www.kaitoy.xyz/2017/08/04/browser-test-framework/</link>
      <pubDate>Fri, 04 Aug 2017 15:29:37 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2017/08/04/browser-test-framework/</guid>
      <description>「2017年夏、Selenium、ヘッドレスブラウザ」の続き。 ServiceNowアプリケーションのブラウザテストをしたくて色々調べている。 前回は、Selenium(WebDriver)とChromeのヘッドレスモードを使うのがよさそうというところまで書いた。
この記事では、ブラウザテストフレームワークを選ぶ。
   (adsbygoogle = window.adsbygoogle || []).push({});  ブラウザ操作ツールとかブラウザテストフレームワークとか Seleniumを直接使って、JUnitなんかのテストフレームワークでブラウザテストを書くこともできるけど、それは結構つらい。 Seleniumは低レベルなブラウザ操作APIを提供するので、単純にテスト書き辛いし、動的サイトを扱うときには、かなり気を使ってwait処理を入れていかないとテストが安定しない。 テスト前に、WebDriverの準備をしないといけなかったりするのも面倒。
なので、昨今はもう少し高級なツールやフレームワークを使うのが普通な模様。 あまり知らなかったので色々記事を読んだ。
 Seleniumアレルギーのための処方箋 ブラウザテストツール総まとめ・2016年夏版 2017年JavaScriptのテスト概論  
結果、ブラウザ操作ツールやブラウザテストフレームワークには以下のようなものがあることが分かった。 (SeleniumやWebDriver系じゃないのも含む。)

 Nightwatch.js
GitHubの★は6835。
JavaScriptでブラウザテストを書けるフレームワーク。 WebDriverプロトコルに対応していて、Seleniumと異なる独自のクライアントAPIを実装。 つまり使えるブラウザの幅が広い。
テストフレームワークは独自のもの。
 WebdriverIO
GitHubの★は3217。
JavaScriptでブラウザを操作できるツール。 WebDriverプロトコルに対応していて、Seleniumと異なる独自のクライアントAPI(ラッパ?)を実装。 つまり使えるブラウザの幅が広い。
独自のテストランナであるWDIO付きで、テストフレームワークにMocha、Jasmine、Cucumberなど、いろいろ利用できる。
 Protractor
GitHubの★は6801。
JavaScriptでブラウザテストを書けるフレームワーク。 WebDriverプロトコルに対応していて、selenium-webdriverをラップしたAPIを提供する。 WebDriverなのでブラウザはなんでも。
テストフレームワークは、Jasmine、Mocha、Cucumberのほか、いろいろ選べる模様。
AngularとAngularJS向けだけどそれ以外にも使える。 Google製なので信頼感があるし、ドキュメントもコミュニティもしっかりしてる。
 Casper.js
GitHubの★は6337。
JavaScriptでブラウザテストを書けるフレームワーク。 使えるブラウザはPhantomJSかSlimerJSだけで、多分WebDriver使ってない。
テストフレームワークは独自のもの。
 Nightmare
GitHubの★は12964。
JavaScriptでブラウザを操作できるツール。 ブラウザは、昔の1系はPhantomJSを使ってたけど、今の2系はElectron。 WebDriverは使ってないはず。
テストフレームワーク機能は付いてないけど、同じ作者のNiffyというNightmareベースのツールがちょっとそれっぽい。
 TestCafe
GitHubの★は3029。
JavaScriptでブラウザテストを書けるフレームワーク。 すごい多機能っぽいし、TypeScriptやasync/awaitをサポートしててなんかモダン。 WebDriverは使ってないっぽいけど、Chrome、Firefox、IE、Edge、Safariなど、一通りのブラウザが使える。 なぜかリモートテストもできる。</description>
    </item>
    
    <item>
      <title>2017年夏、Selenium、ヘッドレスブラウザ</title>
      <link>https://www.kaitoy.xyz/2017/07/12/2017-selenium-headless-browsers/</link>
      <pubDate>Wed, 12 Jul 2017 22:36:22 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2017/07/12/2017-selenium-headless-browsers/</guid>
      <description>現在仕事でServiceNow上で動くアプリケーションを開発していて、それのブラウザテストをどうやろうかというのを少し考えたので、書き残しておく。
   (adsbygoogle = window.adsbygoogle || []).push({});  ServiceNowとは 本題とほとんど関係ないけど、一応ServiceNowに簡単に触れる。
ServiceNowはITサービス管理のSaaS。 世界的にはITサービス管理のデファクトスタンダードとなっているが、日本ではこれから盛り上がりそうといった感じ。
アプリケーションを開発するプラットフォームとしての側面もあり、JavaScript(ブラウザ側とサーバ側両方)でServiceNowの機能を拡張し、他システムと連携させたり処理を自動化したりできる。
アプリケーションがServiceNowプラットフォームで動くので、テスト方法が悩ましい。 Automated Test Frameworkというテストフレームワークが提供されてはいるが、2017年1月にリリースされたばかりということもあるのか、機能がしょぼく、大したことはできない。 これが自前でブラウザテスト環境を作ろうと思った理由。
アプリケーションがJavaScriptなので、テストもJavaScriptで書きたい。
ブラウザテストとは ここでブラウザテストとは、稼働しているWebアプリケーションに、HTTPクライアントで接続して、レンダリングされたWebページを操作して実行する自動E2Eテストのこととする。 HTTPでWebコンテンツを取得して、HTML・CSSをパースしてレンダリングして、JavaScriptを実行するツール、つまりWebブラウザを何にするかというのと、それを自動で操作するのをどうするかというのと、テストどう書くのかということと、書いたテストをどう実行するかということと、テスト結果をどう集計してレポートするかといった辺りを考える必要がある。
Qiitaの記事「ブラウザテストツール総まとめ・2016年夏版」にブラウザテストのためのツールが色々載っている。 レイヤや目的が異なるツールがちょっとごっちゃになってる気がするけど。
SeleniumとかWebDriverとか ブラウザテストはWebDriver抜きでは語れないので、とりあえずそれについて書く。 それにはまずSeleniumについて語らなければなるまい。
ブラウザテスト創世記にはこうある。
 神は「光あれ」と言われた。 するとSeleniumがあった。
神はその光を見て、良しとされた。 神はその光と闇とを分けられた。
神は光をSelenium RC (aka Selenium 1)と名づけ、 闇 をSelenium WebDriver (aka Selenium 2)と名づけられた。
 (Seleniumの歴史をもっとちゃんと知りたければこの記事を読むべし。)

要は、今ブラウザテストと言ったらSelenium、Seleniumと言ったらSelenium WebDriverというわけだ。
Selenium WebDriverは、WebDriver APIでブラウザやDOMエレメントを操作するツール。 このAPIを実装したクライアントライブラリが各言語(Java、Ruby、Python、JavaScriptなど)で提供されていて、テストコードから利用できる。
APIの裏ではドライバなるものが暗躍していて、OSやブラウザのネイティブAPIを使ってブラウザを操作している。 このドライバはブラウザごと(Chrome、Firefox、IEなど)に用意されていて、その実装形式がドライバ毎に割と違っている。 例えばFirefox用のやつ(Firefox Driver)はFirefox のアドオンを使うし、Chrome用のやつ(ChromeDriver)は独立したネイティブアプリを介してブラウザを操作する。
ドライバは(基本的に)ブラウザと同じマシンにある必要があり、実行するテストコードとも(基本的に)同居している必要がある。 テストを実行するマシンとは別のマシンのブラウザでテストしたければSelenium Server (aka Selenium Standalone Server)を使う。 Selenium Serverはブラウザとドライバと同じマシンで動き、テストコードから送信されたブラウザ操作コマンドを受信してドライバに伝える、プロキシ的な働きをしてくれる。</description>
    </item>
    
    <item>
      <title>git rebaseを図解する</title>
      <link>https://www.kaitoy.xyz/2017/06/10/git-rebase/</link>
      <pubDate>Sat, 10 Jun 2017 00:00:17 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2017/06/10/git-rebase/</guid>
      <description>この記事を読んだ、またはGitのオブジェクトモデルを理解していることを前提に、Gitの git rebase というコマンドについて説明する。
このコマンドは、コミット履歴を改変できるGit特有のコマンドで、分かり辛いGitコマンドの中でも最も分かり辛い部類のものだ。 Gitの最後の関門と言えよう。 けど、それだけに使いこなせばとても便利なものでもある。

   (adsbygoogle = window.adsbygoogle || []).push({});  git rebaseがもつたった一つの機能 git rebaseにはいろんなオプションがあって、ちょっと調べただけだと、コミットを移動する機能とコミットを修正する機能の二つがあるように見えるかもしれないが、実際は単一の機能しかないシンプルなコマンドだ。
その機能とは、指定した範囲のコミットが含む変更を、別に指定したコミットのコードベースに適用するというもの。
コマンドの基本形は次のようなものだ。
$ git rebase --onto master dev bugfix このコマンドは、bugfixから辿れるコミット群から、devから辿れるコミット群を除いたコミット群が含む変更を、masterのコードベースに適用する。
と書いても分からないので図解する。

       
このスライドを見ると、git rebaseに指定した3つのブランチのそれぞれの使われ方が分かるはず。
git rebase --onto master dev bugfixが実行する処理をもっと正確に言うと、
 bugfixをcheckoutして(i.e. HEADをbugfixにして)、 dev..HEADのコミット群が含む変更を、それぞれ仮領域にパッチとして保存して、 git reset --hard masterして、 仮領域に保存した変更を、HEADが指すコミットのコードベースにひとつひとつ順番に適用する。  
上記コマンドでbugfixのところを省略すると、ステップ1のcheckoutが省略される。 言い換えると、上記コマンドは次の二つのコマンドに分解できる。
$ git checkout bugfix $ git rebase --onto master dev さらに、--onto masterを省略すると、ステップ3のreset先が変わり、devになる。 このときのコマンドの形は、</description>
    </item>
    
    <item>
      <title>Elasticsearch、Logstash、Filebeat、elasticsearch-headでログを見てみた</title>
      <link>https://www.kaitoy.xyz/2017/04/04/elasticsearch-in-nnmi-log/</link>
      <pubDate>Tue, 04 Apr 2017 09:24:12 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2017/04/04/elasticsearch-in-nnmi-log/</guid>
      <description>NNMiログをFilebeatで集めてLogstashで構造化してElasticsearchに入れてelasticsearch-headで見てみたけど、ログ量が少なかったせいかあんまり恩恵が感じられなかった話。
   (adsbygoogle = window.adsbygoogle || []).push({});  Elasticsearchとは ElasticsearchはElastic社が開発しているElastic Stack(旧ELK Stack)というオープンソースなデータ収集分析ツール群のコア製品。 内部でLuceneを使っていて、そのためJava製。 「分散型RESTful検索/分析エンジン」と自称しているが、スキーマレスでNoSQLなドキュメント指向分散データベース管理システムとも見れる。
Elasticsearchインスタンスはノードと呼ばれ、単一または複数のノードによるシステムはクラスタと呼ばれる。 同一ネットワークに複数のノードを立ち上げると自動で相互検出してクラスタを構成してくれ、そこにデータを入れると自動で冗長化して分散配置してくれるので、堅牢でレジリエントでスケーラブルなシステムが簡単に構築できる。
Elasticsearchが管理するデータの最小単位はドキュメントと呼ばれ、これはひとつのJSONオブジェクトで、RDBにおける行にあたる。 つまり、JSONオブジェクトの各フィールドがRDBにおける列にあたる。 同種のドキュメントの集まりはインデックスと呼ばれ、これはRDBにおけるテーブルにあたる。 テーブルのスキーマにあたるものはマッピングと呼ばれ、ドキュメントのフィールドの型情報(e.g. string, integer)などを含み、Elasticsearchが自動で生成してくれる。(指定もできる、というかすべきらしい。) インデックス内ではさらにタイプという属性でドキュメントをカテゴライズできる、が、マニュアルからはタイプはあまり使ってほしくない雰囲気が感じられる。
(2018/4/25追記: 6.0で、一つのインデックスに複数のタイプを作ることができなくなり、7.0では完全にタイプが廃止されることになった。)
因みに、インデックスがRDBのデータベースでタイプがRDBのテーブルと説明されることもあるが、これは古いたとえで、公式が間違いだったとしているので忘れてあげたい。
インデックスは分散処理のために分割でき、分割した各部分はシャードと呼ばれる。 シャードを冗長化のためにコピーしたものはレプリカシャードと呼ばれ、レプリカシャードにより成るインデックスはレプリカと呼ばれる。 デフォルトでは、ひとつのインデックスは5つのシャードに分割され、1つのレプリカが作成される。
インターフェースはREST APIだけ。 REST APIに検索したいドキュメントを投げると、ドキュメントのフィールド毎に自動で形態素解析とかして転置インデックス作って保管してくれる。 検索もJSONで表現したクエリをREST APIに投げることで結果をJSONで受け取ることができる。 検索は転置インデックスや分散処理のおかげで速く、またスコアリングによってより適切な結果が得られるようになっている。
今回使ったのはv5.2.1。
Logstashとは LogstashはElastic Stackに含まれる製品で、データ収集エンジンであり、データの受け取り、解析/加工、出力の三つの機能を持つリアルタイムパイプラインを構成する。 この三つの機能はそれぞれインプットプラグイン、フィルタプラグイン、アウトプットプラグインで提供されていて、プラグインの組み合わせにより様々なパイプラインを構成できる。
インプットプラグインは単位データ(一回分のログなど)を受け取り、タイムスタンプなどのメタデータを付けたりパースしたりしてイベントを生成し、パイプラインに流す。 フィルタプラグインはインプットプラグインからイベントを受け取り、設定されたルールに従って情報を拡充したり変更したり構造化したり秘匿情報を消したりしてアウトプットプラグインに渡す。 アウトプットプラグインは指定されたディスク上のパスやデータベースやアプリやサービスにイベントを書き込んだり送信したりする。
名前の通りもともとログ収集ツールだったが、今では様々なデータに対応していて、テキストログファイルの他にsyslog、HTTPリクエストやJDBCなんかの入力を受けることもできる。
Ruby(とJava)で書かれている。
今回使ったのはv5.2.2で、プラグインは以下を使った。
 インプット: beats 3.1.12: Beats(後述)からデータを受け取るプラグイン。LumberjackというElastic社が開発しているプロトコルを使い、TCPネットワーク上でデータを受信する。 フィルタ: grok 3.3.1: 正規表現でパターンマッチして非構造化データを構造化するプラグイン。ログ解析の定番で、例えば、ログからタイムスタンプ、クラス名、ログメッセージを抽出したりする。組み込みのパターンが120個くらいあり、Apache HTTP Serverやsyslogのログであれば自分で正規表現を書く必要はない。 アウトプット: elasticsearch 6.2.6: Elasticsearchにイベントをポストするプラグイン。  Beatsとは BeatsもElastic Stackに含まれる製品で、データを採取してLogstashやElasticsearchに送信する製品群の総称。 libbeatというGoのライブラリを使って作られていて、以下のようなものがある。</description>
    </item>
    
    <item>
      <title>Firedrop(プライベートベータ)が全く期待外れだった件</title>
      <link>https://www.kaitoy.xyz/2017/03/05/firedrop-private-beta/</link>
      <pubDate>Sun, 05 Mar 2017 23:28:03 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2017/03/05/firedrop-private-beta/</guid>
      <description>Firedropという現在開発中のサービスがある。 WebサイトのデザインをAIがサポートしてくれるサービスだ。
2016年夏のニュースを見たとき、AIがテキストコンテンツを解析してサイトを自動構成してくれ、さらにA/Bテストなどを自動でやってサイトを継続的に改善すると言う衝撃的なふれこみだったので、即座にアーリーアクセスに登録した。
それからしばらく忘れていたが、3月2日にプライベートベータへの招待メールが来たので早速試してみたら、かなりのスモールスタートをしたようで全く期待外れだった。

   (adsbygoogle = window.adsbygoogle || []).push({});  
Firedrop(プライベートベータ)の機能 Firedrop(プライベートベータ)では、SachaというAIがWebサイトの構築をサポートしてくれる。 こいつが実のところほとんど知性を感じない単なるチャットボットで、なるほどこれは見事な人工無脳だと感心してしまうほどだ。
Firedropのアカウントを作るとまず、Sachaとチャットしながらサイトの概要(タイトル、概要、画像など)を教えることになる。 するとSachaがざっくりとシングルページのサイトを作ってくれるので、それをまたSachaとのチャットで調整したりコンテンツ追加したりする。
チャットと言っても、基本はこちらは5,6個ある選択肢の中からセリフを選ぶサウンドノベル方式で、一応任意の文章も入力できるがあいさつするくらいしか使い道がない。
追加コンテンツはテキストと画像を渡すと自動でレイアウトしてくれるが、すごくいい感じにしてくれるというわけでもないし、むしろ画像が見切れたりするし、細かい調整はできないので、妥協できるレイアウトになるまでチェンジを繰り返すデリヘル方式を採ることになる。 デリヘルなんて利用したことないけど。
画像は自分でアップロードもできるけどFiredropが提供しているものもあって、後者のやつはSachaにキーワードを伝えるとそれっぽい画像を探してくれるあたりに唯一知性を感じる。
デザインができたらSachaに頼むとfiredrop.meドメインで公開してくれる。
(FiredropのUIのスクリーンショットを載せようかと思ったけど、プライベートベータの規約を読んだ感じだめそうだったので載せない。)
実際に作ってみた 今回実際にFiredropでGoslingsのサイトを作ってみて、できたのがこれ。
ひどい。
そもそも当初のテキストコンテンツを解析してサイトを自動構成というコンセプトはどこへ行ったのか。 GoslingsのReadmeを入力したらシャレオツなサイトをささっと作ってくれるイメージだったんだけど。
まだまだ開発中の機能がたくさんあるそうなので、GAまでにはもうちょっとなんとかなるんだろう。 あまり期待はしない。</description>
    </item>
    
    <item>
      <title>Hibernateはどのようにして私のキャリアを破滅寸前にしたか</title>
      <link>https://www.kaitoy.xyz/2017/02/23/how-hibernate-ruined-my-career/</link>
      <pubDate>Thu, 23 Feb 2017 00:25:03 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2017/02/23/how-hibernate-ruined-my-career/</guid>
      <description>このエントリでは、Grzegorz Gajosによる記事、How Hibernate Almost Ruined My Careerを紹介する。 (Grzegorzから和訳と転載の許可は得た。) 以下はその全文の和訳だが、意訳超訳が混じっているので、もとのニュアンスを知りたければ元記事を読んでもいいし、読まなくてもいい。

   (adsbygoogle = window.adsbygoogle || []).push({});  想像してくれ。 君はJava開発者で、次のビッグプロジェクトを開始しようとしているところだ。 君は、そのプロジェクト全体に影響する根本的な決断をする必要がある。 君の柔軟なデータモデルをオブジェクト指向で抽象化するベストな方法を選択したい。生のSQLを扱いたくはないからね。 どんな種類のデータもサポートしたいし、理想では全種のデータベースをサポートしたい。
すぐに思いつくのは、単にHibernateを使うという解だ。そうだろ？ Javaディベロッパの90%は君に同意するだろう。 しかし、それって正しい決断になっているだろうか？
Hibernateが一般に受け入れられているスタンダードだからといって盲目的に採用してしまうと、どんな問題が発生するかを見てみよう。
モニカというJava開発者がいるとしよう。 モニカは最近出世してアーキテクトの役職に就き、会社の新製品のための技術スタックを選定する責任者になった。 彼女は、Javaの世界にはデータベースとのやり取りを扱うたった一つの適切なツールがあることを知っている。Hibernateだ。 Hibernateは良く知られ支持されているJPAのスタンダードではあるが、プロジェクト開始前に多少のチェックをするのが定跡だ。 幸いにも、同僚のベンが適任者を知っている。
4年前、Hibernateは銀の弾丸かのように見えた  ベン: やあモニカ。ジョンを紹介させてくれ。彼はHibernateの達人だ。君の助けになるはずだ。 モニカ: ジョン、時間を取ってくれてありがとう。 今、私たちが次なる目玉製品を開発しようとしてるのは知ってる？ 次のFacebookやGoogleになるつもりなの。 忙しくなるわ。巨大なものになる。 本当にすてき！ みんなとても興奮してる！ 私はアーキテクトの役に就いたから、とりあえず採用する技術スタックを選定しなければいけないの。 ひとつだけまだ欠けてるのが永続化なんだけど… ジョン: Hibernate！ モニカ: そう！そのとおり！ わたしもそう考えていたの！ それならわたしたちにぴったりで上手く行きそうでしょう。 マーケットと豊富な実績に裏付けられた、真の業務問題のための真の業務ソリューション。 とてもたくさんのポジティブな経験談を聞いたことがあるわ。 けど、一つ問題があって、チームメンバのひとりがそれに絶対反対してるの。 アプリケーションとデータベースの間に別のレイヤを加えるのを気にして。 彼はすごく頭がいいから、これが良い決断だと納得させるには本当にしっかりした根拠が必要なの。 助けてくれる？ ジョン: もちろん、よろこんで！ Hibernateは、実際、すばらしいツールです。 銀行といった、大きな真の業務ソリューションで広く使われています。 それを使って失敗するということはありえません。 永続化ときたらHibernate。 Javaで書いているならそれが完全に正しい選択ですし、さらには他の言語への移植もあります。 どれだけ多くの職務記述書がそれを要求しているか！ モニカ: 全く同感！ 同じことを感じていたわ。 前のプロジェクトで、ほとんどのところで生のJDBCからSQLを使っていたんだけど、ばかげてた！ そうでしょ！ けど、実は、ほんとに優秀なSQL屋がチームにいて、Hibernateが生成したSQLを見て神経質になってるの。 きたなくて読みにくいって。 これって将来問題になりそう？ ジョン: 気を付けてください。DBA屋ってのは違ったものの見方をします。 彼らはプロジェクトにおける自分の立場をHibernateに奪われるのではないかと危惧しています。 さらに、データベースには組み込みのクエリ最適化機構があるので、クエリの実際の見た目がどんなかは気にする必要はありません。 データベースが最適化してくれます。 それが高速開発ってものです。 SQLにはまねできません。 モニカ: ほんとに？ もうSQLを触らなくていいの？ すごい！ この前DBAがクエリの最適化に数週間かけていたわ。 数週間！ あぁ、こんなこと言うのは恥ずかしいんだけど、わたしたちが何を使っていたか分かる？ …ストアドプロシージャ(笑) もうくちゃくちゃだった。 そのプロジェクト、まだそれ使ってるって信じられる？ そこの人たちほんとに気の毒だわ。 未だにあんな退屈なコードを何度も何度も書かないといけないなんて。 あれってJavaというかもうSQLプロジェクトじゃない？ ジョン: それはまさにオブジェクト指向とリレーショナルのアプローチの違いです。 いわゆるオブジェクト指向インピーダンスミスマッチですね。 Hibernateはその溝を埋めてくれます。 開発者はビジネスロジックの構築に専念できます。 ステークホルダもマネージメント全体も幸せになれます。 最も重要となることをしましょう。ビジネスです！ ボイラープレートの多くは消え去り、魔法のようで目には見えませんが、ロジックとデータとの間にしっかりとしたコネクションができるのです。 モニカ: 相互協調。充実したシナジー。 まるでデータベースが最初から言語の一部だったかのよう。 信念の技術的飛躍の指導者になれてとても幸せ。 ソフトウェアという旅路でワープスピードに乗ったみたい。 ジョン: そう！その調子！ モニカ: わーい！すごーい！ わくわくしてきた！たーのしー！ ありがとうジョン。 準備万端！  3年前、柔軟性のないソリューションとともに大きくなる苦悩  モニカ: ねえジョン、去年話したプロジェクト覚えてる？ ジョン: もちろん。調子はどうですか？ モニカ: もうすぐリリースするわ。 全て順調。だけどいくつか疑問がわいてきたの。 ジョン: いいですよ。聞いてください。 モニカ: ええと、もうデータベーススキーマを一から生成することはできないんだけど、データロスのないスキーマ変更をサポートするにはどうするのが一番いい？ ジョン: えぇ、まず、Hibernateはプロダクション環境での移行ツールとして使われることを想定していません。 FlywayDBやLiquibaseといったものを使うべきです。 結構簡単ですよ。移行スクリプトを書いて、それでエンティティモデルを更新しつつ、Hibernateのマッピングも直して、 実際のデータベース構造と同期するようにしてください。 モニカ: ふーん、分かった。前のプロジェクトでは単に生のSQLで移行してた。 ジョン: それでもいいと思います。エンティティモデルとスキーマが同期してさえいれば、好きなやり方でやってください。 モニカ: そうね。 もう一つ、わたしたちいつも遅延フェッチと即時フェッチの問題と戦ってるの。 ある時、全部を即時でやることに決めたんだけど、それって最適じゃないでしょ？ それに、たまにセッションが残ってないせいかなにかで、フィールドにアクセスできないことがあるの。 それって普通？ ジョン: もっとHibernateについて学ぶ必要があります。 データベースとのマッピングは単純ではありません。 複数のやりかたがあるのが普通です。 その中から自分に合ったものを選ぶのです。 遅延フェッチはオブジェクトを必要なときにロードできるようにしますが、アクティブなセッションの中で実行しないといけません。 モニカ: わたしたちはまだ最終的なデプロイでどのデータベースエンジンを使うべきか迷ってるの。 Hibernateってポータブルだと思ってたけど、MS SQLの魔法を使うためにちょっとネイティブクエリを使ってて、実際にはプロダクション環境ではMySQLで行きたいんだけど。 ジョン: HibernateはdetachedなCriteriaクエリかHQLを使っている限りは柔軟です。 ネイティブクエリを使ったらそのデータベースにソリューションが固定されちゃいますよ。 モニカ: それならMS SQL専用にしないとダメみたいね。 最後の質問。チームメンバからHQLには「limit」キーワードがないと聞いたわ。 冗談かと思ったけど、わたしも見つけられなかった。 バカな質問で申し訳ないんだけど… ジョン: 確かに。HQLには「limit」はありません。 クエリオブジェクトでコントロールすることはできます。 データベースベンダ依存のものなので。 モニカ: 他の要素は全部HQLにあるのに変ね。 まあ気にしないで。 時間取ってくれてありがとう！  2年前、わたしたちは今再びSQLでソリューションをハックしている  モニカ: ジョン、最初わたしたちSQLを触らないつもりだったけど、今はその必要があると感じてる。 要件が大きくなってきていて、それを避ける手立てはないみたいなの。 間違ったことをしているかもしれないけど、またSQLを毎日のように使い始めたわ。 ジョン: いえ、それは間違っていません。 最初期にはデータベースを気に掛ける必要はありませんでしたが、プロジェクトが進んだ今では、SQLを使ってパフォーマンスの最適化に取り組むのはいいことです。 モニカ: ときどきエラーを調査するのに数日かかるの。 なぜ期待通りに動かないのか、なぜ思いがけない結果が出力されるのかが全く分からないから、Hibernateが生成したSQLを解析しないといけないみたい。 Hibernateのバグトラッカーに載ってる有名な問題に当たったこともある。 それだけじゃない。エンティティモデルの同期を保ったまま適切な移行処理を書くのは難しいの。 Hibernateの内部をよく調査して、どう動くのかを予測する必要があって、時間を取られてしまう。 ジョン: 学習曲線というのは常にあります。 たくさんの記述はいりませんが、どう動くかは知っておく必要があります。 モニカ: 大きなデータセットを扱うのも厄介。 最近データベースに大量のインポートをしたんだけど、あまりにも遅かった。 あとで、速くするにはセッションをクリアする必要があったって分かったんだけど、それでもまだ全然遅い。 だから生のSQLで書き直すことにしたの。 笑ったわ。生のSQLを書くのが実際最速の方法だったから。 だから最後の選択肢としてそうすることに決めたの。 ジョン: インポートはオブジェクト指向な処理ではないです。 Hibernateはオブジェクト指向設計に焦点を当てています。 ネイティブクエリという選択肢を忘れてはいけません。 モニカ: Hibernateのキャッシュがどう動くか知りたいんだけど、教えてくれる？ ちょっと分からないの。 一次キャッシュとか二次キャッシュとかあるけど、どういうものなの？ ジョン: もちろんです。 それはいわゆる永続データのトランザクションレベルキャッシュです。 クラスタやJVMレベルで、クラス毎やコレクション毎のキャッシュを設定できます。 クラスタキャッシュを組み込むことさえできます。 しかし、キャッシュは他のアプリケーションが永続化領域に加えた変更については関知しないことを覚えておいてください。 期限切れのキャッシュデータを定期的に消すように設定することはできますが。 モニカ: ごめん。気分が悪くなってきた。 もう少し説明してくれる？ ジョン: はい。 saveとかupdateとかsaveOrUpdateにオブジェクトを渡したり、load、get、list、iterate、scrollでオブジェクトを取得するときは常に、そのオブジェクトはセッションの内部キャッシュに追加されます。 一次キャッシュからオブジェクトやそのコレクションを削除することもできます。 モニカ: あぁ… ジョン: さらに、キャッシュモードを制御することもできます。 normalモードでは読み込みと書き込みで二次キャッシュを使います。 getモードでは二次から読みますがライトバックはできません。 putはgetと同じですが二次から読むことはできません。 refreshモードもあります。 これは二次に書き込みますが、そこからは読み込まず、use minimal putsプロパティを無視し、データベースからの全ての読み込み時に二次キャッシュを強制リフレッシュします。 モニカ: いいわ。わかった。 ちょっと考えさせて。 もう遅いわ。行かなきゃ。 説明ありがとう！ ジョン: どういたしまして！  2週間前、Hibernateをあきらめる  モニカ: ジョン、わたしソフトウェア開発の新時代に入ろうとしてるんだと思ってた。 一光年の飛躍をしてるんだと思ってた。 けど、4年たった今も、わたしたちは同じ問題に対応してるみたい。単に違う方向から。 私はHibernateのアーキテクチャ、設定、ロギング、ネーミング戦略、Tuplizer、エンティティ名リゾルバ、拡張IDジェネレータ、IDジェネレータ最適化、ユニオンサブクラス、XDocletマークアップ、インデックス付きコレクションの双方向関連、3項関連、idbag、暗黙的ポリモーフィズムと他の継承マッピングの組み合わせ、二つの異なるデータストア間でのオブジェクトレプリケーション、detachedオブジェクトと自動バージョニング、コネクション開放モード、ステートレスセッションインターフェース、コレクション永続化の分類法、キャッシュレベル、遅延/即時フェッチ、他にもいろんなことを学ばなければならなかった。 わたしの知ってる全てがあっても、ひどい失敗になっていたと思う。 ソフトウェアの出来損ないだわ！ 究極の失敗！ 大参事！ アルマゲドン！ ジョン: 待ってください！なにがあったんですか？ モニカ: わたしたち行き詰ったの。 わたしたちのアプリケーションの性能はばかばかしいほど遅い！ レポートを取得するのに二日も待たないといけない！ 二日でやっと顧客にダッシュボードを生成して見せられるの。 つまり毎日計算処理を開始させなければならない上に、ダッシュボードの情報はどんどん遅れてしまう。 うちのDBAエキスパートがクエリ最適化に2か月取り組んでるけど、データベース構造がめちゃくちゃで。 それを手伝ってる開発者もいるけど、困ったことに、DBAはSQLで考えているから、開発者はそれをdetached CriteriaかHQLに翻訳しようと何日も費やしてしまうの。 今となっては性能がかなり重要だから、できるだけネイティブSQLを使おうとしてるわ。 なんにせよ、データベーススキーマがはっきり間違っちゃってるから大したことはできない。 オブジェクト指向な視点ではそれでいいと感じていたけど、リレーショナルな視点では最悪だったみたい。 どうしてこうなっちゃったんだろう？ 開発者はエンティティ構造を変えるのはかなりの労力になると言うから、それをする余裕はないし。 前のプロジェクトは乱雑ではあったけど、そんな窮地には陥らなかった。 既存のデータを処理する完全に別のアプリケーションを書くこともできた。 今は、生成されたテーブルを変えるのは危険だわ。 エンティティモデルが完全に正しく動くことを保証するのは本当に難しいもの。 けどこれさえも最悪な点ってわけではないわ！ 性能改善するには、データベース問題だけでなく、データベースとアプリケーションの間のレイヤ全体の問題も解決しないといけない。 それが圧倒的！ この新しく加わった人たちはね、コンサルタントなの。 彼らはデータを抽出して、なにか別のストレージに入れて、外側から計算を実行しようとしてる。 どれも時間かかりすぎ！ ジョン: なんと言っていいか分かりません。 モニカ: いいのよジョン、あなたを責めはしないわ。 わたしはHibernateを選択して全ての問題を解決しようとしたけど、今ではそれが銀の弾丸ではないと分かる。 もうダメージは負ったし、それをなかったことにはできない。 実は、あなたにお願いしたいことがあるの。 わたしはこの4年間のキャリアをHibernateのあれこれとの戦いに費やしてしまったわ。 もう今の会社でわたしに未来はないみたい。 助けてくれない？  今日、学んだ教訓は？  ジョン: やあピーター、モニカを紹介するよ。 ピーター: やあモニカ！ わたしたちは新しい次なる目玉を開発しようとしてるんだけどね。 巨大なものになりそうだよ！ Uberみたいになりたいんだ！ 永続化について何か知って… モニカ: Hibernateはダメ！  まとめ モニカはHibernateのエキスパートだ。 しかし、この例ではHibernateは間違った選択だった。 彼女のソリューションが以前より大きな問題に変化したと気付いたときには、プロジェクト全体を脅かす最大の脅威になってしまっていた。</description>
    </item>
    
    <item>
      <title>ブログアドレスを変更したときにやったこと</title>
      <link>https://www.kaitoy.xyz/2017/02/14/change-subdomain/</link>
      <pubDate>Tue, 14 Feb 2017 09:51:42 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2017/02/14/change-subdomain/</guid>
      <description>このブログの閲覧数がそこそこの規模になってきたので、Google AdSenseで小遣い稼ぎを始めようとしたら、最近サブドメインがwwwじゃないとできないようになったようだったので、サブドメインをtbdからwwwに変更した話。
変更自体はそんなに難しくなかったけど、Googleの検索順位を保つためにいろいろ気を使う必要があった。

   (adsbygoogle = window.adsbygoogle || []).push({});  ブログアドレスの変更 以前にも書いたが、このブログはHugoで作ってGitHub Pagesでカスタムドメインで公開している。
コメント欄を設けるためにDisqusを使っている。
Cloudflareを使って全体をHTTPS化していて、その関係でkaitoy.xyzドメインの名前解決にはCloudflareのDNSを使っている。
アクセス解析などのためにGoogle AnalyticsとGoogle Search Consoleを使ってる。
この構成で、ブログアドレスの変更に必要だった修正を列挙する。(この順にやったわけではない。)
1. ブログソース修正 Hugoの設定ファイルであるconfig.tomlに書いてあるbaseurlの値をhttps://tbd.kaitoy.xyzからhttps://www.kaitoy.xyzに変え、また、各記事の内部リンクのURLもwwwのに変えた。
あとrobots.txtのSitemapのURLもhttps://www.kaitoy.xyz/sitemap.xmlに更新した。
2. GitHub Pagesの設定変更 ブログリポジトリに行って、SettingsのGitHub Pages欄のCustom domainの値をhttps://www.kaitoy.xyzに変えた。
ついでにブログリポジトリのトップに表示されるDescriptionのWebsiteの値も新しいURLに変更した。
この変更によりありがたい副作用もあった。 GitHub Pagesはwwwというサブドメインを特別扱いしていて、以下の恩恵を受けられるのだ。
 wwwを省略したURL(apex domain)でアクセスすると、GitHub Pagesサーバがwww付きのURLにリダイレクトしてくれる。 安定していて速い。  3. CloudflareのDNS設定変更 CloudflareのDNSで、もともとCNAMEレコードでkaitoy.github.io(GitHub Pagesのデフォルトのドメイン)のエイリアスをtbdにしていたのをwwwに変更した。
また、上記の通りapex domainでGitHub Pagesにアクセスしても上手いことやってくれるようになったので、www.kaitoy.xyzのエイリアスをkaitoy.xyzとするCNAMEレコードを追加した。 CloudflareのDNSはapex domain(i.e. kaitoy.xyz)に対するCNAMEレコード設定をサポートしているので、これでwww.kaitoy.xyzでもkaitoy.xyzでもGitHub Pagesにルーティングされるようになった。
4. Disqusの設定変更 ホームの右上の歯車アイコンからAdminを開いて、ヘッダのSettingsからブログのURLを選んでその設定画面を開き、Website URLをhttps://www.kaitoy.xyzに変更した。
5. Google Analyticsの設定変更 管理タブのプロパティ設定のデフォルトの URLをhttps://www.kaitoy.xyzに変更しただけ。
Googleのページランクを保つためのあれこれ 以前もどこかに書いたが、どんなにすばらしい内容の記事を書いてもGoogle検索結果の2,3ページくらいまでに出てこないんであれば誰も読んでくれない。 このブログのいくつかの記事はそれなりにいいキーワードでいい検索順位になっていたので、サブドメイン変更によってページランクに悪影響が出るのはなるべく避けたかった。
調べたら、Google Search Consoleのヘルプにまさにその悪影響を防ぐ方法が載っていたので、これに従ってあれこれした。</description>
    </item>
    
    <item>
      <title>Goslings開発メモ - その5: Spring Boot最終編 (静的リソース処理)</title>
      <link>https://www.kaitoy.xyz/2017/01/24/goslings-development-memo5-spring-boot-static-resources/</link>
      <pubDate>Tue, 24 Jan 2017 09:01:49 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2017/01/24/goslings-development-memo5-spring-boot-static-resources/</guid>
      <description>「Goslings開発メモ - その4: Spring Boot続続続編 (ロギング)」の続き。
Spring Boot最終編で、静的リソース処理について。

   (adsbygoogle = window.adsbygoogle || []).push({});  Spring Boot(Spring MVC)での静的リソース処理 この時点でのGoslingsは単なるREST APIサーバで、アクセスしてもJSONを返すだけだ。 アプリとしての体を成すためには、そのAPIを利用するクライアントコード、つまりHTMLドキュメントやCSSファイルやJavaScriptファイル(静的リソース)も返すようにしないといけない。 HTMLドキュメントを返す場合、普通はなんらかのテンプレートエンジンを使うものだが、Goslingsは本当に単純なGUIなので、サーバに置いたHTMLファイルをそのまま返したい。
「Getting Started Guides」にはServing Web Content with Spring MVCというのが乗っているが、これはThymeleafというテンプレートエンジンを使うものなのでちょっと違う。

Spring Bootリファレンスガイドによると、クラスパス(またはServletContextのルート)の/static/、/public/、/resources/、/META-INF/resources/のいずれかに静的リソースを置けば、特にコードを書かなくてもクライアントからアクセスできるらしい。 (逆に、一般的に静的リソースを置く場所である、プロジェクトのsrc/main/webapp/には置くべきでないとのこと。これは、jarにパッケージングするときにビルドツールに無視されることが多いため。)
この仕組みについて、この記事を参考にちょろっとソースを見た感じでは、これらのパスはResourcePropertiesのCLASSPATH_RESOURCE_LOCATIONSに定義されていて、これをWebMvcAutoConfigurationがResourceHandlerRegistryでリソースロケーションとして登録することで静的リソース置き場たらしめている模様。 (このResourceHandlerRegistryはResourceHttpRequestHandlerを設定するファサード的なものっぽい。)
で、@SpringBootApplication(その1参照)が付いているクラスがあって、spring-webmvc.jarがクラスパスにあると、@EnableWebMvcがSpring Bootによって付けられ、そこからごにょごにょして上記WebMvcAutoConfigurationが実行される。 spring-webmvc.jarはspring-boot-starter-web.jar(その1参照)が引っ張ってくる。

なお、Spring MVCの静的リソース処理の全体の流れについては 、ちょっと古いけど「handling static web resources」という記事が分かりやすい。 要は、URLに指定されたパスからサーバ上のリソースを探し当てるResourceResolverというものが優先度順に連なっているリゾルバチェイン(ResourceResolverChain)があって、まずこいつがリソースを取得する。 次に、そのリソースを加工するトランスフォーマチェイン(ResourceTransformerChain)というものに通し、その結果をクライアントに返す。 トランスフォーマチェインはResourceTransformerが連なったもの。 リゾルバチェインとトランスフォーマチェインは上記ResourceHttpRequestHandlerに設定される。
リゾルバには以下の様なものがある。
 PathResourceResolver: ResourceHttpRequestHandlerに設定されたリソースロケーションからリソースを単純に検索するリゾルバ。 CachingResourceResolver: キャッシュからリソースを検索するリゾルバ。テンプレートエンジンの処理結果のキャッシュとかが返るのは多分ここから。 GzipResourceResolver: gzipで圧縮されたリソース、つまりURLで指定されたパスに.gzという拡張子を付けたリソースを検索するリゾルバ。 VersionResourceResolver: リソースバージョニングを実現するためのリゾルバ。 WebJarsResourceResolver: WebJarsのjarファイル内のリソースを検索するリゾルバ。  リゾルバの設定などについてはQiitaのこの記事ががよくまとまっている。 凝ったことをしたいときは参照しよう。</description>
    </item>
    
    <item>
      <title>Goslings開発メモ - その4: Spring Boot続続続編 (ロギング)</title>
      <link>https://www.kaitoy.xyz/2017/01/17/goslings-development-memo4-spring-boot-logging/</link>
      <pubDate>Tue, 17 Jan 2017 00:15:25 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2017/01/17/goslings-development-memo4-spring-boot-logging/</guid>
      <description>「Goslings開発メモ - その3: Spring Boot続続編 (例外処理)」の続き。
Spring Boot続続続編で、ロギングについて。

   (adsbygoogle = window.adsbygoogle || []).push({});  Spring Bootアプリにおけるロギング Spring Bootアプリにおけるロギングについては公式のマニュアルとHow-toガイドを読むべし。 この記事にはこれらの内容をまとめておく。

Spring Bootは内部でのロギングにApacheのCommons Loggingを使っている。
Commons Loggingはファサードライブラリだ。 つまり、Commons LoggingはロギングAPIだけをアプリケーションに提供し、実際のログ出力処理をするロギング実装ライブラリへの橋渡しとして機能する。 ロギング実装ライブラリには色々な選択肢があるが、Spring BootはJUL、 Log4j 2、Logback用のデフォルト設定を備えているので、これらのいずれかを使うのが楽であろう。
全てのスターターはspring-boot-starter-loggingというロギングスターターに依存していて、これがLogbackを使うので、普通はそのままLogbackを使うことになる。 spring-boot-starter-loggingは、JUL、Commons Logging、Log4j、SLF4Jによるログ出力をLogbackにルーティングするため、アプリ側や他の依存ライブラリがこれらを使っていてもLogbackに一本化できる。
spring-boot-starter-loggingの代わりにspring-boot-starter-log4j2に依存し、Log4j 2を使う方法もあるが、Goslingsには普通にspring-boot-starter-loggingを使った。
また、Goslings本体のログ出力には、プレースホルダを使いたかったのでSLF4Jを使った。
Spring Bootアプリにおけるロギング設定 Spring Bootが備えているデフォルトのロギング設定は、ERROR、WARN、INFOレベルのログをいい感じにフォーマットしてコンソールに吐くというものになっている。
以下この設定の変更方法などを書く。
ファイルへのログ出力 ログをファイルにも吐くようにするには、logging.fileというプロパティでファイルパスを指定するか、logging.pathというプロパティでディレクトリパスを指定すればいい。 (後者の場合ログファイル名はspring.logになる。)
Spring Bootアプリでプロパティを指定する方法は色々あり(こことかここ参照)、大抵はapplication.propertiesで指定するんだろうけど、手軽にコマンドラインで以下の様に指定することもできる。
java -jar build/libs/goslings-0.0.1.jar --logging.file=build/hoge.log 
ログファイルはデフォルトで10MBでローテーションする。
ログレベル ログレベルには重大度の低い方からTRACE、DEBUG、INFO、WARN、ERROR、FATALの6段階があり、指定したログレベル以上のログが出力される。(OFFというログ出力を止めるものもある。) つまりSpring BootのデフォルトのログレベルはINFOだということだ。(LogbackにはFATALがなくERRORとして出力される。)
ログレベルはlogging.level.&amp;lt;ロガー名&amp;gt;という形式のプロパティで指定できる。 例えばコマンドラインから指定するなら以下の感じ。
java -jar build/libs/goslings-0.0.1.jar --logging.level.org.springframework.web=DEBUG 
全ロガーのログレベルはlogging.level.rootで指定できる。</description>
    </item>
    
    <item>
      <title>Goslings開発メモ - その3: Spring Boot続続編 (例外処理)</title>
      <link>https://www.kaitoy.xyz/2017/01/13/goslings-development-memo3-spring-boot-exception/</link>
      <pubDate>Fri, 13 Jan 2017 14:01:01 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2017/01/13/goslings-development-memo3-spring-boot-exception/</guid>
      <description>「Goslings開発メモ - その2: Spring Boot続編 (DI)」の続き。
Spring Boot続続編で、例外処理について。

   (adsbygoogle = window.adsbygoogle || []).push({});  Spring MVCアプリにおける例外処理 Goslingsは前々回書いたようにspring-boot-starter-webというスターターを使っていて、つまりSpring MVCアプリだ。
Spring MVCアプリにおける例外処理についてはちょっと古いがこの記事に詳しい。
まず、Goslingsの構成で例外処理を何も書かなかった場合、コントローラのリクエストハンドラから例外が投げられると、ログにスタックトレースが出力され、クライアントにはHTTPステータスコード500 (Internal Server Error)とともに以下の様なデフォルトのエラーページが返る。

なんだかこれでも十分な気がするが、実際にはちゃんと明示的に例外処理をしたほうがいいだろう。 エラー時に返すHTTPステータスコードをカスタマイズしたり、遷移するページを変えたりしたくなるだろうから。
記事によれば、リクエストハンドラ内で例外をキャッチして処理するのはイケてなくて、関心事の分離のために別の場所に処理を書くのが良いらしい。
Spring MVCアプリにおける例外処理には以下の3つの段階がある。
 投げる例外をカスタマイズする 例外クラス毎の例外ハンドラをコントローラに実装する コントローラ間で共用する例外ハンドラクラスを作る  以下それぞれについて書く。
1. 投げる例外をカスタマイズする リクエストハンドラから投げる例外に@ResponseStatusをつけることで、クライアントに返すHTTPステータスコード(とリーズンフレーズ)をカスタマイズできる。
例えば以下のような例外を投げると、HTTPステータスコード500 (Internal Server Error)の代わりに400 (Bad Request)がクライアントに返る。
@ResponseStatus(HttpStatus.BAD_REQUEST) public final class BadRequestException extends RuntimeException { // 省略 } 2. 例外クラス毎の例外ハンドラをコントローラに実装する コントローラのメソッドに@ExceptionHandlerをつけてやると、そのメソッドは例外ハンドラになり、そのコントローラのリクエストハンドラから特定の例外が投げられたときの処理を書くことができる。 さらに例外ハンドラに@ResponseStatusをつければ、HTTPステータスコードをカスタマイズできる。 例外ハンドラの戻り値はリクエストハンドラのと同様に処理されるので、遷移するページ等も自由にカスタマイズできる。
Goslingsでは、上記BadRequestExceptionからは@ResponseStatusを削除したうえで、RestApiV1Controllerに以下の様に例外ハンドラを書いた。
public final class RestApiV1Controller { // 例外ハンドラ  @ResponseStatus(HttpStatus.</description>
    </item>
    
    <item>
      <title>Goslings開発メモ - その2: Spring Boot続編 (DI)</title>
      <link>https://www.kaitoy.xyz/2017/01/10/goslings-development-memo2-spring-boot-di/</link>
      <pubDate>Tue, 10 Jan 2017 00:21:27 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2017/01/10/goslings-development-memo2-spring-boot-di/</guid>
      <description>「Goslings開発メモ - その1: Spring Boot編」の続き。
Spring Boot続編で、DIについて。

   (adsbygoogle = window.adsbygoogle || []).push({});  DIとは DIはDependency Injectionの略。依存性注入と訳される。
これは、Javaの文脈で具体的目に言うと、あるクラスが依存する具象クラスのインスタンス化と取得をフレームワークに任せることで、具象クラス間の直接的な依存を排除し、よってコンポーネント間を疎結合にする手法。 これにより、アプリの拡張性を高めたり、テストがしやすくなったりする。(参考記事)
Spring FrameworkはもともとこのDI機能を提供するフレームワーク(i.e. DIコンテナ)として普及した。
GoslingsでDI Goslingsサーバの内部機能はざっくり、クライアントからのREST API呼び出しを処理するユーザインタフェース層と、Gitリポジトリにアクセスするデータベース層に分かれる。
Gitリポジトリにアクセスする部分は今回はJGitで実装するが、将来的に別のライブラリで実装しなおす可能性が微レ存なのと、Goslingsの開発自体がWebアプリ開発の練習でもあるので、ちゃんとしたアーキテクチャでと思い、DAOパターンを使ってやった。
つまり例えば、GitのコミットオブジェクトはJGitのAPIではRevCommitクラスで表されるが、ユーザインタフェース層からはリソースクラスであるCommitクラス(前回参照)を扱う以下の様なDAOインターフェースを呼ぶようにし、JGit依存の実装とは切り離す。
public interface ObjectDao { public Commit[] getCommits(String token) throws DaoException; } (ObjectDao.javaの完全なソースはこれ)

ObjectDaoを実装するObjectDaoImplクラスでは、以下の様にJGitを使ってごりごりと実装を書く。
public final class ObjectDaoImpl implements ObjectDao { // フィールド定義は省略  @Override public Commit[] getCommits(String token) { try { return StreamSupport.stream(resolver.getGit(token).log().all().call().spliterator(), false) .map(this::convertToCommit) .toArray(Commit[]::new); } catch (NoHeadException e) { // エラー処理  } } private Commit convertToCommit(RevCommit commit) { // RevCommitをCommitに変換する処理  } }</description>
    </item>
    
    <item>
      <title>Goslings開発メモ - その1: Spring Boot編</title>
      <link>https://www.kaitoy.xyz/2017/01/03/goslings-development-memo1-spring-boot/</link>
      <pubDate>Tue, 03 Jan 2017 23:36:01 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2017/01/03/goslings-development-memo1-spring-boot/</guid>
      <description>「Goslings開発メモ - その0: 紹介と概要と設計編」の続き。
Spring Boot編。

   (adsbygoogle = window.adsbygoogle || []).push({});  Spring Bootとは Spring BootはSpring FrameworkというJavaのWebアプリケーションフレームワークを簡単に利用するためのツールやライブラリ群。
これを使うと、Webアプリケーションコンテナ(e.g. Tomcat)なしで起動できるSpringアプリケーションを、自動コード生成も設定ファイル作成もせずに作ることができる。 必要な設定は自動で構成され、設定のカスタマイズもアノテーションでできる。
GAになったのが2014年4月なのでかなり新しいものだが、JavaのWebアプリケーションを作るためのものとしては今世界的に最も流行っているもの。
私が昔とあるWebアプリを作った時はSpring Rooという[RADツール](https://ja.wikipedia.org/wiki/RAD_(%E8%A8%88%E7%AE%97%E6%A9%9F%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0%E7%92%B0%E5%A2%83)が熱かったが、これはコード自動生成をして開発を助けてくれるもので、なんだか結局あまり流行らなかったようだ。
Goslingsには最新バージョンの1.4.3.RELEASEを使った。
Spring Bootことはじめ 包括的網羅的なドキュメントは「Spring Boot Reference Guide」だが、今回あまり深く学ぶ時間が取れなかったのでこれはちら見した程度。 それよりも、ユースケースごとのチュートリアルが60個以上も載っている「Getting Started Guides」を参考にした。
Goslingsサーバは基本REST APIサーバなので、上記チュートリアルの内「Building a RESTful Web Service」を見ながら以下を実施した。
1. プロジェクト作成 チュートリアルにはGradleプロジェクトのディレクトリ構成を手動で作るところから書いてあるけど、そこはIDEなどで楽できる。 私はEclipseを使っていて、いつのまにかGradleプラグインであるEclipse Buildship: Eclipse Plug-ins for GradleとGradle IDE Packがインストールされていたので、これらを使った。
どちらのプラグインでもプロジェクトは作成できるが、Qiitaのこの記事にあるとおり、Gradle IDE Pack(に含まれるGradle (STS) Integration for Eclipse by Pivotal)で作った場合、Gradle Wrapperが生成されないなどの問題があるので、Buildshipの方で作成。 ただ、Gradle IDE Packの方がパッケージ・エクスプローラでの見え方がちょっとよかったので、こちらでプロジェクトをインポートしなおした。
(上がBuildshipのやつで、下がGradle IDE Packのやつ)</description>
    </item>
    
    <item>
      <title>Goslings開発メモ - その0: 紹介と概要と設計編</title>
      <link>https://www.kaitoy.xyz/2016/12/11/goslings-development-memo0-intro-design/</link>
      <pubDate>Sun, 11 Dec 2016 15:26:45 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/12/11/goslings-development-memo0-intro-design/</guid>
      <description>つい先日Goslingsというものを作った。 Gitのリポジトリの中身をビジュアライズするWebアプリケーションだ。 なんとなく見て楽しいという効用がある他は、Gitの勉強にちょっと使えるかもしれないという程度のものだが、もともとGit Advent Calendar 2016のネタを作るために作ろうと思ったものなので、とりあえずはこんなものでいいのだ。 将来気が向いたら、リポジトリの変更をリアルタイムに反映したり、リポジトリの操作もできるように拡張してもいいかもしれないけど、実用性が感じられないので多分やらない。
因みに、goslingsというのはgeese(雁)の子供を指す、ちょっとマイナーな英語。

Gitオブジェクトを見るアプリだから、GOで始まる名前にしようかと思っていて、そういえば今住んでいるFort Collinsに大量にいるgeeseの子供がgoslingsというし、並んで歩いている姿がちょうどコミットグラフのようだと思い、Goslilngsと名付けた。 単数形だとカナダのイケメン俳優かと思われてしまうので、複数形にした。goslingが一人でいることってないし。
GoslingsはSpring BootやJGitなどの習作でもある。 学んだことはアプリケーションとしてアウトプットするとよく身に付くものだ。 また文章としてもアウトプットしておくとさらによく身に付き、備忘録にもなるので、Goslingsの開発メモをいくつかのエントリに分けて書いていくことにする。
まずはSpring Boot編を書こうかと思うが、その前にGoslingsの設計等について書いておく。

   (adsbygoogle = window.adsbygoogle || []).push({});  Goslingsのアーキテクチャ GoslingsはWebサーバとして動き、始めにクライアントにHTML文書を返した後は、REST APIサーバとして働く。
サーバ側はJavaでできていて、Spring BootとJGitを使っている。 JGitを使いたかったのでJavaにしたが、そうでなければNodeで書きたかった。
因みに、今回はコーディングの詳細にあまりこだわらないつもりだったので、Lombokで楽をしようかと思ったけど、うっとうしいバグを踏み、どうやっても回避できなかったので使うのやめた。 二度と使うまい。
クライアント側はJavaScript(ES2015 + async/await)のSPAで、禁jQuery縛り。 React + Reduxというのをやってみたかったが、なんか大げさだしそこまで時間がとれなそうだったので、フレームワークなしで作った。ので、 「You Don&amp;rsquo;t Need jQuery」とにらめっこしながら書いた。
Gitのコミットグラフの描画には、vis.jsを使った。 Stack Overflowの回答から雰囲気で選んだけど、やりたかったことが全部できて、見た目もよかったのでよかった。
サーバはDockerで動かすためにステートレスに作ったつもりで、後述の作業ディレクトリをコンテナ間で共有し、サーバの負荷に応じてコンテナを増やしたり減らしたり、簡単にスケールするようになっているはず。
Goslingsの機能設計 Goslingsサーバにブラウザでアクセスすると、まず参照したいGitリポジトリのURIを入力するフォームが表示される。 ここにはローカルにあるリポジトリへのファイルシステム上のパス(e.g. C:\repos\project-hoge\.git)か、リモートにあるリポジトリのURL(e.g. https://repos.foo.com/project-hoge.git)を入力できる。

URIを入力してBrowseボタンを押下すると、Goslingsの作業ディレクトリ(デフォルトではtmpディレクトリの下のgoslings)に、ローカルリポジトリの場合はそこへのsymlinkを、リモートリポジトリの場合はベアなクローンを作成する。 いずれの場合にも、正規化したURIから生成したUID(SHA-1ハッシュ)をsymlinkファイル名とクローンディレクトリ名に使う。 サーバはリポジトリの準備ができたら、そのUIDをトークン(i.e. リポジトリ引換券)としてクライアントに渡す。 クライアントはそのトークンを使って、リポジトリの情報をサーバに要求する。
こうすることで、以下の様に後でリポジトリを取り扱いやすくなる。
 クライアントやサーバは、可変長の長ったらしい特殊文字の含まれたURIの代わりに、40文字の数字とアルファベットだけで構成されたトークンでリポジトリを特定でき、処理がしやすい。 後でサーバがリポジトリにアクセスする際、ローカルとリモートを区別する必要がないので、処理がしやすい。 サーバ内部でリポジトリというエンティティを扱う際、リポジトリに直接触るデータレイヤと、クライアントからのリクエストをさばくインターフェースレイヤとの間で、単なる文字列であるトークンをやりとりすればよく、データレイヤの実装の詳細をインターフェースレイヤに曝さなくてよくなり、レイヤをきれいに分離できる。これはJavaのインターフェースを作ってやってもできるが、インターフェースのAPIを考える手間を考えるとトークンの方が楽。  クライアントはトークンを受け取ったらコミットグラフビューに遷移する。</description>
    </item>
    
    <item>
      <title>Currently Pcap4J Doesn&#39;t Work on Bash on Windows</title>
      <link>https://www.kaitoy.xyz/2016/11/19/pcap4j-doesnt-work-on-bow-yet/</link>
      <pubDate>Sat, 19 Nov 2016 11:41:07 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/11/19/pcap4j-doesnt-work-on-bow-yet/</guid>
      <description>TL;DR I&amp;rsquo;ve attempted to run Pcap4J on Bash on Windows (BoW) but it didn&amp;rsquo;t work due to lack of support for network staff in BoW.

   (adsbygoogle = window.adsbygoogle || []).push({});  What&amp;rsquo;s Bash on Windows Bash on Windows is a feature released in Windows 10 Anniversary Update to add Linux fanctionalities to Windows.
With this feature, we can run Bash and several Linux commands on Windows.</description>
    </item>
    
    <item>
      <title>Bash on WindowsでWindows側からUbuntu側のファイルをいじると壊れることがあるので注意</title>
      <link>https://www.kaitoy.xyz/2016/11/19/bow-do-not-change-linux-files-from-windows/</link>
      <pubDate>Sat, 19 Nov 2016 01:05:26 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/11/19/bow-do-not-change-linux-files-from-windows/</guid>
      <description>Bash on WindowsでWindows側からUbuntu側のファイルをいじると危険という情報を見つけたので、試してみたら確かに困った状態になった話。

   (adsbygoogle = window.adsbygoogle || []).push({});  Bash on Windowsとは Bash on Windows (aka BoW)は、2016/8/3に公開されたWindows 10 Anniversary Updateで使えるようになった、Windows上でBashが使えるようになる機能。
POSIX APIのWindows実装を提供するCygwinなどとは違い、WindowsのサブシステムとしてUbuntuが動き、その上でBashが動き、そこからUbuntu用のバイナリをそのまま利用できるというもの。
2016/11/17現在でまだベータ版の機能。
Windows側からUbuntu側のファイルをいじると壊れる問題 Microsoftの中の人のブログに、BoWがセットアップされた環境で、Windows側からUbuntu側のファイル(i.e. %localappdata%\lxss\以下のファイル)をいじると壊れるという話があった。 いかにもやってしまいそうな操作で危険だし、実際このブログの人はこれに関する問い合わせに毎日1,2件対応しているそうな。
原因は上記ブログに詳しいが、簡単に言うと、Windows側のプロセスがUbuntu側のファイルを作ったり編集したりする際、パーミッションなどのメタデータを適切に設定しないため、Ubuntu側でファイルが壊れたと判断されてしまうから。 こうなると、結果としてファイルが消えてしまったり、壊れたデータで上書きされてしまったりするとのこと。
因みに、Ubuntu側からWindows側のファイルをいじるのは問題ないらしい。
再現確認 そういえばまだBoWをさわったことがなかったので、セットアップして件の問題を体験してみた。
環境は、VMware Player 7.1.0で作ったVMに評価版のWindows 10 Enterprise v1607をインストールしたもの。 セットアップは公式の手順に従うだけ。2ステップだけの簡単な手順。
セットアップ後、コマンドプロンプトでbashとうつとBoWが起動する。(初回はインストール処理が走り、十数分待たされる。)
[コマンドプロンプト → Bash] 
再現確認に使うのはhogeと書いたhoge.txt。 これをWindows側のC:\Users\kaitoy\Desktop\とUbuntu側の/home/kaitoy/に置く。
[コマンドプロンプト]
[Bash]

Windows側からは、Ubuntuのファイルシステムが%localappdata%\lxss\にマウントされているように見える。 (lxssはエクスプローラーのオプションから「保護されたオペレーティングシステムファイルを表示しない（推奨）」のチェックをはずさないと見えない。見えなくてもアドレスバーにパスを入力すればアクセスできるけど。)

一方Ubuntu側からは、WindowsのCドライブが/mnt/cにマウントされているように見える。
[Bash]

ここで、コマンドプロンプトを開き、%localappdata%\lxss\hoge\kaitoy\(i.e. Ubuntu側の/home/kaitoy/)にcdし、hoge.txtをechoで編集してみた。
[コマンドプロンプト]

したらBashから見えなくなった。アクセスしようとすると「Input/output error」というエラーになる。これが件の現象か。
[Bash]
エクスプローラからは見えていたので、GUIで%localappdata%\lxss\hoge\kaitoy\hoge.txtを削除したら正常な状態に戻った。

再度同じhoge.txtを作り、今度はメモ帳で編集して内容をfooに変えてみた。 この場合は特に問題なし。なぜだ?</description>
    </item>
    
    <item>
      <title>git checkoutを図解する</title>
      <link>https://www.kaitoy.xyz/2016/10/08/git-checkout/</link>
      <pubDate>Sat, 08 Oct 2016 16:39:46 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/10/08/git-checkout/</guid>
      <description>この記事を読んだ、またはGitのオブジェクトモデルを理解していることを前提に、Gitの git checkout というコマンドについて説明する。
このコマンドは普通ブランチを切り替えるものと説明されるが、主たる機能は オブジェクト格納領域から指定されたファイルを取り出し、ワーキングディレクトリに配置する ものである。 つまりこれがGitにおけるチェックアウトで、チェックアウト=ブランチの切り替えではない。
コマンドに与える引数によっては HEAD の付け替え、つまりはブランチの切り替えもする、というだけ。
git checkout の動作を HEAD の付け替えの有無によって分けて考えると分かりやすく覚えやすいので、以下そのように説明する。

   (adsbygoogle = window.adsbygoogle || []).push({});  HEADを付け替えないgit checkout HEAD を付け替えない git checkout は、引数にワーキングディレクトリ内の ファイルまたはディレクトリへのパスを与えた場合 のもの。 ディレクトリを指定した場合はそれ以下の全ファイルが操作対象となる。 パスは絶対パスかカレントディレクトリからの相対パスで、複数指定できる。
つまりは以下の様なコマンド形式になる。
git checkout &amp;lt;パス(複数可)&amp;gt;
これを実行すると、指定したファイルについて、インデックスが指しているブロブ をオブジェクト格納領域から取り出し、ワーキングディレクトリのファイルを置き変える。
        
上のスライドではインデックスが指しているブロブを取り出したが、任意のブロブを取り出すこともできる。 この場合、以下の様なコマンド形式を使う。
git checkout &amp;lt;コミット&amp;gt; &amp;lt;パス(複数可)&amp;gt;
このコマンド形式だと、指定したコミットが指すツリー以下のブロブ が取り出される。 &amp;lt;コミット&amp;gt;の部分には、コミットオブジェクトのSHA1ハッシュ値、参照(i.e. ブランチかタグ)、シンボリック参照(e.g. HEAD)を指定できる。(実際にはこれらが全てではないが、実用的にはこの3種。)
この形式だと、ワーキングディレクトリだけでなく、取り出すブロブを指すよう インデックスも更新される ことに注意。</description>
    </item>
    
    <item>
      <title>Gitの良さが分からない？ ちょっとそこに座れ</title>
      <link>https://www.kaitoy.xyz/2016/10/06/git-vs-subversion/</link>
      <pubDate>Thu, 06 Oct 2016 00:18:05 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/10/06/git-vs-subversion/</guid>
      <description>Gitの良さがいまだに分からないという人がいるようなので、Git派の一人としてSubversion(以下SVN)と比較してのGitの良さ(メリット)について語りたい。 (GitとSVNの違いについては他の人の記事に詳しいのであまり書いていない一方、勢い余ってGitのデメリットも書いた。)

   (adsbygoogle = window.adsbygoogle || []).push({});  本題に入る前に、冒頭にリンクを貼った記事についてひとつだけつっこんでおく。 つっこみどころは他にも沢山あるけど。
 ※話の前提としてgitとSVNを採用している現場に下記のような割と違いがあるとする。
git イシューごとにブランチを切り、ローカルでコミットして、リモートブランチにpushして、GitHub・GitLab・Bitbucket経由でマージリクエスト。コードレビューの後にマージ。
SVN リモートのtrunkに個々人が直接コミット。コードレビューはあまりない。ブランチを切ることもない。
このような違いが出る背景には次のものがある。
gitを採用する現場は、猫も杓子もgit-flowというプラクティスに従う傾向がある gitを採用する現場は、コードの品質もある程度管理する傾向がある SVNは集中型でありブランチ機能などが非常に使いにくい SVNを採用する現場はコードの品質よりも「リリースに含めるならさっさとコミット」と考える傾向がある   この前提には無理がある。
Gitのところに書いてあるのが、Gitというツールの枠を大きくはみだしたGitHub Flowというブランチ戦略+開発プロセスに当たるものであり、 それでGitを批判するのはお門違いであろうという点については、Gitの流行がGitHubの人気によるところが大きく、GitHubを使えることがGitの大きなメリットであるので、目をつむることにする。(マージリクエストを使う羽目になるデメリットなんて言いがかりでしかないとだけ言っておく。)
看過できないのは、SVNを使った開発がコードレビューもブランチもないという点。
どこの世界の話をしているんだろうか。 Gitが世に出る前は世間にコードレビューもブランチもあまりなかったかのような前提だが、もちろんそんなことは全くない。 60万個以上のOSSプロジェクト情報を統括するOpen HUBによれば、OSSプロジェクトの46%がSVNを使っている。この中にはGitの誕生以降にSVNを使い始めたプロジェクトも多くある。270000余りのプロジェクトの大部分がブランチすら使っていないとでも?
GitHub Flowと対比するために無理やりこじつけたんだろうけど、その無理のせいで議論のスタート地点からめちゃくちゃだ。
まともな開発にはコードレビューもブランチも必要だ。 品質管理もリリース管理もしないなら要らないのかもしれないが、そんないい加減な開発現場を前提にSVNかGitかなんて議論しても意味がない。 高品質なソフトウェアを効率よく開発するために則りたい素晴らしい開発フローがあるとして、そのフローをSVNやGitやその他のツールないしひょっとしたらアナクロな日付フォルダの内どれがもっとも上手く実現してくれるか、というのがあるべき議論だ。 この「素晴らしい開発フロー」には一般的に品質管理と並行開発が含まれていて、それらにはコードレビューとブランチの利用が含まれている。 Git(+GitHub)がこんなにも急速にSVNに取って代わって流行ったのは、分散リポジトリの仕組みとブランチの軽量な実装によって効率的な並行開発が実現でき、またプルリクエストなどの機能によりコードレビューを含む快適なソーシャルコーディングが実現できるからだ。 逆に言えば、Gitが流行ったことが、人々が効率的な並行開発やコードレビューを開発フローに取り入れたかった証拠と言えるかもしれない。
Gitのメリット 前置きが長くなったが、少なくともブランチとコードレビューを活用した高品質で高効率なソフトウェア開発をしたいという前提で、SVNに対するGitのメリットを挙げてみたい。
1. リポジトリ構造がシンプル Gitリポジトリはすごくシンプルに作られているそうな。 確かに、その構造を見ると、add、commit、log、resetくらいは自前ですぐに実装できそうだ。
このシンプルな構造のおかげで、Gitリポジトリは壊れにくい。ここで壊れにくいとは、リポジトリ内部で不整合が起こりにくいということで、コマンドミスでコミット履歴が一部消えたりとかいうトラブルは壊れるに入らない。
実のところSVNリポジトリの構造を知らないので経験的なことしか言えないが、SVNリポジトリ(というより作業ディレクトリの管理情報?)はちょくちょく変な状態になり、クリーンアップしたり、酷い時には.svn内のファイルを手動でいじったりしなければならなかった。
因みに、シンプルというのはリポジトリサイズがすごく小さいということにはならず、同等の履歴を含むGitリポジトリとSVNリポジトリはだいたい同サイズなんだそうな。
2. ブランチが軽い Gitのブランチは単一のコミットを指す参照で、リポジトリ内ではSHA-1ハッシュ値が書かれただけのたった一つのファイルに過ぎない。 その為ブランチは一瞬で作成できるし、ディスクも圧迫しないので、じゃんじゃん作ってじゃんじゃん消せる。 さらに、ローカルリポジトリに過去の全ファイルの全バージョンが入っているという分散リポジトリの特長のおかげで、ブランチの切り替えも軽快にできる。 ローカルから必要なファイルを作業ディレクトリに展開するだけなので。
一方SVNはそもそもブランチをサポートする直接的な機能がないため、ブランチはリビジョンのコピーという形で実装されている。 コピーと言ってもハードリンクみたいなものでディスク上に物理的なコピーが作られるわけではなく、軽量という点ではGitと大差ないが、集中リポジトリなせいでブランチの切り替えには差が出る。 svn switchにしろsvn checkoutにしろネットワークの向こうのサーバとの通信が必要なので、それなりの時間がかかるし、通信が途切れると切り替えられなくなる。
冒頭に貼った記事にはGitはブランチを切り替える際にstashとかしないといけなくて面倒とあったが、そんなのSVNだって同じだし、stashすればいいだけだし、stashという機能があるだけSVNよりまし。Gitならコミットはあとから書き変えられるので、stashの代わりに一時的にコミットしちゃってもいい。
それも嫌ならworktree使えばよろしい。
3. バージョン間の差分取得が速い Gitは全てのファイルについて全てのバージョンのコンテンツをまるまるリポジトリに持っている。 一方SVNのリポジトリにはバージョン間の変更が記録されている。 このため、あるファイルについて任意のバージョン間の差分を取るのに、Gitはシンプルにそれぞれのバージョンのファイルを取り出して比較するだけでよいが、SVNは隣り合ったバージョンでなければバージョン間の変更を足し合わせて差分を計算しなければいけない。</description>
    </item>
    
    <item>
      <title>Pcap4J on Nano Server on Hyper-V Containers on Windows 10 on VMware Playerにトライ</title>
      <link>https://www.kaitoy.xyz/2016/09/15/pcap4j-on-hyper-v-container-on-win10/</link>
      <pubDate>Thu, 15 Sep 2016 13:56:35 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/09/15/pcap4j-on-hyper-v-container-on-win10/</guid>
      <description>Pcap4Jが動くHyper-VコンテナをWindows 10上でビルドしようとしたけど3合目あたりで息絶えた話。

   (adsbygoogle = window.adsbygoogle || []).push({});  Hyper-V Containersとは Hyper-V Containersは、MicrosoftによるWindowsネイティブなコンテナ技術であるWindows Containersの一種で、これによるコンテナは、同じくWindows Containersの一種であるWindows Server Containersのものに比べて、より厳密に隔離されている分、起動コストが高い。
実体はDockerそのもので、コンテナイメージはDocker Hubからpullできるし、コンテナの操作や管理はdockerコマンドでやる。(昔はコンテナ操作用PowerShellコマンドレットもあったが、不評だったので廃止したようだ。) ソースもLinuxとWindowsで一本化されている。
Windows 10のAnniversary Updateで正式にリリースされたが、なんだかあまり注目されていない気がする。
Docker for Windowsとは全く別物なので注意。
Hyper-V Containersのインストール (on VMware Player) 自前のPCが5年前に買ったdynabookでWindows 10をサポートしていないので、VMware PlayerのVM上のWindows 10にHyper-V Containersをインストールしてみる。
VMは、Windows 7に入れたVMware Workstation 11.1.0 build-2496824に付属の VMware Player 7.1.0 build-2496824で作ったもの。 VMのバージョンは11.0。 2CPUでメモリは2.5GB。 ネットワークインターフェースはNAT。 このVMを、Hyper-Vが使えるように設定しておく。
この記事にしたがい、Windows 10の評価版をダウンロード。 今公開されている評価版はAnniversary Update適用済みのバージョン1607で、Hyper-V Containersをサポートしている。
これをさっき作ったVMにインストール。
Windows 10を起動し、以下、Windows Containers on Windows 10に従って進める。
 containers機能有効化
PowerShellプロンプトを管理者権限でひらき、以下のコマンドでcontainers機能を有効化。
Enable-WindowsOptionalFeature -Online -FeatureName containers -All 1分程度経つと再起動を促されるので再起動。</description>
    </item>
    
    <item>
      <title>Hyper-Vコンテナ(Nano Server)でunzipしたいならjarを使え</title>
      <link>https://www.kaitoy.xyz/2016/09/12/unzip-on-nanoserver/</link>
      <pubDate>Mon, 12 Sep 2016 16:46:54 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/09/12/unzip-on-nanoserver/</guid>
      <description>Nano Serverでunzipしたかっただけだったのに、妙に苦労した話。

   (adsbygoogle = window.adsbygoogle || []).push({});  Nano Serverとは Nano Serverは、Windows Server 2016で追加されるWindows Serverの新たなインストール形式で、Server Coreよりさらに機能を絞り、リモートで管理するクラウドホストやWebサーバ向けにに特化したもの。
Server Coreが数GBくらいなのに対し、Nano Serverは数百MBととても軽量で、それゆえ起動が速くセキュア。
unzipとは unzipとは、[zip](https://ja.wikipedia.org/wiki/ZIP_(%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%83%95%E3%82%A9%E3%83%BC%E3%83%9E%E3%83%83%E3%83%88)ファイルを解凍する、ただそれだけのこと。
ただそれだけのことで、基本的な機能だと思うのだが、Windowsはこれをコマンドラインで実行する方法をつい最近まで正式に提供していなかった。
Nano Serverでunzip Windows 10のHyper-V Containersの上でPcap4JのビルドとテストをするDockerイメージをビルドしたくて、そのための依存ライブラリなどをインストールする処理をDockerfileに書いていて、ADDでzipをダウンロードしたところまではいいんだけど、このzipどうやって解凍してやろうかとなった。 (Dockerホストに置いたものをコンテナにADDするのはなんか格好悪いから無しで。Dockerfile裸一貫で実現したい。)
Windows 10のHyper-V Containersは、現時点でNano Serverしかサポートしていないのが厳しい点。Server Coreだったら楽だったのに。

以下、いろいろ試したことを書く。
正攻法: Expand-Archive PowerShellの v5 で実装されたExpand-Archiveというコマンドレットでzipを解凍できる。 Nano ServerのPowerShellのバージョンを確認したら 5.1 だったのでこれでいけるかと思った。
C:\&amp;gt;powershell -command &amp;#34;$PSVersionTable.PSVersion&amp;#34; Major Minor Build Revision ----- ----- ----- -------- 5 1 14284 1000 
したらこのエラー。
Add-Type : Cannot find path &amp;#39;C:\System.</description>
    </item>
    
    <item>
      <title>オープンソースプロジェクトのすゝめ</title>
      <link>https://www.kaitoy.xyz/2016/08/21/an-encouragement-of-open-sourcing/</link>
      <pubDate>Sun, 21 Aug 2016 20:54:12 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/08/21/an-encouragement-of-open-sourcing/</guid>
      <description>人は生まれながらにして貴賤の別なく、ただオープンソースプロジェクトを勤めて物事をよく知る者が貴人となるなり。
昔、偉い人がそんな感じのことを言っていたような。
私がGitHubで開発しているライブラリ、Pcap4J のスターの数がつい先日 200 に達したのを記念して、これまでどんな活動をしてきたか、この活動によって何を得たかなどについて書きたい。
願わくは、この記事に触発されてオープンソースプロジェクトを始める人のあらんことを。

   (adsbygoogle = window.adsbygoogle || []).push({});  Pcap4Jとは？ Pcap4Jは、パケットキャプチャとパケット解析をするJavaのライブラリ。 ニッチ。
ただ最近になってビッグデータ解析技術が発達し、大量のパケットをリアルタイムで解析してシステムや運用にフィードバックするというのが現実的になってきたので、パケットキャプチャへの注目が高まってきている雰囲気がある。 こういう分野ではJavaがまだかなり人気なのもあってワンチャンある。
パケットキャプチャの部分は pcap のラッパ。 パケット解析の部分は割とプラガブルで、外からプロトコル追加などのカスタマイズができるはできるんだけど、作りのせいなのかJavaなせいなのか解析器を書くのが結構つらい。
競合は jpcap や jNetPcap など。 Google.comでjava packet captureと検索するとだいたいjpcap、Pcap4J、jNetPcapの順で表示される。
打倒jpcap。
数字で見るPcap4Jプロジェクト Pcap4Jリポジトリの一番古いコミットは 2011/12/18。 東日本大震災後の節電施策として実施された休日シフト中にコーディングしていた覚えがあるので、多分2011年夏くらいから開発していたんだけど、とりあえずこの最古のコミットをプロジェクトの開始とすると、スターが200になった 2016/8/11 まで 1698日 かかったことになる。 約 0.118個/日。遅い…
コミット数は 559個。ほとんどが自前のコミット。 プロジェクト成長過程の動画を Gource というツールで生成してみたが、一人でかけずりまわっているのがよく分かる。
  コミット頻度は約 0.33個/日 で、だいたい3日に1コミット。 思っていたより多いけど、胸張れるほどの頻度ではない。
リリースは 17個 で、約 0.30個/月。少ない…
Issuesが 52個、プルリクエストが 16個。 自分ではIssuesもプルリクエストもあまり作らないので、ほとんどが他人からのもの。 ちゃんとチケット駆動にしてトレーサビリティを確保しておくべきだったと後悔している。 けど面倒だし今更なので今後も適当にコミットしちゃう。
あとはWatchが 28人、Forkが 66個、コントリビュータが 7人。</description>
    </item>
    
    <item>
      <title>GitHub Pagesの新機能、ソース設定が地味にいい</title>
      <link>https://www.kaitoy.xyz/2016/08/18/simpler-github-pages-publishing/</link>
      <pubDate>Thu, 18 Aug 2016 00:26:06 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/08/18/simpler-github-pages-publishing/</guid>
      <description>今日、よりシンプルにGitHub Pagesを使えるようになったというアナウンスがあり、ソース設定という新機能が追加されていたので、さっそく試してみた話。

   (adsbygoogle = window.adsbygoogle || []).push({});  GitHub Pagesの新機能: ソース設定 GitHub PagesにはUser Pages、Organization Pages、Project Pagesの三種類があるが、ソース設定が使えるのはProject Pages、つまりGitHubリポジトリごとに使えてusername.github.io/projectnameのようなURLのやつだけ。
今まではProject Pagesで公開するサイトのソースはgh-pagesという名のブランチに置く必要があったが、ソース設定によりmasterブランチのルートに置いたりmasterブランチの/docsフォルダに置いたりもできるようになった。
ソース設定の使い道 Pcap4Jのホームページのソースをmasterブランチの/docsフォルダに置く設定にしたら捗った。
Pcap4JのホームページはHugoで作っていて、以前は、Hugoのソースをpcap4j-hpリポジトリのmasterブランチに置き、gh-pagesブランチを作ってそこにHugoのビルド成果物(=ホームページのソース)を入れていた。
ローカルPCでは、masterをcloneして、そこからgit worktreeでgh-pagesを別のフォルダにチェックアウトしておいてあり、Hugoのビルドオプションでgh-pagesのフォルダにビルド成果物を出力するようにしていた。 これだと、ホームページを修正したい場合、まずmasterでHugoソースを修正してgit add/commit/push、次いでビルドしてgh-pagesフォルダに移動してgit add/commit/push、というように、二度手間で面倒だった。
Hugoのビルド成果物をmasterブランチの/docsフォルダに置けるようにできれば、git add/commit/pushはビルド後にmasterに対して一回だけやれば済むようになる。
gh-pagesからmasterブランチの/docsフォルダへの移行 GitHubのヘルプを参考にしつつ、
 ローカルPCで、masterの作業ディレクトリのルートにdocsというフォルダを作り、gh-pagesのフォルダの中身を全てそこに移動。 masterのdocsをgit add/commit/push。 GitHubのpcap4j-hpリポジトリのページに行き、SettingsタブのGitHub PagesセクションのSourceをgh-pages branchからmaster branch /docs folderに変えてSaveボタンをクリック。
  
実にこれだけ。 カスタムドメインにしていてもこれだけ。簡単。ダウンタイムもなし。
あとはローカルPCのgh-pagesの作業ディレクトリを削除したり、gh-pagesブランチを削除したり。</description>
    </item>
    
    <item>
      <title>Docker for Windowsがコレジャナかった</title>
      <link>https://www.kaitoy.xyz/2016/07/31/docker-for-windows/</link>
      <pubDate>Sun, 31 Jul 2016 14:34:16 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/07/31/docker-for-windows/</guid>
      <description>7/28にDocker for Winodws(とDocker for Mac)の正式版リリースのアナウンスがあったので試してみたけど、期待していたものと違ったしなんだか上手く動かなかった話。

   (adsbygoogle = window.adsbygoogle || []).push({});  Docker for Windowsとは Docker for WindowsはDocker Toolboxの後継製品。(多分。)
Docker ToolboxはWindowsやMacでDockerを使うための製品で、以下のコンポーネントからなる。
 Docker Engine
コンテナランタイム。
 Docker Compose
複数のコンテナを組み合わせたアプリケーション/サービスの構築/管理ツール。
 Docker Machine
Docker仮想ホストのプロビジョニング/管理ツール。
 Kitematic
Dockerコンテナを管理するGUIを提供する製品。 Docker Machineと連携してローカルマシンへのDocker仮想ホストのプロビジョニングもしてくれる。
  Docker Toolboxを使うと、VirtualBoxのLinux VMをWindows/Mac上にプロビジョニングして、そのVMにDockerをインストールして、Windows/Macから利用できる。
Docker for Windowsもだいたい同じで、Docker EngineとDocker ComposeとDocker MachineをWinodwsで利用するための製品。 ElectronベースでOracleのVirtualBox依存なKitematicの代わりに、ネイティブなインストーラがWindows内蔵のHyper-Vを使ってDockerをセットアップしてくれる。 Hyper-Vを使うため、VirtualBoxより速くて高信頼らしい。 KitematicはDocker for Windowsには付属しないが、別途ダウンロードすればコンテナ管理に使える。Docker for WindowsとDocker Toolboxとは共存はできない。
私は勝手にDocker for WindowsはHyper-V ContainersのデスクトップOS版のようなものかと勘違いしていて、Windowsのコンテナが使えるようになったのかと期待したが違った。 Docker for Windowsは単にDocker ToolboxのVirtualBoxがHyper-Vになっただけのもので、結局Linux VMの中でDockerを使うだけのものだということにセットアップ中に気付いた。</description>
    </item>
    
    <item>
      <title>Windows Server 2016 TP5でWindows Containersにリトライ</title>
      <link>https://www.kaitoy.xyz/2016/07/11/windows_containers_on_tp5/</link>
      <pubDate>Mon, 11 Jul 2016 00:30:33 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/07/11/windows_containers_on_tp5/</guid>
      <description>Windows Server 2016のTechnical Preview 5(TP5)が公開されていたので、 TP4でバグに阻まれて挫折した、Windows ContainersでPcap4Jを使ってパケットキャプチャする試みにリトライした話。

   (adsbygoogle = window.adsbygoogle || []).push({});  OSセットアップ TP4のときと同じ環境。
以降はWindows Server Containersのクイックスタートガイドに沿ってセットアップを進める。 TP4からは大分変わっていて、単一のPowershellスクリプトを実行する形式から、Powershellのコマンドレットを逐次手動実行する形式になっている。 面倒だけど何やってるかわかりやすくて好き。
コンテナ機能のインストール  管理者権限のパワーシェルウィンドウを開く
コマンドプロンプトから以下のコマンドを実行。
powershell start-process powershell -Verb runas コンテナ機能のインストール
開いた青いパワーシェルウィンドウで以下のコマンドを実行するとコンテナ機能がインストールされる。
Install-WindowsFeature containers 数分で終わる。
インストールされたのはHyper-V ContainersじゃなくてWindows Server Containersの方。 クイックスタートガイドをみると、前者がWindows 10向け、後者がWindows Server向けというように住み分けされているっぽい。TP4では両方ともWindows Serverで使えたんだけど。
 再起動
変更を有効にするために再起動が必要。
Restart-Computer -Force  Dockerインストール Dockerは、コンテナイメージの管理やコンテナの起動などもろもろの機能を提供するDockerデーモンと、その機能を利用するためのCLIを提供するDockerクライアントからなる。この節ではそれら両方をインストールする。
 Dockerインストールフォルダ作成
管理者権限のパワーシェルウィンドウを開いて、以下のコマンドでDockerインストールフォルダを作成。
New-Item -Type Directory -Path &amp;#39;C:\Program Files\docker\&amp;#39; Dockerデーモンインストール
まずはデーモンの方をインストール。
Invoke-WebRequest https://aka.ms/tp5/b/dockerd -OutFile $env:ProgramFiles\docker\dockerd.exe -UseBasicParsing 数分。</description>
    </item>
    
    <item>
      <title>CloudflareでブログをHTTPS化</title>
      <link>https://www.kaitoy.xyz/2016/07/01/https-support-by-cloudflare/</link>
      <pubDate>Fri, 01 Jul 2016 14:17:41 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/07/01/https-support-by-cloudflare/</guid>
      <description>最近GitHub PagesがHTTPSに正式対応したというニュースを見たことをきっかけに、このブログをCloudflareで常時HTTPS化した話。

   (adsbygoogle = window.adsbygoogle || []).push({});  このブログ このブログはGitHub Pagesでホストされている。 GitHub Pages上のWebサイトはデフォルトでは&amp;lt;GitHubユーザ名&amp;gt;.github.ioというドメインで公開されるが、ちょっとかっこつけたかったのでカスタムドメイン(www.kaitoy.xyz)にした。
GitHub Pagesは2014年3月から非公式にHTTPSをサポートしていて、2016年6月8日に正式サポートを表明したが、これは&amp;lt;GitHubユーザ名&amp;gt;.github.ioドメインだけが対象であり、カスタムドメインはHTTPSサポートされていない。
(2018/5/3追記: 2018/5/1にGitHub PagesのカスタムドメインのHTTPSサポートが発表された)
要するにこのブログにはHTTP接続しかできない状態だった。 これをなんとかHTTPSに対応させたかった。
なぜHTTPS HTTPS化(常時SSL化)が世界的な流行りな雰囲気を感じていたのと、なにより、Googleに優遇してもらえるから。 Googleの検索結果の2,3ページ目までに出てこないなら、そのサイトはこの世に存在しないのとあまり変わらない。
昔はHTTPSにするとSSLプロトコルのオーバーヘッドや暗号化/復号化処理によりHTTPに比べて遅くなると言われていたが、最近ではサーバ/クライアントマシンの性能が上がり、このデメリットは気にするほどのものではなくなった。 逆に、常時SSL化するとSPDYやHTTP/2といった高速なプロトコルの恩恵を受けることができるようになり、HTTPより速くなることもあるらしい。
カスタムドメインなGitHub PagesサイトをHTTPS対応する方法 上記の通りこのブログはカスタムドメインでGitHub Pagesのサポートがなく直接にはHTTPS対応できない。 よって間接的に対応することになるので、リバースプロキシを使うことになる。 リバースプロキシサーバを自分で運用するのは大変なので、CDNサービスを利用する。
CDNサービスでまず思い当たったのはAWSのCloudFrontだけど、なんだか大げさで面倒そう。 あとはCloudflareが有名なので調べたところ、手軽で無料でよさそうだったのでこれにした。
因みに、ごく最近始まったサービスのKloudsecというのも見つけたけど、まだベータが付いているし、遅いだのそもそもつながらないだの評判が悪かったのでこれは無し。
Cloudflareを利用すると、もともとだいたいこんな感じ↓だったのが、
          こんな感じ↓になる。多分。
            上のスライド中のリバースプロキシは実際にはいくつもあり、エニーキャストによってブラウザから一番近いものが使われる。
Cloudflare事始め Cloudflareの始め方はQiitaの記事を参考にした。
 Cloudflareのアカウント作成
Cloudflareのサイトに行ってSign upのリンクからメアドとパスワードを渡してアカウントを作成。</description>
    </item>
    
    <item>
      <title>ソフトウェアプロジェクトの7つの大罪</title>
      <link>https://www.kaitoy.xyz/2016/06/25/seven-deadly-sins-of-a-software-project/</link>
      <pubDate>Sat, 25 Jun 2016 18:00:29 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/06/25/seven-deadly-sins-of-a-software-project/</guid>
      <description>このエントリでは、Yegor Bugayenkoによる記事、Seven Deadly Sins of a Software Projectを紹介する。 (Yegorから和訳と転載の許可は得た。) 以下はその全文の和訳だが、意訳超訳が混じっているので、もとのニュアンスを知りたければ元記事を読んでもいいし、読まなくてもいい。

   (adsbygoogle = window.adsbygoogle || []).push({});  保守性は近代ソフトウェア開発において最も重要な美徳だ。 保守性は基本的に、新規開発者が本格的な修正を始める前に必要な学習時間で測ることができる。 学習時間が長いほど保守性は低い。 必要な学習時間が無限に近いプロジェクトもあるが、これは文字通り保守不能だ。 私はソフトウェアを保守不能にする7つの基本的で致命的な罪があると考えている。 それらについてここに書く。
アンチパターン 不幸にも、我々が使っているプログラミング言語は柔軟すぎる。 可能なことが多過ぎ、禁止されていることは少なすぎる。 例えばJavaは、数千のメソッドを持った単一の「クラス」でアプリケーション全体を記述することに何の反抗もしない。 このアプリケーションは技術的にはコンパイルして実行できる。 しかしこれはゴッドオブジェクトと呼ばれるよく知られたアンチパターンだ。
つまり、アンチパターンは技術的には設計に取り入れることができるが、一般的には取り入れるべきではないとされている。 言語ごとに多くのアンチパターンがある。 プロダクトに使われているアンチパターンは、生きている有機体の中の腫瘍のようなものだ。 いったん成長し始めると止めるのは非常に難しい。 やがて体全体が死に至る。 やがてソフトウェア全体が保守不能になり、書き直さなければならなくなる。
ひとたびアンチパターンを使ってしまうと、その量は次第に増え、「腫瘍」は育つばかりだ。
これは特にオブジェクト指向言語(Java、C++、Ruby、Python)に当てはまる。 これらが手続き型言語(C、Fortran、COBOL)から多くを引き継いでしまっているからだ。 また、OOP開発者が手続き型で命令的な思考をする傾向にあるからだ。残念なことに。
ところで、既存の有名なアンチパターンのほかに、私は以下のものもダメなコーディング法だと考えている。
 NULL参照 ユーティリティクラス 可変オブジェクト GetterとSetter オブジェクト関係マッピング(ORM) シングルトン Controllers、Managers、Validators Public Static メソッド キャスト  私ができる実践的な提案は、読んで学ぶということだけだ。 ここに挙げた本か私の著書「&amp;ldquo;Elegant Objects」が多分助けになるだろう。 常にソフトウェアの品質を疑い、「動く」ということだけで満足してはいけない。 ちょうど癌のように、診断が早ければ早いほど生き残る可能性が大きい。
追跡不能な変更 コミット履歴を見るとき、全ての個々の変更に対して、何を、誰が、なぜ変更したのかがわからないといけない。 さらに、これら3つの情報を得るのにかかる時間は秒単位で計測しないといけない。 殆どのプロジェクトがこのようにできていない。 以下に実践的な提案を示す。
常にチケットを使う プロジェクトやチームがどんなに小さくても、例え一人だけでも、修正しようとしている全ての問題に対してチケット(GitHub issues)を作れ。 チケットに問題の簡単な説明とそれに対する考えを記述しろ。 このチケットをその問題に関する全ての情報の一時的なストレージとして使え。 将来、他の誰かがその「不可解なコミット」が何であるかを理解するために参照する可能性のある全ての情報をそこに書け。</description>
    </item>
    
    <item>
      <title>Pcap4J in Kotlin</title>
      <link>https://www.kaitoy.xyz/2016/04/16/pcap4j-in-kotlin/</link>
      <pubDate>Sat, 16 Apr 2016 11:09:53 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/04/16/pcap4j-in-kotlin/</guid>
      <description>Groovyに続いて、KotlinでPcap4Jを使ってパケットキャプチャしてみた。
KotlinからでもPcap4Jちゃんと動くよということを実証するのが主な目的。 また、今後JavaなアプリはKotlinで書こうかと思っているので、その予習も兼ねている。

   (adsbygoogle = window.adsbygoogle || []).push({});  Kotlinとは KotlinはJVM言語、つまりJavaのバイトコードにコンパイルされてJavaの実行環境で動くプログラミング言語のひとつ。 IntelliJ IDEAで有名なJetBrains社によってOSSとして開発されている。
2011年に生まれた新しめな言語で、2016/2/17にv1がリリースされ、主にAndroidアプリの開発用として注目されている。
「実用的」であることを売りにしていて、つまり少ない学習コストで導入でき、既存のJavaコードやMavenなどのツールとの相互運用性を持つとされている。 IntelliJ IDEA、Android Studio、Eclipseといった主要なIDEのサポートもあり、開発環境は整っている。 v1以降の後方互換性の維持も表明されていて、長期サポートが必要な製品開発にも堪える。
さらに、厳格な静的型付けやNullable/Non-Null型などにより安全性を確保しつつ、型推論やラムダ式などで生産性を高めている。
Javaのバイトコードだけでなく、JavaScriptを生成するバックエンドを持っているのも大きな特徴。 ユースケースがよく分からないが。
GitHubにホストされているKotlinプロジェクトは、2016/4/15現在、全体の 0.1% (3493&amp;frasl;3215549) しかない。 v1のリリースは結構注目を集めたので、この割合は今後増えていくと期待される。
Kotlinのインストール チュートリアルに従えば、IDEやテキストエディタ+コマンドラインの環境を整えてHello Worldを書いて実行するところまで簡単にできる。 筆者はEclipse(Mars)とコマンドラインの環境をWindows 7上で作った。 Kotlinのバージョンは1.0.1-2。
コマンドラインについては、GitHub Releasesからアーカイブをダウンロードして、適当なところに展開してbinフォルダにパスを通すだけ。 前提となるJavaについては、環境変数JAVA_HOMEを設定するか、javaコマンドにパスを通せばいい模様。
因みにKotlinの書き方は、Kotlin Koansという例題集をオンラインのIDEで解きながらを学ぶことができる。
パケットキャプチャ with Pcap4J in Java Pcap4Jでパケットキャプチャするコードを普通にJavaで書くと以下の様になる。 (Groovyの時のと一緒。)
 これを実行すると、パケットキャプチャするネットワークインターフェースを選択し、5つのパケットをキャプチャしてタイムスタンプと共にコンソールに表示する。
パケットキャプチャ with Pcap4J in Kotlin 上記処理をKotlinで書くと以下の様になる。
 メインクラスはGroovy同様書かなくていいが、main関数は必要。
型推論があってとても楽。 ラムダ式、補間文字列(String interpolation)、名前付き引数といったモダンめな機能は普通に使える。 (名前付き引数はJavaで書いたメソッドをKotlinから呼ぶときは使えない。)
オープンクラスを実現する機能であるExtensionsをPcapHandleに使ってみた。 なんだか便利そう。
Nullable/Non-Null型がすごい。言語仕様でNullPointerExceptionが発生しないように守ってくれる。 例えばfilterは宣言の時点では初期化文でnullが入る可能性があるのでNullableなStringという型に推論されるが、filter?.letというNullチェックをするメソッドに渡したブロック内では自動でNon-NullなStringにキャストされ、filter.lengthを安全に評価できるようになっている。 Nullチェックをしないでfilter.lengthと書くとコンパイルエラーになる。すごい。
けどJavaのコードから返ってくるオブジェクトは普通、プラットフォーム型というものになり、このNullセーフな仕組みが働かない。 これに対してはNull可能性アノテーションを使えば幸せになれるらしい。</description>
    </item>
    
    <item>
      <title>Pcap4J in Groovy</title>
      <link>https://www.kaitoy.xyz/2016/04/10/pcap4j-in-groovy/</link>
      <pubDate>Sun, 10 Apr 2016 00:05:27 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/04/10/pcap4j-in-groovy/</guid>
      <description>GroovyでPcap4Jを使ってパケットキャプチャしてみた。
GroovyからでもPcap4Jちゃんと動くよということを実証するのが主な目的。 また、さすがにそろそろMavenを卒業してGradle(下記)使おうと思うので、予習も兼ねている。

   (adsbygoogle = window.adsbygoogle || []).push({});  Groovyとは GroovyはJVM言語、つまりJavaのバイトコードにコンパイルされてJavaの実行環境で動くプログラミング言語のひとつ。 Javaのプログラマにとってとっつきやすい文法を保ちつつ、動的型付けを実現し、またRubyなどのスクリプト言語の記法や機能を取り入れ、生産性を高めている。
現在はApacheソフトウェア財団によって管理され、OSSとして開発が進められている。
WebアプリケーションフレームワークのGrails やビルドツールのGradleで採用されている。 Gradleは最近Javaプロジェクトのビルドツールの主流になっていて、Groovyはその定義ファイルを記述する言語として知名度が高いが、Groovyで開発されているプロジェクトとなるとあまり多くないようだ。 GitHubにホストされているGroovyプロジェクトは、2016/4/9現在 0.8%弱 (25,087/3,200,229) しかない。
なぜ人気がないのかはよく分からないが、少なくとも、長くて打ちにくい名前とダサいロゴは不評のようだ。
Groovyのインストール Windows 7にGroovy 2.4.6をインストールする。
本家サイトの手順に従い、Binary Releaseのアーカイブをダウンロードして、適当なところに展開して、展開したフォルダのパスを環境変数GROOVY_HOMEにセットし、%GROOVY_HOME%\binをPATHに追加するだけ。
Java 6以降が前提なので、JAVA_HOMEにJDK 1.7.0_17のパスをセットしておいた。JREでもいいはず。
パケットキャプチャ with Pcap4J in Java Pcap4Jでパケットキャプチャするコードを普通にJavaで書くと以下の様になる。
 これを実行すると、パケットキャプチャするネットワークインターフェースを選択し、5つのパケットをキャプチャしてタイムスタンプと共にコンソールに表示する。
パケットキャプチャ with Pcap4J in Groovy 上記処理をGroovyで書くと以下の様になる。
 メインクラスを書かなくていいところが大きい。 変数の型を書かなくていいのも楽。 ラムダ式でクロージャも作れるし補間文字列(String interpolation)も使える。
また、ここでは使っていないが、オープンクラスなどのメタプログラミングもサポートされている。
上記コードは、Pcap4J 1.6.2、Slf4J 1.7.12、JNA 4.2.1を使って、以下のコマンドで実行できることを確認した。
groovy -cp &amp;#34;pcap4j-core.jar;jna.jar;slf4j-api.jar;pcap4j-packetfactory-static.jar&amp;#34; Pcap4jLoop.groovy tcp これはスクリプト的な実行方法だが、groovycコマンドで事前にコンパイルしてclassファイルを生成し、javaコマンドで実行することもできる。
困ったところ  本家サイトのドキュメントが分かり辛い。
頭から読んでいくと急にディープな部分に引き込まれ、なかなかコードを書き始められなかった。
最近の言語やフレームワークのサイトはチュートリアルに従って動くコードを見ながら概要から詳細に理解を深められる形になっていることが多いので、仕様の詳細が羅列されている感じのGroovyサイトはなんとも読みにくかった。</description>
    </item>
    
    <item>
      <title> ズンドコキヨシ with Pcap4J - ZUNDOKOプロトコルを実装してみた</title>
      <link>https://www.kaitoy.xyz/2016/03/19/zundoko-kiyoshi-with-pcap4j/</link>
      <pubDate>Sat, 19 Mar 2016 11:47:03 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/03/19/zundoko-kiyoshi-with-pcap4j/</guid>
      <description>先週くらいから巷でズンドコズンドコ騒いでいると思ってはいたが、昨日ようやくその元ネタを見た。 以下のツイートだ。
Javaの講義、試験が「自作関数を作り記述しなさい」って問題だったから
「ズン」「ドコ」のいずれかをランダムで出力し続けて「ズン」「ズン」「ズン」「ズン」「ドコ」の配列が出たら「キ・ヨ・シ！」って出力した後終了って関数作ったら満点で単位貰ってた
&amp;mdash; てくも (@kumiromilk) 2016年3月9日 
面白い。 巷ではこれをいろんな言語で実装したりしているみたいでさらに面白い。
私もこのビッグウェーブに乗らないわけにいかないので、専門分野であるネットワーク周りを開拓しようと思い、ZUNDOKOプロトコルというものを考案して実装してみた。書いたソースはGitHubにおいた。

   (adsbygoogle = window.adsbygoogle || []).push({});  ZUNDOKOプロトコル クライアントはサーバに「ズン」か「ドコ」を送る。
サーバは「ズン」を4回受信した後に「ドコ」を受信するとクライアントに「キ・ヨ・シ！」を返す。
クライアント/サーバ間でやり取りするメッセージ(Zundokoパケット)のフォーマットは下図。
0 15 31 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | zundoko (null-terminated string) | | | | | | | | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ 要はzundokoフィールドがあるだけ。 このzundokoフィールドは20 byte固定長で、NULL (0x00)で終わるUTF-8の文字列を保持する。
このメッセージを運ぶ下位レイヤはEthernetで、EtherTypeは0x01FF。
Ethernetにした理由は実装(下記)が楽だから。 EtherTypeはIANAでExperimentalとされている範囲から適当に選んだ。もちろんIANAに登録などはしていない。
因みに、Ethernetヘッダを加えた、クライアント/サーバ間でやり取りする完全なパケットは以下の様になる。(プリアンブルとかは除く。)
0 15 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Dst Hardware Address | + + | | + + | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Src Hardware Address | + + | | + + | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | EtherType (0x01FF) | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | zundoko | | (null-terminated string) | | | | | | | | | | | | | | | | | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | padding | | | 実装 Pcap4Jを使ってクライアントとサーバを実装した。 書いたのは以下の3つのクラス。(といくつかのインナークラス。)</description>
    </item>
    
    <item>
      <title>継続的インテグレーションは死んだ</title>
      <link>https://www.kaitoy.xyz/2016/02/09/continuous-integration-is-dead/</link>
      <pubDate>Tue, 09 Feb 2016 00:34:41 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/02/09/continuous-integration-is-dead/</guid>
      <description>このエントリでは、Yegor Bugayenkoによる記事、Continuous Integration is Deadを紹介する。 (Yegorから和訳と転載の許可は得た。) 以下はその全文の和訳だが、意訳超訳が混じっているので、もとのニュアンスを知りたければ元記事を読んでもいいし、読まなくてもいい。

   (adsbygoogle = window.adsbygoogle || []).push({});  数日前、「なぜ継続的インテグレーションは機能しないのか」という私の記事がDevOps.comに公開された。 それとほぼ同じ日に、Twitterで非常に否定的な批評が送られてきた。
 継続的インテグレーションが機能しないとはどういうことだ。この人気なすばらしいアイデアが。
 その求めてもない質問への返事をここに書く。
私はこの分野に関して多少の経験があるが、それに基いた論拠は挙げない。 代わりにロジックだけを頼りにする。
ところで、私には50以上のオープンソースや営利プロジェクトで5年間Apache Continuum、Hudson、CruiseControl、Jenkinsを利用した経験がある。 さらに、数年前fazend.com(2013年にrultor.comに改名)というホスト型継続的インテグレーションサービスを開発した。 現在TravisとAppVeyorのアクティブユーザでもある。
継続的インテグレーションはどう機能すべきか 考え方はシンプルで明確だ。 masterブランチ(Subversionなら/trunk)に新しくコミットをする度に、継続的インテグレーションサーバ(またはサービス)はプロダクト全体のビルドを試みる。 「ビルド」というのはコンパイル、ユニットテスト、統合テスト、品質解析などを意味する。
その結果は「成功」か「失敗」だ。 もし成功だったら「ビルドがクリーン」であると言う。 もし失敗だったら、「ビルドが壊れている」と言う。 通常、ビルドが壊れるのは、以前通っていたユニットテストを通らなくするような新しいコードをだれかがコミットしたからだ。
これは問題の技術的な面だ。 この部分はいつも上手くいく。 まあ、依存が直書きされてるとか、ビルド環境が十分分離されていないとか、ビルドの並列性が完全じゃないとか、そういう問題はあるかもしれないが、この記事はそれらについてではない。 アプリケーションが上手く書かれていてユニットテストが安定しているなら、継続的インテグレーションは簡単だ。 技術的には。
組織的な面を見てみよう。
継続的インテグレーションというのは、ビルドを実行するサーバだけを指すのではなく、上手く機能すべき管理的/組織的プロセスだ。 プロセスが上手く機能するとは、Jez Humbleが「継続的デリバリー: ビルド、テスト、デプロイの自動化による確実なソフトウェアリリース」の55ページで言っていることそのものを意味する。
 もしビルドが失敗したら、開発チームは何をやっていたとしてもそれを中断して、そのビルドの問題を速やかに直す。これが重要だ。
 これが上手くいかず、上手くできないことだ。
誰がこれを必要としているのか 既に述べた通り、継続的インテグレーションとは、開発チーム全体を止めて壊れたビルドを修正させることだ。 繰り返すが、ビルドが壊れたら直ちに、それを修正し、ビルドを安定した状態に戻すコミットを入れることに全員が集中すべきだ。
ここでひとつ疑問が生じる。誰が、活動中のチーム内の誰がこれを必要としているのだろうか?
一刻も早く新しい機能をリリースしたいプロダクトオーナ? または、締め切りに責任を持つプロジェクトマネージャかもしれない。 もしくは、他の誰かが作りこんだバグをプレッシャーを受けながら修正すること嫌うプログラマかもしれない。
誰がこの継続的インテグレーションを好み、誰が必要としているのか?
誰でもない。
実際に何が起こるのか 教えよう。 私は何度も見たことがある。 シナリオはいつも同じだ。 継続的インテグレーションのビルドステータスは単に無視されるようになる。 ビルドがクリーンか壊れているかにかかわらず。 そして以前のやり方が継続される。
Jez Humbleが推奨するように開発を止めて問題に対応したりしない。</description>
    </item>
    
    <item>
      <title>Pcap4J Meets Windows Containers</title>
      <link>https://www.kaitoy.xyz/2016/01/22/pcap4j-meets-windows-containers/</link>
      <pubDate>Fri, 22 Jan 2016 17:46:43 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/01/22/pcap4j-meets-windows-containers/</guid>
      <description>Windows Containers で Pcap4J のコンテナをビルドしてみた話。

   (adsbygoogle = window.adsbygoogle || []).push({});  Windows Containersとは Windows Containersは、MicrosoftがDocker, Incと提携して開発しているコンテナ技術で、Windows版Dockerとも言われる機能。 今年リリースされる Windows Server 2016 に実装される予定で、その3つめのテクニカルプレビューである Windows Server 2016 Technical Preview 3 (2015/8/19公開)から評価できるようになった。
Windows Containersには次の二種類がある。
 Windows Server Containers
プロセスと名前空間の分離を実現する機能で、これによるコンテナはカーネルをホストと共有する。 つまり本家Dockerに近い形の機能。
 Hyper-V Containers
それぞれのコンテナを軽量化されたHyper-Vの仮想マシンっぽいものの上で動かす機能。 このコンテナの実行にはHyper-Vが必要。 Windows Server Containersよりコンテナ間の分離性が高く、カーネルの共有もしないが、そもそもそれってコンテナなの?
  どちらも同じようなインターフェースで操作でき、このインターフェースにはPowershellのコマンドレットとDockerコマンドの二種類がある。
より詳しくは、Microsoftによる解説や@ITのこの記事がわかりやすい。 また、Qiitaのこの記事がDockerとWindows Server Containersのアーキテクチャを詳細に説明していて面白い。
Windows Containersセットアップ まず、Windows 7 x64のノートPCにVMware Player 7.1.0を入れてWindows 10 x64用のVM(CPU2つとメモリ2.5GB)を作り、そこに2015/11/19に公開された Windows Server 2016 Technical Preview 4 をインストール。 コマンドでいろいろ設定するの慣れていないのでGUI(Desktop Experience)付きで。 (リモートデスクトップ使えばよかったのかもしれないけど。) ロケールは英語以外は問題が起きそうなので英語で。</description>
    </item>
    
    <item>
      <title>Pcap4J with Four Native Libraries on Windows 10</title>
      <link>https://www.kaitoy.xyz/2016/01/12/pcap4j-with-four-native-libraries-on-windows10/</link>
      <pubDate>Tue, 12 Jan 2016 08:43:30 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/01/12/pcap4j-with-four-native-libraries-on-windows10/</guid>
      <description>I did some basic tests for Pcap4J 1.6.2 on Windows 10 Pro on VMware Player 7.1.0 using the following native packet capture libraries:
 Official WinPcap 4.1.3 Unofficial WinPcap based on libpcap 1.7.4 Win10Pcap 10.2 Npcap 0.0.5  This article explains each of the above libraries and tells the test results.

   (adsbygoogle = window.adsbygoogle || []).push({});  Official WinPcap WinPcap is the most common native packet capture library developed based on libpcap.</description>
    </item>
    
    <item>
      <title>ソフトウェアアーキテクトは何をするのか?</title>
      <link>https://www.kaitoy.xyz/2016/01/11/who-is-software-architect/</link>
      <pubDate>Mon, 11 Jan 2016 14:41:29 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/01/11/who-is-software-architect/</guid>
      <description>このエントリでは、Yegor Bugayenkoによる記事、What Does a Software Architect Do?を紹介する。 (Yegorから和訳と転載の許可は得た。) 以下はその全文の和訳だが、意訳超訳が混じっているので、もとのニュアンスを知りたければ元記事を読んでもいいし、読まなくてもいい。

   (adsbygoogle = window.adsbygoogle || []).push({});  君のプロジェクトにはソフトウェアアーキテクトが居るだろうか? 必要だと思う?
まあ、ほとんどのアジャイルチームはそのような役割を明確には定義せず、民主的な感じで働く。 全ての重要な技術的な意思決定はチーム全体で議論され、最多数の投票を得た解決策が採用される。 しばらくして、このようなチームが「ソフトウェアアーキテクト」バッジを誰かのTシャツに付ける事に決めたときは、もっとも評判のいいプログラマがそのバッジを手にする。
このバッジが彼の責務を変えることはまれだけども。 結局、チームは同じように働き続け、全員を巻き込んだ技術的議論を楽しむ。 つまり、ソフトウェアアーキテクトは責務が明確に定義された役割というよりもステータスになる。 それは最年長で最も権限のある人へのチームメンバからの尊敬の印になる。そうだろ?
全く間違っている!
アーキテクトは品質の責任を負う 普通はアーキテクトは最も知識、スキル、経験、権限がある人がなるということは明らかだ。 もちろん普通はアーキテクトは他の人よりもものを知っていて、必要に応じて外交的指導的手腕を発揮してその知識を伝達する。 アーキテクトは普通はチームの中で最も賢いやつだ。
しかしこのことは、彼をアーキテクトたらしめているものではない。
そして、チームに必要なものでもない。
私のソフトウェアアーキテクトの定義こうだ。 アーキテクトは品質の責任を負う人だ。
「責任 (blame)」を職責 (accountability) とか 責務 (responsibility) と言い換えてもいいが、私は「責任 (blame)」という言葉を使うのがいいと思う。 なぜなら、開発中の製品の全ての品質問題がアーキテクトの個人的な失敗であることをより強調するからだ。 もちろん、その責任の対価として、品質がよかった場合には満足した顧客からの称賛は全てアーキテクトのものだ。
これがチームに必要なものだ。 開発するソフトウェアの品質に対して誰かが個人的に責任を負うのだ。
プロジェクトマネージャの仕事は、アーキテクトによる全ての技術的決定に対して誰にも不信を抱かせないようにすること アーキテクトが他のメンバにどのように責任を委譲するかはアーキテクト自身の仕事だ。 知識やスキル、品質管理ツール、ユニットテストフレームワーク、権限、コーチング、体罰、何を使おうとも、それが彼の仕事だ。 プロジェクトマネージャは品質管理をソフトウェアアーキテクトに委譲した。 それをさらにどう委譲するかはソフトウェアアーキテクト次第だ。
ソフトウェアアーキテクトの役割は全てのプロジェクトにおいて重大だ。 たとえたった二人のプログラマが同じデスクで働いている場合でもだ。 二人のうち一人はアーキテクトでなければならない。
理想的なアーキテクトは上記の長所の全てを持つ。 彼は全員の意見を聞いて考慮に入れる。 彼はよいコーチであり先生だ。忍耐もある。 彼は効果的な伝達者であり交渉人だ。 外交官だ。 技術的な領域のエキスパートだ。
しかし、たとえこうした長所全てを持たなくても、彼の決定は常に最終決定だ。
そして、プロジェクトマネージャの仕事は、アーキテクトによる全ての技術的決定に対して誰にも不信を抱かせないようにすることだ。 これが委譲というものだ。 責任には常に権力が伴う。
プロジェクトマネージャは定期的にアーキテクトの成果を評価すべきだ。 チームで開発中の製品の品質はアーキテクトの個人的な(!)責任だということを思い出してほしい。 どんな問題であっても彼の問題だ。 彼を責めたり罰したりすることを恐れてはいけない。 ただし、罰を有効なものにするためには、アーキテクトの行動に対して全力で応えるべきだということをを忘れてはいけない。 繰り返すが、彼の決定は最終決定だ。</description>
    </item>
    
    <item>
      <title>pcap-ng support in Pcap4J</title>
      <link>https://www.kaitoy.xyz/2016/01/10/pcap-ng-support-in-pcap4j/</link>
      <pubDate>Sun, 10 Jan 2016 09:52:06 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/01/10/pcap-ng-support-in-pcap4j/</guid>
      <description>Sometimes I receive inquiries about support for pcap-ng files in Pcap4J. I wrote the result of my investigation on it in this article.

   (adsbygoogle = window.adsbygoogle || []).push({});  What&amp;rsquo;s a pcap-ng file A pcap-ng file (i.e. a file with .pcapng extension ) is a packet dump file in The pcap Next Generation Capture File Format (or pcap-ng format for short). This format was created to overcome the limitations of the traditional Libpcap File Format (or pcap format for short) which is used in pcap files.</description>
    </item>
    
    <item>
      <title>オブジェクト指向プログラミングにおいてユーティリティクラスに代わるもの</title>
      <link>https://www.kaitoy.xyz/2016/01/03/oop-alternative-to-utility-classes/</link>
      <pubDate>Sun, 03 Jan 2016 23:36:01 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/01/03/oop-alternative-to-utility-classes/</guid>
      <description>このエントリでは、Yegor Bugayenkoによる記事、OOP Alternative to Utility Classesを紹介する。 (Yegorから和訳と転載の許可は得た。) 以下はその全文の和訳だが、意訳超訳が混じっているので、もとのニュアンスを知りたければ元記事を読んでもいいし、読まなくてもいい。

   (adsbygoogle = window.adsbygoogle || []).push({});  ユーティリティクラス(またはヘルパークラス)は、スタティックメソッドだけを持っていて、状態を内包しない「構造体」だ。 Apache CommonsのStringUtils、IOUtils、FileUtilsや、GuavaのIterables、Iterators、またJDK7のFilesはユーティリティクラスのいい例だ。
ユーティリティクラスはよく使われる共通機能を提供するので、この設計手法はJava(やC#、Rubyなど)の世界でとても人気だ。
要するに我々は、DRY原則に従い、重複を避けたい。 だから、共通コードをユーティリティクラスに入れて必要に応じて再利用する。
// これはひどい設計なので再利用しないように。 public class NumberUtils { public static int max(int a, int b) { return a &amp;gt; b ? a : b; } } 実際、これはとても便利なテクニックだ!?
ユーティリティクラスは悪だ しかし、オブジェクト指向の世界では、ユーティリティクラスはかなり悪い(酷いという人さえいるかもしれない)手法だ。
これについては多くの議論がある。 いくつか挙げると、Nick Malikの「ヘルパークラスは悪か?」、Simon Hartの「なぜヘルパー、シングルトン、ユーティリティクラスはだいたい間違っているのか」、Marshal Wardの「ユーティリティクラスを避ける」、Dhaval Dalalの「ユーティルクラスを殺せ!」、Rob Bagbyの「ヘルパークラスは問題の兆候」。
また、StackExchangeにはユーティリティクラスについての質問がいくつかある。 例えば、「ユーティリティクラスが悪なら、どこに共通コードを書けばいい?」とか、「ユーティリティクラスは悪」とか。
これらの主張は要するに、ユーティリティクラスは適切なオブジェクトではないということだ。 だから、オブジェクト指向の世界に適合しない。 ユーティリティクラスは、当時の人々が機能分割パラダイムに慣れていたために、手続き型言語から受け継がれた。
君がこの主張に同意し、ユーティリティクラスを使うのをやめたがっていると想定し、そいつをどのように適切なオブジェクトに置き換えるかを例を挙げながら教えよう。
手続き型の例 例えば、テキストファイルを読んで、行で分割し、各行をトリムして、その結果を別のファイルに保存したいとする。 これはApache CommonsのFileUtilsを使えばできる。
void transform(File in, File out) { Collection&amp;lt;String&amp;gt; src = FileUtils.</description>
    </item>
    
    <item>
      <title>git resetとrevertを図解する</title>
      <link>https://www.kaitoy.xyz/2016/01/01/git-revert-reset/</link>
      <pubDate>Fri, 01 Jan 2016 18:38:02 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2016/01/01/git-revert-reset/</guid>
      <description>この記事を読んだ、またはGitのオブジェクトモデルを理解していることを前提に、Gitの git revert と git resetというコマンドについて説明する。 この二つはしばしばコミットを取り消すコマンドとして同じ文脈で説明されることが多いのでこのエントリでも一緒に説明するが、実際は全く異なるコマンドだし、そもそもどちらもコミットを取り消すコマンドではない。

   (adsbygoogle = window.adsbygoogle || []).push({});  git revert git revertは、指定したコミットが持ち込んだ変更を打ち消すコミットを追加する。 リバースパッチを適用すると言ってもよい。 コミットを追加しかしないので、このコマンドによって既存のコミットが消えたり変わったりすることはない。
図にすると以下の感じ。単純。
   git reset git resetには二つの機能がある。 インデックスを再設定する(i.e. resetする)機能と、HEADを付け替える(i.e. resetする)機能だ。
インデックスの再設定 インデックスの再設定をするコマンドはgit reset &amp;lt;ワーキングディレクトリ内のファイルのパス(複数可)&amp;gt;。 これを実行すると、指定したファイルについて、HEADが指すコミットが指すツリー内のブロブを指すようインデックスを更新する。
何を言っているのかわからないので図にする。
    (この図では便宜的にHEAD、つまり参照をオブジェクト格納領域内に書いているが、実際には別の場所にあることに注意。)
図を見ると、git add Readme.mdとgit reset Readme.mdがだいたい逆のことをしていることがわかる。 要するに、git add &amp;lt;パス&amp;gt;は指定したファイルをステージし、git reset &amp;lt;パス&amp;gt;は指定したファイルをアンステージする。
HEADの付け替え HEADの付け替えをするコマンドはgit reset &amp;lt;コミット&amp;gt;。 これを実行すると、HEADが指しているコミットを指すようORIG_HEADを作成または更新し、指定したコミットを指すようHEADを更新する。 オプションによってはさらにインデックスやワーキングディレクトリを指定したコミットが指すツリーと同期するよう更新する。
このオプションには--soft、--mixed (デフォルト)、--hardの三種類があり、それぞれのオプションを付けた時の更新対象を次の表に示す。
   オプション HEAD インデックス ワーキングディレクトリ     --soft ○     --mixed ○ ○    --hard ○ ○ ○</description>
    </item>
    
    <item>
      <title>Gitの分散バージョン管理の仕組み</title>
      <link>https://www.kaitoy.xyz/2015/12/31/git-dvc/</link>
      <pubDate>Thu, 31 Dec 2015 01:02:59 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/12/31/git-dvc/</guid>
      <description>このエントリでは、この記事を読んだ、またはGitのオブジェクトモデルを理解していることを前提に、Gitの分散バージョン管理の仕組みについて説明する。

   (adsbygoogle = window.adsbygoogle || []).push({});  Gitの分散バージョン管理 分散バージョン管理とは、分散したリポジトリでのバージョン管理ということ。 ここでリポジトリが分散しているとは、同じプロジェクトの履歴を管理する完全で独立したリポジトリが複数あるということ。 これにより一つのプロジェクトの開発を地理的に分散して並行して進めることができる。
Gitは分散バージョン管理のために、リポジトリのクローン(≒コピー)を作る機能と、リポジトリ間でコミットグラフを同期する機能を提供している。
リポジトリのクローンを作ると言うと、オリジナルとクローンの間に格差があるような気がするが、 実際にはGitは全てのリポジトリが対等であるという思想のもとで実装されている。 このため、リポジトリをクローンする時には(デフォルトで)クローン元の完全なコミットグラフがクローンにコピーされるし、任意のリポジトリ間のデータのやり取りをpeer-to-peerでできる。 クローンからクローンを作ることももちろん可能。
git pushでデータを送る先をアップストリームと呼ぶことはあるし、次節でローカルリポジトリとリモートリポジトリという関係が出てくるが、これはあくまでその時点でそういう設定になっているというだけ。 アップストリームはいつでもいくつでもgit remoteコマンドで追加したり削除したりできる。
このような実装により、Gitの分散バージョン管理ではリポジトリ間で柔軟なデータのやり取りができる。 例えば以下の様な複雑なリポジトリネットワークを組むこともできる。
ローカルリポジトリとリモートリポジトリ 一人の開発者から見て、手元にあるリポジトリを ローカルリポジトリ と呼ぶのに対して、git pushやgit pullやgit fetchでデータをやり取りする相手のリポジトリを リモートリポジトリ と呼ぶ。 リモートリポジトリとのやり取りは、リモート追跡ブランチ と リモート というものを使って実装されている。
リモート追跡ブランチ リモート追跡ブランチは、ローカルリポジトリの.git/refs/remotes/に格納される参照で、リモートリポジトリ内のローカルブランチのコミットグラフを取得してローカルリポジトリ内に保持するために使われる。 git branch -rでその一覧が見れる。
「追跡」ブランチというだけあって、リモートリポジトリ内でコミットグラフが成長した場合、この変更に追随することができる。 このためのコマンドがgit fetch。 因みにgit pullは、git fetchでリモート追跡ブランチを更新した後、git merge(オプションによってはgit rebase)でそのリモート追跡ブランチをローカルブランチにマージするのと同じ。
リモート リモートとは、リモートリポジトリのこと、またはリモートリポジトリに接続するための定義のこと。 この定義は、ローカルリポジトリの.git/configにremoteセクションとして書かれている。 以下がその例。
[remote &amp;#34;origin&amp;#34;] fetch = +refs/heads/*:refs/remotes/origin/* url = git@github.com:kaitoy/blog.git セクション名のところに&amp;quot;origin&amp;quot;とあるがこれは、この定義で接続するリモートリポジトリをGitコマンドなどでoriginと指定できるということ。 ここで定義されているのはurlとfetchで、それぞれ以下を意味する。
 url
リモートリポジトリのURL。 つまり、リモートリポジトリがどのサーバのどのディレクトリにあって、それとのデータのやり取りをどのプロトコルでやるかという定義。 このURLには以下の書式が使える。</description>
    </item>
    
    <item>
      <title>Gitのマージを図解する</title>
      <link>https://www.kaitoy.xyz/2015/12/28/git-merge/</link>
      <pubDate>Mon, 28 Dec 2015 01:05:29 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/12/28/git-merge/</guid>
      <description>このエントリでは、Gitが提供するマージのための機能の内、主なもの4つ、真のマージ、リベース、ファストフォワードマージ、チェリーピック について図解する。 ここでマージとは、とあるブランチのコミットが入れた修正を別のブランチに取り込むこととする。
この記事を事前に読んでGitのオブジェクトモデルを理解しておくと分かりやすいかもしれない。
ここで説明するマージは全てローカルリポジトリ内のブランチを操作対象とする。

   (adsbygoogle = window.adsbygoogle || []).push({});  真のマージ 真のマージは、複数のブランチでそれぞれ開発が進んでいて、つまりそれぞれのコミットグラフが伸びている場合に、それらの修正を統合するときに実行する。 マージするブランチはいくつでも指定できる。
基本的なコマンドはgit merge &amp;lt;ブランチ(複数可)&amp;gt;。
操作に成功すると、マージ後のプロジェクトの状態を表すコミット(マージコミット)が作られ、カレントブランチの先頭に追加される。 マージコミットは、マージした全てのブランチが指していたコミットを親として持つ。
このマージはマージコミットを追加するだけであり、既存のコミットを一切変更しないことを認識しておくべし。
以下、真のマージの実行例を図示する。
     リベース リベースは、あるブランチで作った一連のコミットの起点(ベース)を移動したいときに実行する。 この操作は一般的にはマージとは呼ばれないが、冒頭に書いたマージの定義からするとマージと見なせないこともないのでここに挙げる。
基本的なコマンドはgit rebase &amp;lt;ブランチ&amp;gt;。 このコマンドは、カレントブランチの起点を指定したブランチが指すコミットに移動する。
この操作に成功すると、カレントブランチで作ったコミットは(実質)消え、それと同等の修正をもたらす別のコミットが移動先のコミットを起点として作成される。(※1)
リベースは既存のコミットを消し、コミットグラフを変更してしまうということを認識しておくべし。
以下、リベースの簡単な実行例を図示する。
   
上のスライドのように単純なコミットグラフならいいが、リベースするブランチが分岐していたりするとややこしいことが起き得る。 そういうケースにはO&amp;rsquo;Reillyの蝙蝠本などでよく勉強してから臨むべし。
(※1: より正確にはgit rebase &amp;lt;ブランチ&amp;gt;は、
 カレントブランチで作った各コミットが入れた変更をパッチにして、 それを古い順に一つずつ、指定したブランチが指すコミットに適用しながら新しいコミットを作り、 カレントブランチが指しているコミットをORIG_HEADで指し、 カレントブランチを最新のコミットを指すよう更新する。  2で、指定したブランチが既にチェリーピック(後述)などでカレントブランチのとあるコミットの変更を取り込んでいた場合、そのコミットのパッチの適用はスキップされ、そのパッチによるコミットも作られない。
また、上でカレントブランチのコミットは実質消えると書いたが、当面はオブジェクトが本当に消えるわけではないし、ORIG_HEADとかが指しているのでもどることもできる。)
ファストフォワードマージ ファストフォワードマージは、マージ先のコミットが全てマージ元に含まれているときに使えるマージ。 この操作は既存のコミットグラフをいじらないしマージコミットも作らない特殊なマージ。 (実のところマージじゃないと言ってもいい。) このマージを実行した後は、コミットグラフは一直線になり、ブランチを作らずにコミットを作った場合と同様になる。
このマージは、git merge &amp;lt;ブランチ&amp;gt;を実行したときに可能であれば実行される。 (でなければ真のマージが実行される。オプションで選択することもできる。)
以下にファストフォワードマージの例を図示する。</description>
    </item>
    
    <item>
      <title>Gitのリポジトリの中身をなるべく正確に理解する</title>
      <link>https://www.kaitoy.xyz/2015/12/27/git-repository/</link>
      <pubDate>Sun, 27 Dec 2015 11:34:18 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/12/27/git-repository/</guid>
      <description>このエントリでは、Gitの基本的な使い方は理解している前提で、そのリポジトリの構造をなるべく正確に説明する。 ここに書いてあることは概ね、筆者がO&amp;rsquo;Reillyの蝙蝠本を読んで得た知識に基づく。
リポジトリの構造というとコアで上級者向けの知識のように聞こえるが、これをまず理解しておくことで強力で複雑なGitの機能を習得するのが非常に楽になる。 具体的には、Gitにおけるブランチの概念などの理解が深まったり、git resetなどのGit特有で分かり辛いコマンドを自信をもって使えるようになったり、なにより、Gitを使う上での最大のハードルである インデックス や HEAD の概念を完璧に理解できるというメリットがある。
チュートリアルを終えたくらいの初心者にこそ読んでほしいエントリである。

   (adsbygoogle = window.adsbygoogle || []).push({});  Gitリポジトリの中身 Gitのリポジトリは、プロジェクトをクローンしたときとかにできる.gitディレクトリ内に詰まっている。 このディレクトリには、オブジェクト格納領域 と インデックス というデータ構造が入っている。 また、参照 (ref) や シンボリック参照 (symref) というものも入っている。
以下、それぞれについて説明する。
オブジェクト格納領域 オブジェクト格納領域は、ファイルシステム上では.git/objects/以下にあたる。
ここには、バージョン管理されているファイルの情報やそのコミット履歴などが保存されていて、具体的には以下の4種類のオブジェクトが置かれている。
 ブロブ
一つのファイルを表すオブジェクト。 バージョン管理対象のファイルの内容(だけ)を保持する。
 ツリー
一つのディレクトリを表すオブジェクト。ブロブや別のツリーを指すポインタを持ち、またそれらが表すファイル/ディレクトリの名前や属性を保持する。 つまり、これとブロブを組み合わせると、ファイルシステム上のディレクトリツリーを表すことができる。
 コミット
一つのコミットを表すオブジェクト。コミット日時やログメッセージなどの情報と、一つ前のコミット(親コミット)を指すポインタと、一つのツリーを指すポインタを持つ。 このツリーはプロジェクトのルートディレクトリを表す。 つまり、一つのコミットは、プロジェクトのある時点でのディレクトリツリー全体を表してもいる。
 タグ
一つの注釈付きタグ(git tag -aで作るタグ)を表すオブジェクト。 タグ名やタグにつけたコメントなどの情報と、一つのオブジェクト(普通はコミット)へのポインタを持つ。 因みに軽量タグ(git tagで作るタグ)はオブジェクトにならない。
  ファイルシステム上で、一つのオブジェクトは一つのファイルに書き込まれ、zlibで圧縮され、.git/objects/以下に配置される。 そのファイルへのパスには、オブジェクトのコンテンツから計算されたSHA1ハッシュの値(i.e. オブジェクトの名前)が使われる。 例えば.git/objects/16/cacde1ddabe1698b0e41e091e4697313e2b7e5というファイルがあったら、これは 16cacde1ddabe1698b0e41e091e4697313e2b7e5 という名のオブジェクトの実体。
git cat-file -p &amp;lt;SHA1ハッシュ&amp;gt;でオブジェクトのコンテンツを見れるので、いくつか見てみると面白い。 たとえばコミットオブジェクトは以下の様になっている。</description>
    </item>
    
    <item>
      <title>ReactをAtomパッケージ開発に使ってみた</title>
      <link>https://www.kaitoy.xyz/2015/12/21/hello-react/</link>
      <pubDate>Mon, 21 Dec 2015 00:07:28 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/12/21/hello-react/</guid>
      <description>私は今HPEのFort Collinsオフィスに居候している。 HPEは最近、Reactを使ったUXフレームワークであるGrommetを開発していて、私が扱っている製品もそれを使う兆しが見えてきた。 Grommetはいずれ仕事で触ることになりそうなので、まずはReactの勉強をと思い、とあるAtomパッケージの開発に敢えて使ってみた。
このエントリには、その作業の中で得た知識などについて書いた。 ただし、Reactを使った開発のノウハウみたいなものまでは得ていないので書いていない。
(因みにGrommetはGitHubで公開されているが、ほとんど話題になっておらずスターも現時点で245しかついていない。。。)

   (adsbygoogle = window.adsbygoogle || []).push({});  Reactとは ReactはFacebookが開発しているWeb UIのフレームワークで、MVCのVだけを実装したもの。 2013年に最初のバージョンが公開され、世界中で流行ってきているらしい。
その特徴(というかほぼ全容)は仮想DOM(Virtual DOM)。 ReactのAPIを使うと、リアルDOMと一対一で対応する仮想DOMのツリーを作ることができ、UIを組み立てられる。 リアルDOMの構築や更新はReactが最適化された方法でやってくれるので、性能がいいUIができるらしい。 因みに、仮想DOM自体はReact特有の技術ではなく、別の実装もある。
もう一つの特徴はJSX。 これは、JavaScriptのコードの中で、XMLみたいな構文で仮想DOMを記述するための拡張構文。 これを使うとReactコードが見やすく簡単に書けるけど、当然普通のJavaScript実行環境では動かないので、プリコンパイルなどが必要になる。
FacebookはReactを使った開発にFluxというアーキテクチャの採用を推奨している。 FluxはMVCアーキテクチャに置き換わるもので、従来の複雑なデータフローに反発し、一方向のシンプルなデータフローを提供する。 Fluxは単なるアーキテクチャで、その全体の実装を支援するフレームワークは現時点では無い。 (多分。Relayが一部支援してくれるっぽい。)
Reactを触った感想 Reactは本当にちょっとしか触っていないので、あまりよく分かっていないんだろうけど、なんだか使いにくかった。
Reactは仮想DOMを作るところしか助けてくれないので、他のことは全部自分でやらないといけない。 FacebookはReact用のウィジェットすら提供していない。 昔仕事で全部入りのDojoを使っていたので、それとのギャップをすごい感じた。
そのうえ、他のフレームワークやライブラリと組み合わせて使おうとすると仮想DOMが壁になってくる。普通のフレームワークはリアルDOMを扱うからだ。 例えば、JavaScriptを書いているとすぐjQueryを使いたくなるが、これでリアルDOMを直接いじってしまってはReactを使う意味がない気がする。
AtomパッケージでReactを使う Reactはnpmでも提供されていて、Atomパッケージの開発に簡単に使える。 パッケージのpackage.jsonのdependenciesにreactとreact-domを入れておけば、パッケージコード中で以下の様に仮想DOMを作れるようになる。
var React = require(&amp;#39;react&amp;#39;); var ReactDOM = require(&amp;#39;react-dom&amp;#39;); class MyComponent extends React.Component { render() { return &amp;lt;div&amp;gt;Hello World&amp;lt;/div&amp;gt;; } } ReactDOM.render(&amp;lt;MyComponent /&amp;gt;, node); BabelによるJSXの手動コンパイル JSXのコンパイルにはBabelを使うのがいい。 手動コンパイルにはBabelのコマンドラインツールが必要で、これはnpmで提供されている。 npmコマンドはAtomに同梱されているので別途インストールは不要。</description>
    </item>
    
    <item>
      <title>impress.jsでのプレゼン資料作成をサポートするAtomパッケージ - impress</title>
      <link>https://www.kaitoy.xyz/2015/12/19/atom-impress/</link>
      <pubDate>Sat, 19 Dec 2015 23:37:08 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/12/19/atom-impress/</guid>
      <description>Atomのパッケージを作った話。
ついでに、パッケージプロジェクト内で別のプロジェクトを取り込んで使いたい場合に、Gitのサブモジュールを使ってはダメという話。

   (adsbygoogle = window.adsbygoogle || []).push({});  impress.js impress.jsというJavaScriptライブラリがある。 HTML5とCSS3とJavaScriptでプレゼン資料を作るためのライブラリで、これを使うと、PowerPointやKeynoteといった従来のツールによるものからは一線を画す斬新な資料を作ることができる。
公式のデモを見ればその魅力を堪能できる。
デモを見ると分かるが、Preziに触発されたライブラリだ。 Preziでも非常に新鮮な資料を作れるが、ほぼ有料で、また作成した資料をPreziのサーバに置かなければいけないので、仕事で使う資料作りには使いにくい。 その点impress.jsは、MIT(とGPLv2)で公開されていて自由に無料で使えるのがよい。
ただし、Preziがスライドという概念から大きく脱却しているのに対して、実のところimpress.jsで作れる資料はあくまでスライドベースだ。 従来のものに比べてスライドの並びに制約がなく、スライド間の遷移がダイナミックというだけだ。 impress.jsでもまあ工夫すればPreziのような資料は作れるが。 独自のオーサリングツール/ビューワに依存するPreziに対し、impress.jsは標準的なHTML/CSS/JavaScriptにだけ依存しているので、jQueryなどのWeb技術を活用してスライドを作れるという副次的なメリットはある。
impress.jsは、2012年に最初のバージョンが公開されてからもう4年近く経つが、未だにそれほど広く使われている様子はない。 PowerPointが幅を利かせているせいもあるだろうが、その使い辛さから利用をためらう人が多いのではないだろうか。 impress.jsはあまりドキュメントが充実しているとは言えない。 GitHubに公開されているREADMEには、使い方はソースを見よ、それで分からないなら使うなとある。 さらにソース中には、impress.jsを使うには、HTMLとCSSのスキルに加えてデザイナーのセンスも必要とある。 かなりハードルを上げている。
このハードルをクリアしていたとしても、実際、impress.jsで資料を作るのはPowerPointに比べて10倍は大変だ。 impress.jsはスライド(impress.js用語ではステップ)間の遷移を制御してくれるだけで、各スライドのコンテンツを作るという部分に関してはなんのサポートも提供しない。 テンプレートもなければ、表やグラフを書く機能もなく、アニメーションも作れない。 そういうことをしたければ、自分で別途ライブラリを探して使うなりしないといけない。
ちょっとした図を書くにも、テキストエディタでちまちまHTMLとCSSを書いて、ブラウザで表示して確認して、思った通りになっていなければディベロッパツールでデバッグして、Web UIでも書いていたんだっけという気になってくる。
impressパッケージ そんな負担を少しでも軽くしたいと思って作ったのがimpressパッケージ。
同じ目的のツール(i.e. オーサリングツール)は実は既にいくつかあった。 なかでも、Hovercraft!というのが高機能で便利そう。 ただ、これらはPowerPointほど自在にスライドを作れるまでには至っておらず、結局は仕上げにHTML/CSSを手でいじる作業が必要になる。(と思う。) また、jQueryのプラグイン使ってかっこいいことしたいとか言う場合にも、手でコードを書かなければいけない。
つまりテキストエディタを開かなければいけない。よってAtomを起動することになる。(私は。)
であれば、オーサリングツールもAtomに統合されていた方が便利なんじゃないの? というのがimpressパッケージを作った動機。
まだ機能は少なくて、新規資料プロジェクトの雛形生成、
ステップをリスト表示するビュー表示、
プレビューができるだけ。

ゆくゆくは、GUIでステップの配置や角度を編集する機能、GUIでステップ内の図を作成する機能を作りたい。 あとできればアニメーションを付ける機能とかも。 Hovercraft!みたいにHTML書かなくてもいいよ、というのを目指すつもりはなくて、あくまでもコーダーのための、コーディングを補助するツールを目指す。
パッケージのサブモジュール impressパッケージは、新規資料プロジェクトの雛形生成機能などのため、impress.jsプロジェクト(のフォーク)をサブモジュールとしてとりこんでいる。
最初はGitのサブモジュールコマンド(git submodule)を使って取り込んでいて、上手くいっているように見えたが、パブリッシュ後に次のような問題が発生した。 即ち、試しにimpressパッケージをインストールしてみたら、サブモジュールのフォルダの中身がからっぽだった。
これは、AtomのパッケージマネージャがパッケージをGitHub Releasesからダウンロードしてインストールするからだ。サブモジュールの中身はGitHub Releasesに登録されるアーカイブに含まれない。このGitHub Releasesの挙動は、サブモジュールを含むGitプロジェクトをクローンした場合、デフォルトではサブモジュールはクローンされないというGitサブモジュールの仕様に関係しているのかもしれない。
この問題をきっかけにGitサブモジュールについてちょっと調べてみた。 蝙蝠本によると、Git開発チームはあまりサブモジュールコマンドの開発に熱心ではなく真面目に作らなかったらしい。 また、あるブログによればサブモジュールコマンドは大分まえからオワコンらしい。このブログによれば、今は多くの場合git subtreeを使うのがいいとのこと。git subtreeは蝙蝠本にもPro Gitにも載ってないのだが。
git subtreeでプロジェクトを取り込んだ場合、親プロジェクトのクローン時にサブプロジェクトもデフォルトでクローンされる仕様だ。 (というか正しくは、サブモジュールと違って、子プロジェクトが親プロジェクトにマージされているから、一緒にクローンされるというだけ。) これを使ってimpressパッケージを構成しなおしてみたら件の問題が解決した。 因みにやりかたは、impressパッケージプロジェクトのルートにimpress.</description>
    </item>
    
    <item>
      <title>Pcap4JがSoftware Quality Award 2015で入賞</title>
      <link>https://www.kaitoy.xyz/2015/12/03/software-quality-award-2015/</link>
      <pubDate>Thu, 03 Dec 2015 12:28:24 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/12/03/software-quality-award-2015/</guid>
      <description>Teamed.ioが主催の、ソフトウェアの品質とその開発プロジェクトの品質への取り組みを競うコンテスト、Software Quality Awardの第一回が2015年4月～11月にかけて開催された。 Teamed.ioのCTOであるYegorとは、彼のブログを和訳してここに載せている関係でたまにメールしているが、そのやりとりの中で誘われたので私もPcap4Jをひっさげてそれに参加した。

   (adsbygoogle = window.adsbygoogle || []).push({});  優勝すると$4,096もらえるということではあったが、150以上のプロジェクトがエントリーしていて、Gulpとか有名なものも入っていたので、どうせ全然ダメだろと思ってエントリー以来なにも対策しなかったが、なんと 8位 入賞を果たしてしまった。 まあ講評をみるとずいぶんこき下ろされてはいるが…
因みに講評は以下の感じ。
 utilパッケージがあってそこにユーティリティクラスがある。クソだ。 NULLが可変オブジェクトで使われている。例えばAbstractPcapAddress。クソだ。 スタティックメソッドとスタティック変数が多すぎる。文字通りどこにでもある。pcap4j-packetfactory-staticという名のスタティックメソッドだらけのモジュールまである。 JavaDocに一貫性がなく、未完なものもある。これとか。 ほんのちょっとのissuesとたった6つのプルリクエストしかない。コミットがissuesにリンクされてない。変更のトレーサビリティはほとんどゼロだ。 リリース手順が自動化されていない。リリースがドキュメントに書かれていない。 静的解析してなくて、そのせいか乱雑なコードがたまにある。 スコア: 3  静的解析くらいは導入しようかな…
ユーティリティクラスとかNULLとかスタティックメソッドは使うのやめるつもりはないけど。
そういえば、入賞者にはスポンサーであるJetBrainsの製品の一年ライセンスがもらえることになっていたはずだが特に連絡がないな。</description>
    </item>
    
    <item>
      <title>Atomパッケージをアンパブリッシュする</title>
      <link>https://www.kaitoy.xyz/2015/12/02/unpublish-atom-package/</link>
      <pubDate>Wed, 02 Dec 2015 11:23:02 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/12/02/unpublish-atom-package/</guid>
      <description>Atomのパッケージをリリースすることをパブシッシュというが、リリースを取り消すことをアンパブリッシュという。 この記事はそのアンパブリッシュのやり方などについて。
筆者の環境は以下。
 Windows 7 x64 Atom 1.2.4 Git for Windows 2.6.3  
   (adsbygoogle = window.adsbygoogle || []).push({});  アンパブリッシュのやり方 リリースしたパッケージのプロジェクトのルートフォルダ(package.jsonがあるところ)にcdして、apm unpublishを実行するだけ。
または、任意のフォルダでapm unpublish &amp;lt;パッケージ名&amp;gt;を実行する。
特定のバージョンだけアンパブリッシュしたい場合は、apm unpublish &amp;lt;パッケージ名&amp;gt;@&amp;lt;バージョン&amp;gt;。例えばapm unpublish disturb-me@0.1.0。
注意すべき点 1: Git Bashでアンパブリッシュするとエラー Git for WindowsのGit Bash上で、Windows版Atomに付属するapmでapm unpublishを実行すると以下のエラーが出る。
Error: EINVAL, invalid argument at new Socket (net.js:157:18) at process.stdin (node.js:693:19) at Unpublish.module.exports.Unpublish.promptForConfirmation (C:\Users\Kaito\AppData\Local\atom\app-1.2.4\resources\app\apm\lib \unpublish.js:87:48) at Unpublish.module.exports.Unpublish.run (C:\Users\Kaito\AppData\Local\atom\app-1.2.4\resources\app\apm\lib\unpublish.js:126:21) at Object.module.exports.run (C:\Users\Kaito\AppData\Local\atom\app-1.2.4\resources\app\apm\lib\apm-cli.js:226:32) at Object.&amp;lt;anonymous&amp;gt; (C:\Users\Kaito\AppData\Local\atom\app-1.2.4\resources\app\apm\lib\cli.js:6:7) at Object.</description>
    </item>
    
    <item>
      <title>ありがとうさようならjapanese-wrap</title>
      <link>https://www.kaitoy.xyz/2015/11/16/thanks-bye-bye-japanese-wrap/</link>
      <pubDate>Mon, 16 Nov 2015 22:38:11 -0700</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/11/16/thanks-bye-bye-japanese-wrap/</guid>
      <description>テキストエディタAtomのとある有名なパッケージの話。

   (adsbygoogle = window.adsbygoogle || []).push({});  以前の記事でも触れたjapanese-wrap。 日本語が画面の端でうまく改行(softwrap)してくれない問題を解決してくれるパッケージ。 Atomで日本語を書く殆どの人がインストールしているであろうパッケージだが、先日11/12にリリースされたAtom 1.2でCJK文字 (中国語・日本語・朝鮮語・ベトナム語の文字)のsoftwrapへの対応が実装されたので、もはや不要になった。
むしろ、Atom 1.2でjapanese-wrapを有効にすると、以下のように残念なことになる。

japanese-wrapにはずっとお世話になってきたので申し訳なく名残惜しくもあるが、AtomのSettingsからDisableまたはUninstallさせてもらうしかあるまい。すると以下の様に直る。

ありがとうさようならjapanese-wrap。</description>
    </item>
    
    <item>
      <title>よいオブジェクトの七つの美徳</title>
      <link>https://www.kaitoy.xyz/2015/10/28/seven-virtues-of-good-object/</link>
      <pubDate>Wed, 28 Oct 2015 13:38:47 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/10/28/seven-virtues-of-good-object/</guid>
      <description>このエントリでは、Yegor Bugayenkoによる記事、Seven Virtues of a Good Objectを紹介する。 (Yegorから和訳と転載の許可は得た。) 以下はその全文の和訳だが、意訳超訳が混じっているので、もとのニュアンスを知りたければ元記事を読んでもいいし、読まなくてもいい。

   (adsbygoogle = window.adsbygoogle || []).push({});  Martin Fowler曰く、
 ライブラリは本質的には呼び出し可能な関数の集合で、最近は普通クラス内にまとめられる。
 クラス内にまとめられた関数? 失礼を承知で言わせてもらうが、これは間違っている。 そして、これはオブジェクト指向プログラミングにおいて、クラスに対する非常に一般的な誤解だ。 クラスは関数をまとめるものではないし、オブジェクトはデータ構造体ではない。
では、なにが適切なオブジェクトなのか? どれが不適切なオブジェクトなのか? その違いは何か? これは論争を呼ぶ主題ではあるが、とても重要だ。 オブジェクトが何かを理解しなければ、オブジェクト指向ソフトウェアをどうやって書くんだ? まあ、JavaやRubyなどのおかげで、書けることは書ける。 しかし、はたして良いものができるだろうか? 不幸にも、これは厳密な科学ではなく、様々な意見がある。 ここに、良いオブジェクトの特性を私なりにリストアップする。
クラス vs オブジェクト オブジェクトについて議論を始める前に、クラスとは何かを定義しよう。 それはオブジェクトが生まれる(インスタント化される)場所だ。 クラスの主な責任は、要求に応じて新しいオブジェクトを構築し、使われなくなったオブジェクトを破壊することだ。 クラスはその子供たちがどのように見えどのように振る舞うべきかを知っている。 言い換えれば、子供たちが従うべき契約を知っている。
クラスが「オブジェクトのテンプレート」であると言われることもある。(例えばWikipediaにはそう書いてある。) この定義はクラスを受動的なポジションに置いているので正しくない。 この定義は、だれかがテンプレートを取得してそこからオブジェクトを構築するということを想定している。 これは、技術的には正しいかもしれないが、概念的には間違っている。 クラスとその子供たちだけが居るのであって、他の誰も関係すべきではない。 あるオブジェクトがクラスに他のオブジェクトを作るように頼み、そのクラスがオブジェクトを構築する。それだけだ。 RubyはJavaやC++に比べてこの概念をかなりうまく表現している。
photo = File.new(&amp;#39;/tmp/photo.png&amp;#39;) photoオブジェクトはFileクラスによって構築される。(newはそのクラスへのエントリポイント。) オブジェクトは、いったん構築されると、自身に基づいて行動する。 オブジェクトは、自身を誰が構築したかとか、何人兄弟姉妹がいるかとかを知っているべきではない。 そう、リフレクションは酷いアイデアだと言っている。 それについては他の記事で詳しく書くとして、ここでは、オブジェクトについてと、その最高と最悪の両面について話そう。
1. 彼は実世界に存在している まず第一に、オブジェクトは生きた有機体だ。 もっと言えば、オブジェクトは擬人化されるべきだ。 つまり、人間(もしくは、君がより好むならペット)のように扱われるべきだ。 基本的にこれは、オブジェクトはデータ構造体や関数の集合ではないということを意味している。 代わりに、オブジェクトは独立したエンティティで、それ自身のライフサイクル、振る舞い、性質を持つ。
従業員、部署、HTTPリクエスト、MySQLのテーブル、ファイルの行、ファイルそのもの、これらは適切なオブジェクトだ。 なぜならこれらは、ソフトウェアを停止した時でも実世界に存在しているから。 より正確には、オブジェクトは実世界のモノの表現のひとつだ。 オブジェクトは実世界のモノと他のオブジェクトとの間のプロキシだ。 そのようなモノが存在しなければ、明らかにオブジェクトは存在しない。</description>
    </item>
    
    <item>
      <title>Step by Step to Add a Protocol Support to Pcap4J (Part 2)</title>
      <link>https://www.kaitoy.xyz/2015/10/12/step-by-step-to-add-a-protocol-support-to-pcap4j-2/</link>
      <pubDate>Mon, 12 Oct 2015 01:00:13 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/10/12/step-by-step-to-add-a-protocol-support-to-pcap4j-2/</guid>
      <description>This is continued from the part 1.
We are adding DHCP support to Pcap4J.

   (adsbygoogle = window.adsbygoogle || []).push({});  Packet Piece Class A packet piece class is a Java class which represents a field of a packet. We should create such classes instead of using a primitive types in some cases.
In the case of DHCP, its flags field includes two fields in itself as like below:</description>
    </item>
    
    <item>
      <title>ORMは不快なアンチパターン</title>
      <link>https://www.kaitoy.xyz/2015/09/13/orm-is-offensive-anti-pattern/</link>
      <pubDate>Sun, 13 Sep 2015 13:52:30 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/09/13/orm-is-offensive-anti-pattern/</guid>
      <description>このエントリでは、Yegor Bugayenkoによる記事、ORM Is an Offensive Anti-Patternを紹介する。 (Yegorから和訳と転載の許可は得た。) 以下はその全文の和訳だが、意訳超訳が混じっているので、もとのニュアンスを知りたければ元記事を読んでもいいし、読まなくてもいい。

   (adsbygoogle = window.adsbygoogle || []).push({});  結論から言えば、ORMはオブジェクト指向プログラミングの原則の全てに違反するひどいアンチパターンだ。オブジェクトをバラバラに引き裂き、もの言わぬ受身なデータ入れに変えてしまう。 小さいWebアプリケーションから、数千のテーブルをCRUD操作するエンタープライズシステムまで、どんなアプリケーションにもORMが存在することはゆるせない。 代わりになるものは? SQLを話すオブジェクトだ。
ORMの仕組み オブジェクト関係マッピング (Object-relatinal mapping、ORM)は、オブジェクト指向言語(例えばJava)からリレーショナルデータベースにアクセスする技術(またはデザインパターン)だ。 ほとんどの言語で複数のORM実装がある。 例えば、JavaのHibernate、Ruby on RalsのActiveRecord、PHPのDoctrine、PythonのSQLAlchemy。 Javaでは、ORMデザインはJPAとして標準化されてさえいる。
最初に、ORMがどう動くかを見てみよう。JavaとPostgreSQLとHibernateを使い、データベースにpost (訳注: ブログポスト、ブログの記事)という単一のテーブルがあるとする。
+-----+------------+--------------------------+ | id | date | title | +-----+------------+--------------------------+ | 9 | 10/24/2014 | How to cook a sandwich | | 13 | 11/03/2014 | My favorite movies | | 27 | 11/17/2014 | How much I love my job | +-----+------------+--------------------------+ で、このテーブルをJavaアプリケーションからCRUD操作したい。(CRUDはcreate、read、update、deleteの略。) まず、Postクラスを書く。(長くてごめん。けどなるべく短くしたんだ。)</description>
    </item>
    
    <item>
      <title>AtomにおけるGIF画像のキャッシュ</title>
      <link>https://www.kaitoy.xyz/2015/09/07/caching-gifs-on-atom/</link>
      <pubDate>Mon, 07 Sep 2015 20:10:31 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/09/07/caching-gifs-on-atom/</guid>
      <description>以前、disturb-meというAtomパッケージを作ったというエントリを書いた。 このエントリでは、disturb-meに見つけたバグの修正のなかで、AtomがGIF画像をキャッシュする問題に対応したという話を書く。

   (adsbygoogle = window.adsbygoogle || []).push({});  disturb-meのバグ 以前のエントリの最後にも書いた通り、disturb-me 1.0.0には、ループしないGIFアニメーション画像を設定で指定した場合、そのアニメーションが画像の初回表示時にしか再生されないというバグがある。
disturb-meは、Ctrl+Alt+d Ctrl+Alt+mと入力すると画像を表示し、もう一度それを入力すると画像を消す。 デフォルトで表示する画像はAtomのロゴで、表示を始める時と消す時にGIF画像でループしないアニメーションを再生する。

このデフォルトの状態で、一度Atomロゴを表示して消して、再度表示して消すと、消すときのアニメーションが再生されない。(表示を始める時のアニメーションはなぜか再生される。)
バグの原因 disturb-meは、imgタグをAtomウィンドウ内に追加した後、そのsrc属性に画像へのパスをセットして画像を表示させるが、どうもAtom(のChromium)が画像をキャッシュしてくれるせいで、一度表示し終わったGIFアニメーションは二度と再生されない模様。 なぜ表示開始時のアニメーションが再生されるかは不明。
バグ修正 外部リソースをロードするときにブラウザによるキャッシュを回避するには、URLにランダムな値をもつクエリストリングを付けるのが常套手段。
今回のバグも、srcにセットするGIF画像のパス(URL)にそのようなクエリストリングをつければよい。 例えば、C:\images\hoge.gifを表示したいなら、&amp;lt;img src=&amp;quot;C:\images\hoge.gif?time=1441559906660&amp;quot;&amp;gt;&amp;lt;img&amp;gt;という風にする。 ここでtimeの値にはDate.now()とかで毎回違う値を生成して使う。
Atomプロトコルの問題 ここで一つ問題が。disturb-meがデフォルトで使うAtomロゴの画像はパッケージに含まれていて、そういうリソースのURLにはAtomプロトコルを使うのが普通。 Atomプロトコルを使うと、atom://disturb-me/assets/atom/white/atom_born.gifみたいに書いて、パッケージ内の相対パスでリソースを指定できる。
このAtomプロトコルが、今の時点(Atom v1.0.11)でクエリストリングに対応していない。困った。
Atomプロトコルの問題への対応 いい機会なので、Atomのソースをfork、cloneして、Atomプロトコルを(簡易的に)クエリストリングに対応させ、ビルドして確認し、プルリクエストを送ってみた。これについてはまた別のエントリで書くかもしれない。
このプルリクエストが取り込まれるまでの暫定対策として、Atomプロトコルハンドラのソースを見て、AtomプロトコルのURLからリソースのファイルシステム上での絶対パスを導いている部分をdisturb-me内にパクって、srcにセットする値としてatom://を使わないようにした。
これでちゃんと動いた。</description>
    </item>
    
    <item>
      <title>Atomウィンドウ内で画像を動かすパッケージ - disturb-me</title>
      <link>https://www.kaitoy.xyz/2015/09/06/disturb-me/</link>
      <pubDate>Sun, 06 Sep 2015 20:18:14 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/09/06/disturb-me/</guid>
      <description>Atomのパッケージを見ていて、便利なパッケージが沢山あるなぁと思いつつ、真面目なパッケージばかりでもつまらないので、たまには不真面目で役に立たないパッケージがあってもいいかと思って作ったパッケージの話。

   (adsbygoogle = window.adsbygoogle || []).push({});  disturb-me 作ったのはdisturb-meというパッケージ。 Ctrl+Altを押しながらdとmを押すとAtomウィンドウ内に画像が表示され、その画像がランダムに動き回り作業の邪魔をするというもの。
画像はパッケージの設定から指定できる。デフォルトではAtomのロゴ。
最初はpac-m●nというパッケージ名にして、ゲーム界のミッキーことパ●クマンが動き回るパッケージにしようと思ってたけど、バンダイナムコからダメだと言われてしまった。 この構想はいつかカタログIPオープン化プロジェクトを利用して実現しようと思う。
disturb-meの作り方 以前別のエントリでAtomパッケージの作り方の基本について書いたので、ここではそこで書かなかったことを書く。
 メインスクリプト - コマンド
今回はコマンドを追加するのでCommandRegistryを使う。 CommandRegistryのインスタンスにはatom.commandsでアクセスでき、そのaddメソッドでコマンドを追加できる。
addメソッドの引数は、第一引数から順に、
 target: コマンドを有効にするDOM要素か、それを示すCSSセレクタ。 commandName: コマンドパレットに表示するコマンド名。全部小文字で、単語をハイフンでつないで、パッケージ名を先頭につけるのがルール。 callback(event): コマンドを実行したときに呼ばれるメソッド。  
disturb-meのコマンドはAtomウィンドウ内のどこでも有効にしたいので、第一引数にはAtomウィンドウを表すカスタムタグであるatom-workspaceを指定する。 コードは以下の感じ。
activate: (state) -&amp;gt; @subscriptions = new CompositeDisposable @subscriptions.add atom.commands.add &amp;#39;atom-workspace&amp;#39;, &amp;#39;disturb-me:toggle&amp;#39;: =&amp;gt; @toggle() toggle: -&amp;gt; # 画像を挿入したり削除したりするコード。 
toggleの中では画像を挿入したり削除したりするわけだけど、この処理は、その画像を表す別のクラスにまかせることにする。 のでtoggleは以下のように書く。
@disturber: null toggle: -&amp;gt; if @disturber? @disturber.stop() @disturber = null else @disturber = new Disturber() document.</description>
    </item>
    
    <item>
      <title>GitHub Pagesでブログ立ち上げ - Hugoを使う</title>
      <link>https://www.kaitoy.xyz/2015/08/28/using-hugo/</link>
      <pubDate>Fri, 28 Aug 2015 23:36:21 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/08/28/using-hugo/</guid>
      <description>GitHub Pagesでブログ立ち上げ - Jekyllのためのツールの続き。 前回は、GitHub Pagesで公開するブログサイトを構築するのに、JekyllとJekyll関連ツールを使おうと四苦八苦したが、結局Jekyllに見切りをつけ、Hugoを使うことに決めた。

   (adsbygoogle = window.adsbygoogle || []).push({});  Hugoとは Hugoは、国内では2014年末くらいから盛り上がってきているブログサイト構築ツール。 そのホームページによると、ウェブサイトフレームワークで、静的サイトジェネレータとのこと。
フレームワークと名乗ってはいるが、その正体は、Markdownで書かれた記事を元にブログサイトのソースを生成するコンテントビルド機能と、記事作成(など)を支援するユーティリティ機能を持ったコマンドラインツール。
また、静的サイトジェネレータというのは、静的なサイトを生成するという意味ではなく、静的にサイトを生成するという意味。もっと言えば、WordPressとかがアクセス時にビルドが走るのに対し、Hugoを使った場合は事前にビルド済みのものをサーバにアップロードすることになる、ということ。らしい。WordPressは使ったことがないのでよく知らないが、Hugoのホームページにそう書いてある。 つまり、Hugoは静的なサイトだけを扱うツールってわけではないので、JavaScriptとかを駆使して動的でインタラクティブなページを作ってもいいはず。
Hugoのインストール インストールガイドに従ってHugoをインストールする。
HugoのGitHub ReleasesからWindows用バイナリをダウンロード。このときはバージョン0.14が最新だったので、hugo_0.14_windows_amd64.zipをダウンロードした。
このzipの中身はhugo_0.14_windows_amd64.exeというバイナリ一つとLICENSE.mdとREADME.mdだけ。 このhugo_0.14_windows_amd64.exeがHugoのすべてなので、これを適当な場所において実行できるようにしとけばよい。 今回は、hugo.batというファイルに以下の内容を書き、PATHの通ったフォルダにいれた。
@echo off C:\Users\Kaito\Desktop\tool\hugo_0.14_windows_amd64\hugo_0.14_windows_amd64.exe%* 
これで、どこからでもhugo [arguments]と打てばHugoコマンドが実行できる。
Hugoのシンタックスハイライト ドキュメントによると、Hugoではシンタックスハイライトを実現する方法を以下の2つから選べる。
 サーバサイド: Hugoでのブログサイト生成時にハイライトしておく方法。 クライアントサイド: クライアントがブログを読み込んだ時にJavaScriptでハイライトする方法。  前者の方が当然クライアントの負荷が軽くなるが、Pygmentsのインストールが必要だったりめんどくさそうなので後者にする。(PygmentsはJekyllのときにすでに入れたけど…)
クライアントサイドでやるのもいくつかやり方があるが、例えばHighlight.jsを使うなら以下をHTMLヘッダに加えるだけでいい。
&amp;lt;link rel=&amp;#34;stylesheet&amp;#34; href=&amp;#34;https://yandex.st/highlightjs/8.0/styles/default.min.css&amp;#34;&amp;gt; &amp;lt;script src=&amp;#34;https://yandex.st/highlightjs/8.0/highlight.min.js&amp;#34;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;script&amp;gt;hljs.initHighlightingOnLoad();&amp;lt;/script&amp;gt; このdefault.min.cssの部分を変えると色々なスタイルが選べる。 このブログではZenburnを使うことにした。
Hugo味見 Hugoコマンドリファレンスを見つつ、Hugoの味見をする。
サイトのひな形を作るコマンドはhugo new site [path]。hugo new site blogを実行して、blogという名のフォルダにサイトの初期ソースを生成。blogの部分はファイルもフォルダも存在しないパスを指定する。
この時点で、blogフォルダ内には以下のものが入っている。
 archetypes: 新規記事作成時に自動で挿入されるFront Matter (後述)のカスタマイズをするためのファイルを置くフォルダ。 content: ブログのコンテンツ(記事など)を置くフォルダ。 data: サイト生成時に使うデータを置くフォルダ。 layouts: サイトのレイアウトを定義するファイルを置くフォルダ。 static: CSSとかJavaScriptとか画像とかのファイルを置くフォルダ。 config.</description>
    </item>
    
    <item>
      <title>GitHub Pagesでブログ立ち上げ - Jekyllのためのツール</title>
      <link>https://www.kaitoy.xyz/2015/08/25/tools-for-jekyll/</link>
      <pubDate>Tue, 25 Aug 2015 22:36:28 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/08/25/tools-for-jekyll/</guid>
      <description>GitHub Pagesでブログ立ち上げ - GitHub PagesとJekyllの続き。 前回は、GitHub PagesとJekyllでブログを始めることにして、Jekyllのセットアップに四苦八苦した。
Jekyllがだいたいセットアップできたところで、どんなサイトデザインにしようか考え始めた。 調べたところ、生のJekyllを使うよりも簡単に見栄えのいいサイトを作れる方法がある模様。

   (adsbygoogle = window.adsbygoogle || []).push({});  Octopress もっとも有名なのはOctopress。 ホームページの説明によると、「Octopress is a framework designed for Jekyll, the static blogging engine powering Github Pages」とのこと。 フレームワークと呼ぶのはちょっと大げさな気がする。 まあ見たところ、Jekyllをサイト生成エンジンとした、ブログサイト構築、ブログエントリ作成、ブログサイトデプロイなどを簡易化するツール。
広く使われていて情報が豊富だし、テーマを選んでエントリの内容をMarkdownで書くだけでかっこいいサイトが作れる。バージョンは2系が主に使われているやつで、3系がβ状態。
血迷って3系に手を出してみる。GitHubにあるREADMEを見ながらWindows 7上にインストールして、適当なサイトを作ろうとするもjekyll buildでエラー。さすがにWindowsじゃだめかと思い、CentOS 7のVMを立ち上げてそこでやってみるもまたjekyll buildでエラー。
心折れかけながらドキュメントなど見ていたら、多くのプラグインがまだ開発中で、3系は基本的な機能しか動かなそうなことが発覚。素直に2系にすることに。
2系は成熟しているし情報が沢山あるので、順調にインストールとテストサイト作成に成功したあたりで、不審な情報を発見した。
Jekyllのドキュメントによると、GitHub Pagesではセキュリティ対策のためにJekyll をセーフモードで実行するため、カスタムプラグインが無効になるとのこと。 Octopressが生成したJekyllソースをGitHub Pagesに上げたらビルドして公開してくれると思っていたけど、OctopressはJekyllのプラグイン機能をもりもり利用しているようなので、上手くいかないようだ。
つまりOctopressをGitHub Pages上のサイトに使うとしたら、結局ビルド成果物をアップしないといけなくなる。JekyllのソースだけをGitHubで管理するように出来たらいいと思っていたが当てが外れた。
Jekyll-Bootstrap Octopressを使うモチベーションが下がり、他のを探したところ、Jekyll-Bootstrapというのを見つけた。
Jekyll-BootstrapはJekyllのソースそのもので、面倒な部分は既にできてるので、ユーザはテンプレートを使って記事の内容を書くだけでいいよ、というもの。テーマ機能と、記事作成作業をRakeで簡易化するためのRakefile付き。
すばらしいことに、「JekyllのソースだけをGitHubで管理するように出来たらいい」という需要に応えることを目指して作られていて、Jekyll-Bootstrapをベースに作ったJekyllソースはGitHub Pages上のJekyllでビルド可能。
まさに求めていたものと心躍った。 が、プロジェクトページを見るにあまり活発に開発が進んでない模様。 廃れ行きそうなツールを使うのもなぁ…
結論 Jekyll-Bootstrapを使うのは気が進まない。Octopressを使うとビルド成果物をアップしないといけない。 どうせビルド成果物を上げるのなら、Jekyllにこだわる必要はないか、ということで、去年末くらいから盛り上がってきているHugoにすることに。Hugoについてはまた別のエントリで書く。</description>
    </item>
    
    <item>
      <title>Atomパッケージを作る - ワード境界を日本語対応させるパッケージ: japanese-word-selection</title>
      <link>https://www.kaitoy.xyz/2015/08/21/japanese-word-selection/</link>
      <pubDate>Fri, 21 Aug 2015 15:31:41 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/08/21/japanese-word-selection/</guid>
      <description>このブログはAtomというGitHubが開発したテキストエディタを使って書いている。 このエントリは、そのAtomのパッケージを作ってみたというお話。

   (adsbygoogle = window.adsbygoogle || []).push({});  Atomとは Atomは、2015/6/25にバージョン1.0がリリースされたばかりの新しいテキストエディタで、そのせいもあってか日本語サポートはあまり充実していない。 例えば、テキストを画面の端で折り返す「Soft Wrap」という機能はマルチバイト文字に対応しておらず、日本語で横に長い文を書いたりすると画面からはみ出てしまって不便。
しかしAtomは、パッケージなる、機能を拡張できるプラグインみたいな仕組みを持っていて、例えば上記Soft Wrapの問題はjapanese-wrapというパッケージをインストールすることで解決できる。 パッケージは誰でも作って配布することができる。
日本語のワード境界 Atomでブログを書いていて不満を感じたのは、日本語のワード境界をちゃんと判定してくれないところ。
以前は(今もたまに)サクラエディタという和製テキストエディタを使っていて、日本語文の中の一語をダブルクリックで選択するという操作をよくやっていた。 例えば、「Atomのパッケージは便利」という文があったら、「パッケージ」の辺りをダブルクリックすると「パッケージ」という単語を選択できる。
Atomでも癖でこの操作をすると、妙に広い範囲が選択されてしまう。 上記例だと「Atomのパッケージは便利」全体が選択されてしまう。不便。
japanese-word-selection この問題を解決してくれそうなパッケージを探したけど見つからなかったので、いい機会と思い自分で作ったのがjapanese-word-selection。ソースはGitHubに。
インストールして有効にすると、日本語のワード境界を判定するようになる。実のところ、とりあえずは文字種の境目を見ているだけ。ひらがな、カタカナ、半角カタカナ、漢字に対応。 特殊文字の全角版の処理どうするとか、あまり深く考えて作ってないけど、使ってて変な挙動を見つけたらおいおい直すということで。
とりあえず、Edit &amp;gt; Text の Delete to Previous Word Boundary と Delete to Next Word Boundary がちゃんと動かないのは見つけた。パッケージで上書きした処理を通っていない気がする。けど、デフォルトでキーバインディングもないし、あまり使われなそうな機能なのでほっておく。
Atomのパッケージの作り方 パッケージの作り方は、Atom Flight Manualのこのあたりを参考に。 Atom Flight ManualにはAtomの使い方からパッケージの作り方まで体系的に纏められているので一度は通して読みたい。
パッケージ開発にあたって、前提として知っておくべきは、AtomはElectronという実行環境の上で動いているということ。 (Atomが先で、そこからElectronがスピンオフした。)
ElectronはざっくりNodeとChromium(Google ChromeのOSS版)でできていて、その上で動くアプリケーションは、HTMLとCSSで書いた画面をChromiumで表示して、それをNodeで動かすJavaScriptで制御する、という形で実装される。AtomはJavaScriptの代わりに、より高級なCoffeeScriptを使っているので、パッケージを作る際はCoffeeScriptのコードをがりがり書くことになる。
パッケージはnpmのパッケージっぽく書く。
AtomはMVVMな感じの設計になっていて、コアのViewModelとかをパッケージからいじることでいろんな機能を実現できる。
以下、備忘録として、japanese-word-selectionを作った時にやったことを書いておく。Atomのバージョンは1.0.7。
 パッケージテンプレート生成
Atomを起動して、Ctrl+Shift+Pでコマンドパレットを開いて、generate packageと入力してEnter。 Package Generatorが起動して、作成するパッケージの名前を聞かれるのでjapanese-word-selectionを入力。(因みに、パッケージ名にatom-というプレフィックスを付けているのをたまに見るが、これは推奨されていない。) するとパッケージのテンプレートが作成され、それを読み込んだAtomウィンドウが開く(下図)。
パッケージ構成については概ね以下の感じ。
 keymaps: キーバインディングを定義するcsonファイルをいれる。 lib: パッケージの機能を実装するCoffeeスクリプトを入れる。  デフォルトで「パッケージ名.</description>
    </item>
    
    <item>
      <title>GitHub Pagesでブログ立ち上げ - GitHub PagesとJekyll</title>
      <link>https://www.kaitoy.xyz/2015/08/15/github-pages-and-jekyll/</link>
      <pubDate>Sat, 15 Aug 2015 10:48:49 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/08/15/github-pages-and-jekyll/</guid>
      <description>このブログを立ち上げたときの作業を、主に備忘録としていくつかのエントリに分けて書く。 このエントリでは主にGitHub PagesとJekyllについて書く。

   (adsbygoogle = window.adsbygoogle || []).push({});  今の構成 このブログは、Hugoで作って、GitHub Pagesで公開している。
Hugoについては別のエントリで書くとして、GitHub Pagesは、GitHubが提供しているウェブページのホスティングサービスで、GitHubに特定の名前のリポジトリ、または任意のリポジトリに特定の名前のブランチを作ってウェブサイトのソースを置くと、公開してくれるというサービス。PaaSにあたるのかな。 GitHub Pagesのサイトに利用方法が載っている。
以下、このブログ立ち上げに向けてやった作業について書く。
GitHub Pages味見 GitHub Pagesを利用するには、GitHubユーザ名.github.io という名前のリポジトリを作るか、任意のリポジトリにgh-pagesという名前のブランチを作って、そこにサイトのソースを置けばいい。そのサイトには、前者の場合はhttp://GitHubユーザ名.github.ioで、後者の場合はhttp://GitHubユーザ名.github.io/リポジトリ名でアクセスできる。 (2016/8/18追記: 今はgh-pagesブランチは不要。)
とりあえず前者をやってみる。
 kaitoy.github.ioという名前のリポジトリを作って、そのルートに「Hello World」とだけ書いたindex.htmlを置く。 ブラウザでhttp://kaitoy.github.ioにアクセスすると、「Hello World」と表示された。  これだけ。
GitHub PagesとJekyll GitHub Pagesには、普通にHTML/CSS/Javascriptのソースを置いてもいいけど、Jekyllを利用することもできる。
Jekyllは、ブログ用の静的サイトジェネレータなるもので、Markdownで書いた記事を元にブログサイトのソースを生成するツール。GitHub Pages用のリポジトリにJekyllのソースをアップロードすると、Jekyllでビルドされ、その結果が公開される。
これはうれしい。 Jekyllのソースとビルド結果を別々に管理しなくてよくて楽だし、公開されるサイトが最新のソースに基づいていることが保証される。
結論から言うと、以下のような理由で結局Jekyllは使わなかったんだけど、Jekyllとの格闘の記録を残しておく。
 Windowsを正式サポートしていない。 Rubyで書かれてるため、ビルドが遅い。ブログエントリが数百とかになると辛くなってくるらしい。 Jekyllを使っても、かっこいいサイトを手軽に作ろうと思ったら、結局ビルド成果物もGitHubに上げないといけなくなる。  Jekyllセットアップ GitHub PagesでJekyll使う場合は、GitHub Pagesと同じJekyll環境を手元に作ってプレビューできるようにしておくべきとのこと。なので、これに従って自分のPC (Windows 7) にJekyllをセットアップする。
 Rubyインストール
JekyllはRubyで書かれてるので、まずはRubyをインストールする。 WindowsなのでRubyInstaller (ver. 2.2.2)をダウンロードしてインストール。 Bundler (RubyのパッケージであるGemの依存をアプリケーションごとに管理するツール) もあるといいらしいので、gem install bundlerを実行してインストール。
 Jekyllインストール</description>
    </item>
    
    <item>
      <title>スタンドアップミーティングはダメマネージャーが好む手法</title>
      <link>https://www.kaitoy.xyz/2015/08/11/daily-stand-up-meetings-are-a-good-tool-for-a-bad-manager/</link>
      <pubDate>Tue, 11 Aug 2015 22:35:09 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/08/11/daily-stand-up-meetings-are-a-good-tool-for-a-bad-manager/</guid>
      <description>このエントリでは、Yegor Bugayenkoによる記事、Daily Stand-Up Meetings Are a Good Tool for a Bad Managerを紹介する。 (Yegorから和訳と転載の許可は得た。) 以下はその全文の和訳だが、意訳超訳が混じっているので、もとのニュアンスを知りたければ元記事を読んでもいいし、読まなくてもいい。

   (adsbygoogle = window.adsbygoogle || []).push({});  スタンドアップミーティング (または単純にスタンドアップ)は、「チームマネージャに状況報告をするためのデイリーチームミーティング」であるとWikipediaに書かれている。 こうしたミーティングは、ソフトウェア開発チームの間でとても人気な手法ではあるが、単なる悪であり、よいマネージャは決してやらない。 以下、その理由を説明する。
私は、スタンドアップのやり方が適切だったり不適切だったりする、と言いたいわけではない。それについて述べた記事は大量にある。 また、スタンドアップを上手く機能するように実施する方法についてアドバイスしようとしているわけでもない。 よいマネージャはデイリースタンドアップを決して実施すべきでないと言っているのだ。 スタンドアップは、単に「機能しない」だけでなく、非常に悪い、時に破壊的なものをマネジメントプロセスにもたらす。それがアジャイルかどうかにかかわらず。 一方、ダメなマネージャは常に、デイリースタンドアップを重要なマネジメント手法として使う。
私の意図を説明するため、マネジメントをいくつかの異なった視点から見ながら、よいマネージャとダメなマネージャが仕事をどのように進めるかを比べてみよう。
情報 ダメなマネージャは進捗を尋ねる。
オフィスを歩き回り進捗を訪ねて回るのは、ひどいマネージャの崇高な習慣だ。 彼は、プロセスと情報伝達フローを適切に構築できるほど賢明ではなく、チームが何をしているかを知らない。 しかし、彼は進捗を知る必要がある。彼もまた上司からちょくちょく尋ねられるからだ。 必要な情報を収集する唯一の方法は、チームに「今何の作業をしているの?」と尋ねることだ。 朝のスタンドアップは、メンバの作業内容を知らないことに気付かれずに、このうっとうしい質問を正式に尋ねる最高の場だ。
よいマネージャは必要なときに報告を受ける。
プロジェクトマネージメントにはコミュニケーション管理が必要だ。 情報伝達フローが適切に構成されていれば、チームメンバはいつどのようにマネージャに報告すればいいかが分かる。 何か問題が起きたとき、そういう状況をどのように報告しなければいけないかを全員が知っている。即時、直接報告するのだ。 作業が完了したとき、必要に応じてプロジェクトマネージャにどのように知らせるかを全員が理解している。 完璧なプロジェクトマネージャは決してチームに質問しない。代わりに、チームが必要なときにマネージャに報告する。 そして、報告を怠るメンバが出たときには、その壊れたコミュニケーションチャネルを修復するのがよいプロジェクトマネージャだ。 ただし、情報収集のためにデイリーミーティングは決して実施しない。
よいマネージャとして、何がゴールで何がプロジェクトマネージャ(またはスクラムマスタ)として重要かをチームに伝えるべきだ。 チームメンバは、マネージャがチームの進捗、リスク、障害、失敗について知るために何が重要であるかを知っているべきだし、チームメンバがマネージャの期待に沿えなければどんなトラブルに陥るかを理解しているべきだ。 プロジェクトやチームが取り組んでいる最も重要な課題についてをチームに伝えることは、よいマネージャとしてすべき仕事だ。 また、よいチームメンバとしては、重要な情報をつかんだら、すぐにマネージャに知らせることが重要だ。 これが完璧なマネージメントというものだ。
もしそのようなチームワークを築いたなら、開発者が今日何をしてどんな問題にあったかを、明日の朝まで待ってから尋ねる必要はなくなる。 マネージャはこういった情報をもっと早く、まさに必要なタイミングで知るようになる。 オフィスの外にいるときでさえ、プロジェクトで起こっていることを知ることができるようになる。 実際には、オフィスは全く不要にさえなるが、これはまた別の機会に議論したい。
デイリースタンドアップはプログラマ間で情報交換する最高の機会で、スクラムマスタに報告してフィードバックを受けるだけの場ではないと言う人がいるかもしれない。 もう一度、同じことを言うが、なぜ、その日の必要になった時点で情報交換をしないのか? なぜ、10人のメンバを毎朝集めて、その内たった5人だけに関係することを議論する必要がある? 答えよう。ダメなマネージャは、チームメンバ間で情報交換する場を用意する他の方法を知らず、朝のスタンドアップを適切なコミュニケーションモデルの代わりとして使う。 こういったミーティングは、マネージャが熱心に働いていて、大げさな給料を受け取るに値するかのような印象を与える。 対照的に、よいマネージャは定期的な状況報告ミーティングをいっさい実施しない。 なぜなら、効果的なコミュニケーションツールの使い方を知っているからだ。 例えば、問題追跡ツール、メール、コードレビュー、意思決定ミーティング、ペアプログラミングなど。
責任 ダメなマネージャはマイクロマネージメントをする。
ダメなマネージャはプロジェクトマネージメントのことをほとんど知らないので、大きな不安を抱えている。 彼はチームのコントロールを失うことを恐れていて、チームを信頼せず、いつも十分な情報を得ていないと感じ、上司から状況を尋ねられたときに動揺する。 このため、彼はチームメンバを抗うつ薬として使う。チームメンバが彼の言う通りのことをしているとき、彼はより安心と安定を感じる。 デイリースタンドアップミーティングは、彼がメンバに何をしているかを尋ね、代わりに何をすべきかを指示するためのすばらしい機会だ。 このマネージャは、メンバに個人の目標と計画を報告するよう強制し、必要だと感じればそれらを修正する。 次のようなやりとりをを何回聞いたことがある?</description>
    </item>
    
    <item>
      <title>Step by Step to Add a Protocol Support to Pcap4J (Part 1)</title>
      <link>https://www.kaitoy.xyz/2015/08/09/step-by-step-to-add-a-protocol-support-to-pcap4j-1/</link>
      <pubDate>Sun, 09 Aug 2015 21:53:29 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/08/09/step-by-step-to-add-a-protocol-support-to-pcap4j-1/</guid>
      <description>I will show how to add a protocol support to Pcap4J in detail giving the example of DHCP (v4) via some posts.

   (adsbygoogle = window.adsbygoogle || []).push({});  Named Number Class First of all, we need to know the packet format. It&amp;rsquo;s explained in RFC 2131 as below:
0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | op (1) | htype (1) | hlen (1) | hops (1) | +---------------+---------------+---------------+---------------+ | xid (4) | +-------------------------------+-------------------------------+ | secs (2) | flags (2) | +-------------------------------+-------------------------------+ | ciaddr (4) | +---------------------------------------------------------------+ | yiaddr (4) | +---------------------------------------------------------------+ | siaddr (4) | +---------------------------------------------------------------+ | giaddr (4) | +---------------------------------------------------------------+ | | | chaddr (16) | | | | | +---------------------------------------------------------------+ | | | sname (64) | +---------------------------------------------------------------+ | | | file (128) | +---------------------------------------------------------------+ | | | options (variable) | +---------------------------------------------------------------+    FIELD OCTETS DESCRIPTION     op 1 Message op code / message type.</description>
    </item>
    
    <item>
      <title>Another way to capture LAN packets with pcap4j container</title>
      <link>https://www.kaitoy.xyz/2015/07/27/another-way-to-capture-lan-packets-with-pcap4j-container/</link>
      <pubDate>Mon, 27 Jul 2015 23:41:49 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/07/27/another-way-to-capture-lan-packets-with-pcap4j-container/</guid>
      <description>2 days ago, I posted an article How to capture packets on a local network with Pcap4J container.
Today, I was reading Docker Docs and found another way to do it. I&amp;rsquo;m writing about it here.

   (adsbygoogle = window.adsbygoogle || []).push({});  &amp;ndash;net option for docker run When we start a docker container we use docker run command. It accepts some options. --net is one of them, which is to set a network mode for a container.</description>
    </item>
    
    <item>
      <title>なぜNullはダメか</title>
      <link>https://www.kaitoy.xyz/2015/07/26/why-null-is-bad/</link>
      <pubDate>Sun, 26 Jul 2015 19:07:20 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/07/26/why-null-is-bad/</guid>
      <description>このエントリでは、Yegor Bugayenkoによる記事、Why NULL is Bad?を紹介する。 (Yegorから和訳と転載の許可は得た。) 以下はその全文の和訳だが、意訳超訳が混じっているので、もとのニュアンスを知りたければ元記事を読んでもいいし、読まなくてもいい。

   (adsbygoogle = window.adsbygoogle || []).push({});  JavaでNULLを使う単純な例を以下に示す。
public Employee getByName(String name) { int id = database.find(name); if (id == 0) { return null; } return new Employee(id); } このメソッドの何が間違っているのか? オブジェクトの代わりにNULLを返す可能性がある、というのが間違っているところだ。 NULLはオブジェクト指向パラダイムにおけるひどい慣習で、全力で避けるべきものだ。 これについては多くの意見が既に発表されている。 たとえば、Tony HoareによるプレゼンNull References, The Billion Dollar Mistakeや、David Westの著書Object Thinkingの全体に渡って述べられている。
ここで、その論拠のすべてをまとめ、NULLの使用を回避して適切なオブジェクト指向構造に置き換える方法の例を紹介したいと思う。
基本的に、NULLの代わりになり得るものはふたつある。
ひとつはNullオブジェクトデザインパターンだ。(それをひとつの不変オブジェクトにするのが最善。)
public Employee getByName(String name) { int id = database.find(name); if (id == 0) { return Employee.</description>
    </item>
    
    <item>
      <title>How to capture packets on a local network with Pcap4J container</title>
      <link>https://www.kaitoy.xyz/2015/07/25/how-to-capture-packets-on-a-local-network-with-pcap4j-container/</link>
      <pubDate>Sat, 25 Jul 2015 19:05:06 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/07/25/how-to-capture-packets-on-a-local-network-with-pcap4j-container/</guid>
      <description>I&amp;rsquo;ll show how to capture packets on a local network with Pcap4J container.

   (adsbygoogle = window.adsbygoogle || []).push({});  Docker network By default, Docker containers are not connected to a local network. They are connected only to a virtual network Docker creates as like below:
  Refer to the Docker doc for more details.
What&amp;rsquo;s a challenge In order to let a Pcap4J container capture packets in a local (real) network, we need to directly connect the container to the local network, because docker0 forwards only packets the destinations of which are in the virtual network.</description>
    </item>
    
    <item>
      <title>Getter/Setterは悪だ。以上。</title>
      <link>https://www.kaitoy.xyz/2015/07/22/getters-setters-evil/</link>
      <pubDate>Wed, 22 Jul 2015 00:21:15 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/07/22/getters-setters-evil/</guid>
      <description>このエントリでは、Yegor Bugayenkoによる記事、Getters/Setters. Evil. Period.を紹介する。 (Yegorから和訳と転載の許可は得た。) 以下はその全文の和訳だが、意訳超訳が混じっているので、もとのニュアンスを知りたければ元記事を読んでもいいし、読まなくてもいい。

   (adsbygoogle = window.adsbygoogle || []).push({});  2003年にAllen Holubが書いたWhy getter and setter methods are evilという有名な記事に端を発する古い議論がある。それは、getter/setterはアンチパターンで避けるべきものなのか、 もしくはオブジェクト指向プログラミングに必須なものなのかというもの。 この議論に少しだけ私の意見を加えたいと思う。
上記記事の要旨はこうだ。 getterやsetterはひどい慣習で、これらを使うやつらはゆるせん。誤解の無いようもう一度言うが、 私はget/setを可能な限り避けるべきだと言っているのではない。それらは君のコードに決して現れてはいけないのだ。
横柄で目につく物言いだろう? 君は15年来get/setパターンを使い続けている尊敬を集めるJavaアーキテクトなんだろう? どこぞの馬の骨にこんなデタラメを言われたくはないだろう? ああ、その気持ちはわかる。私がDavid WestのObject Thinkingという本に出会ったとき、 私もほとんど同じことを感じた。 Object Thinkingは、私が今まで読んだオブジェクト指向プログラミングについての本の中で最高のものだ。 だからお願いだ。ひとまず落ち着いて。私に説明させてほしい。
既存の論拠 オブジェクト指向の世界で、アクセッサ(getterやsetterの別名)に反対する論拠はいくつかあるが、 私にはそれら全てが十分に有力であるとは思えない。ひとつひとつ簡単に見ていこう。

 頼め、尋ねるな
Allen Holub曰く、「ある処理をする際、その処理のために君が欲しい情報をオブジェクトに尋ねてはいけない。 その情報を持ったオブジェクトにその処理をするよう頼みなさい。」
  
 カプセル化原則違反
setterを通してどんな新たなデータも入力できるので、 一つのオブジェクトをその他の様々なオブジェクトが様々に扱うことができてしまう。 また、だれでもオブジェクトを変更できるので、 オブジェクトが単純に自身の状態を安全にカプセル化できない。
  
 実装の詳細の暴露
あるオブジェクトから他のオブジェクトを取得できる場合、前者のオブジェクトの実装の詳細に過度に依存してしまう。 もし明日その実装、例えば返すオブジェクトの型が変わったら、周辺のコードも書き換えないといけない。
  
これらの全ての論拠は正当だが、重要なポイントが抜けている。
根本的な誤解 ほとんどのプログラマはオブジェクトはメソッドを持ったデータ構造だと考えている。 ここでBozhidar Bozhanovによる記事、Getters and Setters Are Not Evilから引用する。</description>
    </item>
    
    <item>
      <title>Pcap4J container with runC</title>
      <link>https://www.kaitoy.xyz/2015/07/19/pcap4j-container-with-runc/</link>
      <pubDate>Sun, 19 Jul 2015 16:25:03 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/07/19/pcap4j-container-with-runc/</guid>
      <description>I tried to run a Pcap4J container with runC.

   (adsbygoogle = window.adsbygoogle || []).push({});  What is Pcap4J? Pcap4J is a Java library for capturing, crafting, and sending packets. It&amp;rsquo;s actually a Java wrapper for libpcap/WinPcap plus packet analyzer. We can see the details in its README.
What is runC? runC is a container runtime developed by Docker and released on June 22, 2015. With runC, we can start a container from a docker image without the docker service or the docker command.</description>
    </item>
    
    <item>
      <title>First Post</title>
      <link>https://www.kaitoy.xyz/2015/07/18/first-post/</link>
      <pubDate>Sat, 18 Jul 2015 13:10:37 -0600</pubDate>
      
      <guid>https://www.kaitoy.xyz/2015/07/18/first-post/</guid>
      <description>初投稿。

   (adsbygoogle = window.adsbygoogle || []).push({});  ブログを立ち上げようと思ったきっかけは、Teamed.ioというCaliforniaのソフトウェアアウトソーシング(?)をやってる会社のCTO、 Yegor Bugayenko (yegor256)のブログのエントリ、How Much Do You Cost?。
これは、Teamed.ioがエンジニアに払うfeeを決める際の指標についてのエントリで、その指標の一つとして Talks and Publications を挙げている。
 Both blog articles and conference presentations make you much more valuable as a specialist. Mostly because these things demonstrate that some people already reviewed your work and your talent. And it was not just a single employer, but a group of other programmers and engineers. This means that we also can rely on your opinions.</description>
    </item>
    
  </channel>
</rss>
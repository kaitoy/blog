<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep learning on To Be Decided</title>
    <link>https://www.kaitoy.xyz/tags/deep-learning/</link>
    <description>Recent content in deep learning on To Be Decided</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© 2015 Kaito Yamada</copyright>
    <lastBuildDate>Sun, 25 Mar 2018 22:43:27 +0900</lastBuildDate>
    
	<atom:link href="https://www.kaitoy.xyz/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>機械学習のHello World: MNISTの分類モデルをKerasで作ってみた</title>
      <link>https://www.kaitoy.xyz/2018/03/25/hello-world-to-ml-with-keras/</link>
      <pubDate>Sun, 25 Mar 2018 22:43:27 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/03/25/hello-world-to-ml-with-keras/</guid>
      <description>機械学習のHello WorldとしてよくやられるMNISTの分類モデルをKeras on TensorFlowで作ってみた話。
   (adsbygoogle = window.adsbygoogle || []).push({});  MNISTとは 手書き数字画像のラベル付きデータセット。 6万個の訓練データと1万個のテストデータからなる。 CC BY-SA 3.0で配布されているっぽい。
一つの画像は28×28ピクセルの白黒画像で、0から9のアラビア数字が書かれている。
画像とラベルがそれぞれ独特な形式でアーカイブされていて、画像一つ、ラベル一つ取り出すのも一苦労する。
Kerasとは Pythonのニューラルネットワークライブラリ。 バックエンドとしてTensorFlowかCNTKかTheanoを使う。 今回はTensorFlowを使った。
やったこと KerasのMNISTのAPIとかコードサンプルとかがあけどこれらはスルー。
MNISTのサイトにあるデータセットをダウンロードしてきて、サイトに書いてあるデータ形式の説明を見ながらサンプルを取り出すコードを書いた。 で、KerasでVGGっぽいCNNを書いて、学習させてモデルをダンプして、ダンプしたモデルをロードしてテストデータで評価するコードを書いた。 コードはGitHubに。
ネットワークアーキテクチャ 入力画像のサイズに合わせてVGGを小さくした感じのCNNを作った。
VGGは2014年に発表されたアーキテクチャで、各層に同じフィルタを使い、フィルタ数を線形増加させるシンプルな構造でありながら、性能がよく、今でもよく使われるっぽい。
VGGを図にすると以下の構造。
実際はバッチ正規化とかDropoutもやるのかも。 プーリング層は数えないで16層なので、VGG-16とも呼ばれる。 パラメータ数は1億3800万個くらいで、結構深めなアーキテクチャ。

VGG-16は244×244×3の画像を入力して1000クラスに分類するのに対し、MNISTは28×28×1を入れて10クラスに分類するので、以下のような7層版を作った。
これでパラメータ数は27万個くらい。 訓練データのサンプル数が6万個なので、パラメータ数が大分多い感じではある。
コードは以下。
inputs: Tensor = Input(shape=(IMAGE_NUM_ROWS, IMAGE_NUM_COLS, 1)) x: Tensor = Conv2D(filters=8, kernel_size=(2, 2), padding=&amp;#39;same&amp;#39;, activation=&amp;#39;relu&amp;#39;)(inputs) x = Conv2D(filters=8, kernel_size=(2, 2), padding=&amp;#39;same&amp;#39;, activation=&amp;#39;relu&amp;#39;)(x) x = MaxPooling2D(pool_size=(2, 2))(x) x = Conv2D(filters=16, kernel_size=(2, 2), padding=&amp;#39;same&amp;#39;, activation=&amp;#39;relu&amp;#39;)(x) x = Conv2D(filters=16, kernel_size=(2, 2), padding=&amp;#39;same&amp;#39;, activation=&amp;#39;relu&amp;#39;)(x) x = Conv2D(filters=16, kernel_size=(2, 2), padding=&amp;#39;same&amp;#39;, activation=&amp;#39;relu&amp;#39;)(x) x = MaxPooling2D(pool_size=(2, 2))(x) x = Flatten()(x) x = Dense(units=256, activation=&amp;#39;relu&amp;#39;)(x) x = Dense(units=256, activation=&amp;#39;relu&amp;#39;)(x) predictions: Tensor = Dense(NUM_CLASSES, activation=&amp;#39;softmax&amp;#39;)(x) model: Model = Model(inputs=inputs, outputs=predictions) model.</description>
    </item>
    
    <item>
      <title>オープンデータメモ</title>
      <link>https://www.kaitoy.xyz/2018/03/04/open-data/</link>
      <pubDate>Sun, 04 Mar 2018 16:22:46 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/03/04/open-data/</guid>
      <description>機械学習の勉強に使えそうなオープンデータのメモ。
   (adsbygoogle = window.adsbygoogle || []).push({});   テキスト  WordNet: 英語の語彙データベース。名詞、動詞、形容詞、副詞ごとに階層的にグルーピングされたDBが提供されている。ライセンスはWordNet Licenseで、著作権表示さえしておけば、目的の制限なく、使用、複製、改変、再配布を無料でできる。 日本語ワードネット: 日本語版WordNet。ライセンスはJapanese WordNetライセンスで、著作権表示さえしておけば、目的の制限なく、使用、複製、改変、再配布を無料でできる。  画像  ImageNet: WordNetの名詞の階層構造に従ってラベル付けされた1400万個以上の画像データ。バウンディングボックスも付いてる。画像はFlickrとかに上がっているもので、そこから自分で無料でダウンロードできる。非商用(研究か教育)目的ならImageNetのサイトから画像をダウンロードできる。 Open Images: 900万個の画像に数千クラスのラベルとバウンディングボックスを付けたデータ。ライセンスはCreative Commons BY 4.0。 MNIST: 手書き数字のラベル付きデータセット。訓練データとテストデータ合わせて7万個。機械学習のHello Worldに使われる。  動画  YouTube-8M: 800万個のYouTube動画を4800クラスでラベル付けしたデータ。ライセンスはCreative Commons BY 4.0。 YouTube-Bounding Boxes: 24万個のYouTube動画に23クラスのラベルと560万個のバウンディングボックスを付けたデータ。ライセンスはCreative Commons BY 4.0。 Atomic Visual Actions(AVA): 5.76万個のYouTube動画を、80種の動作についてラベル付けしたデータ。ライセンスはCreative Commons BY 4.0。  音声  Speech Commands Datase: 6.5万個の1秒音声データで、30種の言葉を数千人が発音してる。ライセンスはCreative Commons BY 4.0。 AudioSet: 200万個の10秒音声データで、527クラスでラベル付けされてる。ライセンスはCreative Commons BY 4.0。  データカタログサイト  DATA GO JP: 日本政府が公開してる公共データ集。 UCI Machine Learning Repository: 現時点で426のデータセットが配布されている。有名なアヤメのデータセットのソースはここ。  単語ベクトル  HR領域の単語ベクトル: 約9.</description>
    </item>
    
    <item>
      <title>CourseraのDeep Learning SpecializationのSequence Modelsコースを修了した</title>
      <link>https://www.kaitoy.xyz/2018/02/27/coursera-deep-learning-sequence-models/</link>
      <pubDate>Tue, 27 Feb 2018 00:49:05 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/02/27/coursera-deep-learning-sequence-models/</guid>
      <description>CourseraのDeep Learning SpecializationのConvolutional Neural Networksコースを修了したのに続き、Sequence Modelsコースを修了した。
   (adsbygoogle = window.adsbygoogle || []).push({});  このコースは、RNNの原理、代表的なアーキテクチャ、自然言語処理などについて学べる3週間のコース。 生成モデルが色々出てきて面白い。 動画は今のところ全部英語。
2018/2/6に始めて、2/27に完了。 22日間かかった。 修了したらまたCertifacateもらえた。
また、これでDeep Learning Specializationのすべてのコースを修了したので、全部まとめたCertifacateももらえた。 結局2ヶ月ほどかかり、1万円以上課金してしまった…
以下、3週分の内容をメモ程度に書いておく。
 1週目
連続データを扱うシーケンス(Sequence)モデルについて学ぶ。 RNN、LSTM、GRU、BRNN。
 動画
 再帰型ニューラルネットワーク(Recurrent Neural Network)
シーケンスモデルにはRNNなどがあって、音声認識(Speech recognition)や自然言語処理(Natural language processing)に使われる。 音楽生成(Music generation)、感情分類(Sentiment classification)、DNA解析(DNA sequence analysis)、動画行動認識(Video Activity Recognition)、固有表現抽出(Named entity recognition)なんてのも。
入力だけが連続データだったり、出力だけが連続データだったり、両方だったり。
自然言語処理では、ボキャブラリ(Vocabulary)を使って単語をone hotベクトルにして扱う。 ボキャブラリは普通5万次元くらいのベクトル。 ボキャブラリにない単語はそれ用(unknown)の次元に割り当てる。
入力や出力の次元がサンプルごとに違うので、普通のNNは使えない。 また、普通のNNだと、文のある個所から学んだ特徴を他の箇所と共有しない。 また、普通のNNだと、入力サイズが大きすぎて、パラメータが多くなりすぎる。 RNNはこうした問題を持たない。
RNNは、最初の単語xを受け取り、層で計算し、最初の出力yとアクティベーションaを出し、そのaと次のxを同じ層で受け取り、次のyとaをだす、ということを繰り返す。 xにかける重みをWax、aにかける重みをWaa、yにかける重みをWyaと呼ぶ。 あとaとyを計算するときに足すバイアスがあって、それぞれba、by。 あるxの計算をするときに、その前のxも使うので、連続データ処理に向いてる。 けど、後のxを考慮しないところが欠点。 この欠点に対処したのがBRNN(Bidirectional RNN)。
RNNのaの活性化関数にはtanhがよく使われる。 ReLUもあり。 yには二値分類だったらシグモイドだし、そうでなければソフトマックスとか。
損失関数は普通に交差エントロピーでいいけど、yがベクトルなので、その各要素について交差エントロピーを計算して、足し合わせたものが損失になる。 ここから逆伝播するんだけど、その際に連続データを過去にさかのぼるので、時をかける逆伝播(BPTT: Backpropagation through time)と呼ばれる。</description>
    </item>
    
    <item>
      <title>CourseraのDeep Learning SpecializationのConvolutional Neural Networksコースを修了した</title>
      <link>https://www.kaitoy.xyz/2018/02/06/coursera-deep-learning-convolutional-neural-networks/</link>
      <pubDate>Tue, 06 Feb 2018 00:37:11 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/02/06/coursera-deep-learning-convolutional-neural-networks/</guid>
      <description>CourseraのDeep Learning SpecializationのStructuring Machine Learning Projectsコースを修了したのに続き、Convolutional Neural Networksコースを修了した。
   (adsbygoogle = window.adsbygoogle || []).push({});  このコースは、CNNの原理、代表的なアーキテクチャ、応用などについて学べる4週間のコース。 動画は今のところ全部英語。 プログラミング課題は初のKeras。
このコースは結構難しくて、特に3週目と4週目は理解に苦しんだ。 というか理解しきれなかったような。 けどNST面白かった。
2018/1/16に始めて、2/6に完了。 22日間かかった。 修了したらまたCertifacateもらえた。
以下、4週分の内容をメモ程度に書いておく。
 1週目
畳み込みニューラルネットワーク(CNN: Convolutional neural network)の基本。
 動画
 畳み込み計算
画像認識でよく使われるNNのアーキテクチャ。
低層ではエッジを検出し、層が進むにつれて複雑な特徴を学習する。
画像を特定の行列(普通は奇数の正方行列。3×3が多い。)で畳み込むことで、特定の方向のエッジを検出できる。 この行列をフィルタ(filter)という。カーネルと呼ばれることもある。 例えば縦なら以下。
[[1, 0, -1], [1, 0, -1], [1, 0, -1]] 縦でもいろいろフィルタはあって、以下はSobelフィルタというもの。
[[1, 0, -1], [2, 0, -2], [1, 0, -1]] 以下はScharrフィルタ。
[[ 3, 0, -3], [10, 0, -10], [ 3, 0, -3]] 縦のフィルタを90度回転すると横のフィルタになる。</description>
    </item>
    
    <item>
      <title>CourseraのDeep Learning SpecializationのStructuring Machine Learning Projectsコースを修了した</title>
      <link>https://www.kaitoy.xyz/2018/01/16/coursera-deep-learning-ml-strategy/</link>
      <pubDate>Tue, 16 Jan 2018 07:56:43 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/01/16/coursera-deep-learning-ml-strategy/</guid>
      <description>CourseraのDeep Learning SpecializationのImproving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了したのに続き、Structuring Machine Learning Projectsコースを修了した。
   (adsbygoogle = window.adsbygoogle || []).push({});  このコースは、深層学習プロジェクトの進め方のコツや問題への対処方法などについて学べる2週間のコース。 今回はプログラミング課題がない。 動画は今のところ全部英語。
ちょっと動画編集ミスが多かった。 同じことを二回言ったり、無音無絵の時間があったり、マイクテストしてたり。
2018/1/13に始めて、1/15に完了。 3日間かかった。 修了したらまたCertifacateもらえた。
以下、2週分の内容をメモ程度に書いておく。
 1週目
モデルの改善をするのに、データを増やしたりハイパーパラメータを変えたり色々な手法がある。 一つを試すのに下手すると数か月とかかかるので、効率よく手法の取捨選択し、モデルを改善していくための戦略について学ぶ。
 動画
 直交化(Orthogonalization)
一つの要素で複数の制御をしようとすると難しいので、一つの制御だけするようにする。 具体的には、以下のことを別々に扱う。
 訓練データに目標の精度までフィットさせる。 devデータに目標の精度までフィットさせる。 テストデータに目標の精度までフィットさせる。 現実のデータでうまく動くようにする。  それぞれの目的について、チューニングすべき要素は別々になる。
早期終了は直行化の原則に反しているので、ほかの方法があるならそっちをやったほうがいい。
 指標(Goal)の設定
モデルの改善はイテレーティブなプロセスなので、サイクルを速く回したい。 そのため、モデルを評価する単一の数値があるといい。 F1スコアとか。平均とか
単一の指標にまとめるのがむずいときもある。 精度と速さとか。 そんなときは一つ以外の指標を足切りだけに使う。 ある閾値以上の速さが出てるもののなかで精度をくらべるなど。
 データの分け方
devデータとテストデータの分布(と評価指標)は同じ感じにしないといけない。 そのために、いったん全データをシャッフルしてから分割する。 訓練データの分布は異なってても問題ない。
訓練:テスト = 70:30とか、訓練:dev:テスト = 60:20:20とかいう比率は、1万くらいのデータなら適当。 けど100万くらいなら、98:1:1くらいが妥当。</description>
    </item>
    
    <item>
      <title>CourseraのDeep Learning SpecializationのImproving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了した</title>
      <link>https://www.kaitoy.xyz/2018/01/12/coursera-deep-learning-improving-deep-neural-networks/</link>
      <pubDate>Fri, 12 Jan 2018 23:41:57 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/01/12/coursera-deep-learning-improving-deep-neural-networks/</guid>
      <description>CourseraのDeep Learning SpecializationのNeural Networks and Deep Learningコースを修了したのに続き、Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了した。
   (adsbygoogle = window.adsbygoogle || []).push({});  このコースは、ディープニューラルネットワークのチューニングなどについて学べる3週間のコース。 今のところ全部英語。
2018/1/5に始めて、1/12に完了。 8日間かかった。 修了したらまたCertifacateもらえた。
以下、3週分の内容をメモ程度に書いておく。
 1週目
OverfittingやUnderfittingを防ぐテクニックについて。
 動画
 データ分割
深層学習のモデル構築は、検討(Idea)、実装(Code)、検証(Experiment)というサイクルの繰り返し(Iterative process)。 取り組む課題、課題のドメイン、データ量、マシンの構成などにより、ハイパーパラメータは変わるので、経験をもって事前に予測することは無理なので、サイクルをどれだけ速く回せるかが鍵。
データは、訓練データ(Training set)、Devデータ(Development set))(a.k.a. Cross-validation set)、テストデータ(Test set)に分割する。 訓練データで学習し、Devデータでハイパーパラメータを評価し、テストデータで最終的な評価と性能見積をする。 テストデータは無くてもいい。
サンプル数が1万くらいだった時代は、6:2:2くらいで分割してたけど、近年は数百万とかのデータを扱い、交差検証データやテストデータの割合はもっと小さくするのがトレンド。 98:1:1など。
Devデータとテストデータは同じようなものを使うべき。 訓練データは必ずしも同様でなくてもいい。訓練データは沢山要るので、別のデータソースからとることもある。
 バイアス vs バリアンス
でかいネットワークで正則化して大量データで学習させるのが吉。
 正則化
過学習(Overfitting)を防ぐため、コスト関数を正則化(Regularization)すべし。
ロジスティック回帰ではL2ノルム(L2 norm)を使ったL2正則化が一般的。 L1正則化はあまり使われない。 L1正則化をすると、wがスパース行列になってモデルを圧縮できると言われることがあるが、経験上その効果はほとんどない。
正則化パラメータλはハイパーパラメータで、Devデータで評価する。
ニューラルネットワークでは正則化項にフロベニウスノルム(Frobenius norm)を使う。
 Dropout(Inverted Dropout)</description>
    </item>
    
    <item>
      <title>CourseraのDeep Learning SpecializationのNeural Networks and Deep Learningコースを修了した</title>
      <link>https://www.kaitoy.xyz/2018/01/05/coursera-deep-learning-neural-networks-and-deep-learning/</link>
      <pubDate>Fri, 05 Jan 2018 15:20:23 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/01/05/coursera-deep-learning-neural-networks-and-deep-learning/</guid>
      <description>CourseraのMachine Learningコースに続いて、同じくAndrew先生によるDeep Learning Specializationを受講中。
これは深層学習の基本を学べるもので、以下の5つのコースからなる。
 Neural Networks and Deep Learning Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization Structuring Machine Learning Projects Convolutional Neural Networks Sequence Models  この内、最初のNeural Networks and Deep Learningを修了したので、記念にブログしておく。
   (adsbygoogle = window.adsbygoogle || []).push({});  Deep Learning Specializationとは Deep Learning SpecializationはCoursera Specializationのひとつ。 Coursera Specializationはサブスクリプションモデルで、つまりあるSpecializationのサブスクリプションを購入すると、受講完了するまで毎月定額の料金を支払うことになる。
Deep Learning Specializationは月$49で、5コース合わせて16週分の内容。 最初の7日間はトライアルで無料なので、この間に全部終わらせられればタダ。 無理だけど。
Deep Learning Specializationでは、PythonとTensorFlowでディープニューラルネットワーク、CNN、RNN、LSTM、Adam、Dropout、バッチ正規化、Xavier/He initializationなどを学べる。 Machine Learningコースと同じく、5分～15分くらいの動画による講義と、小テストと、プログラミング課題から構成されている。
プログラミング課題は、coursera hubという、ホステッドJupyter Notebookで解いて提出できるので楽。
Neural Networks and Deep Learningコースとは ディープニューラルネットワークの仕組みを学んで実装する4週間のコース。 また、深層学習の偉い人へのインタビューを見れる。 Machine Learningコースと被っている内容が少なくなく、かなり楽だったが、結構ペースが速いので、Machine Learningコースをやっていなかったら辛かったと思う。</description>
    </item>
    
  </channel>
</rss>
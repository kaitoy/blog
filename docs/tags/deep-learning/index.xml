<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>To Be Decided </title>
    <link>https://www.kaitoy.xyz/tags/deep-learning/</link>
    <language>en-us</language>
    <author>Kaito Yamada</author>
    <rights>(C) 2018</rights>
    <updated>2018-02-06 00:37:11 &#43;0900 JST</updated>

    
      
        <item>
          <title>CourseraのDeep Learning SpecializationのConvolutional Neural Networksコースを修了した</title>
          <link>https://www.kaitoy.xyz/2018/02/06/coursera-deep-learning-convolutional-neural-networks/</link>
          <pubDate>Tue, 06 Feb 2018 00:37:11 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2018/02/06/coursera-deep-learning-convolutional-neural-networks/</guid>
          <description>&lt;p&gt;&lt;a href=&#34;https://www.kaitoy.xyz/2018/01/16/coursera-deep-learning-ml-strategy/&#34;&gt;CourseraのDeep Learning SpecializationのStructuring Machine Learning Projectsコースを修了した&lt;/a&gt;のに続き、&lt;a href=&#34;https://www.coursera.org/learn/convolutional-neural-networks&#34;&gt;Convolutional Neural Networksコース&lt;/a&gt;を修了した。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;このコースは、CNNの原理、代表的なアーキテクチャ、応用などについて学べる4週間のコース。
動画は今のところ全部英語。
プログラミング課題は初のKeras。&lt;/p&gt;

&lt;p&gt;このコースは結構難しくて、特に3週目と4週目は理解に苦しんだ。
というか理解しきれなかったような。
けどNST面白かった。&lt;/p&gt;

&lt;p&gt;2018/1/16に始めて、2/6に完了。
22日間かかった。
修了したらまた&lt;a href=&#34;https://www.coursera.org/account/accomplishments/certificate/MVNK5ZA5CDKA&#34;&gt;Certifacate&lt;/a&gt;もらえた。&lt;/p&gt;

&lt;p&gt;以下、4週分の内容をメモ程度に書いておく。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1週目&lt;/p&gt;

&lt;p&gt;畳み込みニューラルネットワーク(CNN: Convolutional neural network)の基本。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;畳み込み計算&lt;/p&gt;

&lt;p&gt;画像認識でよく使われるNNのアーキテクチャ。&lt;/p&gt;

&lt;p&gt;低層ではエッジを検出し、層が進むにつれて複雑な特徴を学習する。&lt;/p&gt;

&lt;p&gt;画像を特定の行列(普通は奇数の正方行列。3×3が多い。)で畳み込むことで、特定の方向のエッジを検出できる。
この行列をフィルタ(filter)という。カーネルと呼ばれることもある。
例えば縦なら以下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[[1, 0, -1],
 [1, 0, -1],
 [1, 0, -1]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;縦でもいろいろフィルタはあって、以下はSobelフィルタというもの。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[[1, 0, -1],
 [2, 0, -2],
 [1, 0, -1]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下はScharrフィルタ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[[ 3, 0,  -3],
 [10, 0, -10],
 [ 3, 0,  -3]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;縦のフィルタを90度回転すると横のフィルタになる。&lt;/p&gt;

&lt;p&gt;深層学習では、フィルタもパラメータとして学習させる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;パディング(Padding)&lt;/p&gt;

&lt;p&gt;&lt;code&gt;n×n&lt;/code&gt;の行列を&lt;code&gt;f×f&lt;/code&gt;のフィルタで畳み込むと&lt;code&gt;n-f+1×n-f+1&lt;/code&gt;の行列になる。
つまり畳み込めば畳み込むほど画像が小さくなってしまう。
また、画像の端のほうはフィルタにかかる割合が小さいので、情報量が小さくなってしまう。
これらを解決するテクニックがパディング(Padding)。
行列の周囲を0でパディングして、サイズを大きくしてから畳み込む。
パディングがないのをValidな畳み込み、出力が入力と同じサイズになるようにパディングするのをSameな畳み込みという。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Strided畳み込み&lt;/p&gt;

&lt;p&gt;畳み込むときにフィルタをずらす幅を1より大きくする。
パディングサイズがpでストライドがsのとき、&lt;code&gt;n×n&lt;/code&gt;の行列を&lt;code&gt;f×f&lt;/code&gt;のフィルタで畳み込むと&lt;code&gt;(n+2p-f)/s+1×(n+2p-f)/s+1&lt;/code&gt;の行列になる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;3次元(カラー画像)の畳み込み&lt;/p&gt;

&lt;p&gt;カラー画像は3次元の行列、つまり&lt;code&gt;n×n×c&lt;/code&gt;の行列で、それを畳み込むのは&lt;code&gt;f×f×c&lt;/code&gt;のフィルタで、出力は&lt;code&gt;n-f+1×n-f+1&lt;/code&gt;の行列になる。
チャネルごとにフィルタを設定して、色ごとにエッジ検出できる。
フィルタごとの出力は全部スタックして、最終的な出力は3次元になる。&lt;/p&gt;

&lt;p&gt;畳み込み層はフィルタの要素数がパラメータ数になる。
入力画像の大きさに依存しないので、パラメータ数が少なくて済み、過学習しにくい。&lt;/p&gt;

&lt;p&gt;入力を複数の畳み込み層に通したら、最終的に3次元の出力をなべてベクトルにして、後ろの層に渡す。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プーリング層(Pooling layer)&lt;/p&gt;

&lt;p&gt;計算量を減らすため、また特徴の抽出のために、畳み込み層のあとに使われる層。
基本Max poolingが使われるけど、Average poolingというのもある。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Max pooling: フィルタをかけた部分を畳み込む代わりに、最大値を出力とする。大きな値が特徴が大きく出ているところだから、特徴を凝縮するイメージだけど、経験的にこれで上手くいくことが分かっているだけで、なぜ上手くいくかは判明していない。この層はパラメータを持たない。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Average pooling: フィルタをかけた部分を畳み込む代わりに、平均を出力とする。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;プーリング層のフィルタは大抵、サイズが&lt;code&gt;2×2&lt;/code&gt;でパディングが0でストライドは2。&lt;/p&gt;

&lt;p&gt;普通、畳み込み層とプーリング層とセットで1層と数える。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;全結合層(Fully connected layer)&lt;/p&gt;

&lt;p&gt;全ノードがメッシュ状につながった普通の層。
畳み込み層とプーリング層のセットがいくつかあって、その出力をベクトルになべて、全結合層につなぐ。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;一般的なCNN&lt;/p&gt;

&lt;p&gt;畳み込み層は、普通nhとnwを縮め、ncを増やす。
また、全体として、層が浅くなるほど出力が減るのが多い。&lt;/p&gt;

&lt;p&gt;CNNはハイパーパラメータが多すぎるので、アーキテクチャは自分で考えるんではなく、論文呼んで自分の問題に合いそうなのを探すべし。&lt;/p&gt;

&lt;p&gt;畳み込み層は全結合層に比べてパラメータ数がかなり少なくて済むのがいいところ。
これはパラメーター共有(Parameter sharing)という、画像のある個所で上手く動いたフィルタ(e.g. 縦エッジ検出器)は、その画像の他の箇所でも上手く働くという考え方がベース。&lt;/p&gt;

&lt;p&gt;また、層間の接続がまばらなのもパラメータを減らす要因。
つまり出力のあるピクセルは、入力のうちフィルタ分のサイズのピクセルとしか関連していない。&lt;/p&gt;

&lt;p&gt;CNNは空間変化の不変性(Translation invariance)に強い。
つまり画像の中の物体の位置が変わってもよく検出できる。
これは同じフィルタを画像全体に適用するから。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;p&gt;CNNの順伝播をNumPyで実装。&lt;/p&gt;

&lt;p&gt;CNNによる画像の分類をTensorFlowで実装。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2週目&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ケーススタディ&lt;/p&gt;

&lt;p&gt;畳み込み層とかプーリング層をどう組み合わせるといいかは、事例を見ていくことで雰囲気をつかめる。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;古いやつ&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;LeNet-5&lt;/p&gt;

&lt;p&gt;1980年代にできたふるいやつ。
モノクロ画像(32×32)の手書き数字認識。&lt;/p&gt;

&lt;p&gt;当時はソフトマックスもReLUもなかった。
けど、畳み込み層とプーリング層のセットを繰り返して入力をチャネル方向に引き伸ばし、全結合層に流し込むアーキテクチャは、モダンなCNNにも通じる。&lt;/p&gt;

&lt;p&gt;5層(内2層が全結合層)の浅いネットワークで、比較的パラメータが少なく、6万個くらい。
モダンなのだとこの1000倍くらいあるのが普通。&lt;/p&gt;

&lt;p&gt;LeNet-5は、チャネルごとに違うフィルタを使っているが、今日では普通同じのを使う。&lt;/p&gt;

&lt;p&gt;また、プーリング層のあとに活性化関数(シグモイド)かけてるのも特殊。
(モダンなアーキテクチャではプーリング層の前にかける?)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;AlexNet&lt;/p&gt;

&lt;p&gt;227×227×3のカラー画像
8層(内3層が全結合層)でパラメータは6千万個くらい。
活性化関数にReLU。&lt;/p&gt;

&lt;p&gt;Local Response Normalizationという正規化層がある。
昨今ではあまり使われない。&lt;/p&gt;

&lt;p&gt;論文が比較的読みやすい。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;VGG-16&lt;/p&gt;

&lt;p&gt;2014年に発表。&lt;/p&gt;

&lt;p&gt;各層に同じフィルタを使い、フィルタ数も線形増加させるシンプルなアーキテクチャ。
16層(内2層が全結合層)で、1億3800万個のパラメータ。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;モダンなやつ&lt;/p&gt;

&lt;p&gt;理論的にはネットワークを深くすると精度が高くなるけど、現実的にそうはいかない。
深いネットワークは勾配消失や勾配爆発で訓練しにくいので。
モダンなアーキテクチャはこの問題に対応。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ResNet(Residual Network)&lt;/p&gt;

&lt;p&gt;残差ブロック(Residual block)を持つ。
このブロックでは、浅い層からの出力を深い層のReLUの入力に足し合わせる。
この深い層からの依存はショートカット(short cut)とかskip connectionとか呼ばれる。&lt;/p&gt;

&lt;p&gt;ショートカットのおかげで深い層の学習が効率的になり、層を152まで深くできた。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Network in Network&lt;/p&gt;

&lt;p&gt;畳み込み層に1×1のフィルタを使う。
1×1畳み込み(one by one convolution)、またはNetwork in Networkと呼ばれる。&lt;/p&gt;

&lt;p&gt;これを使うと、入力のhとwを変えずに、チャネル数を減らして計算量を減らしたり、非線形性を追加することができる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Inception Network (GoogleNet)&lt;/p&gt;

&lt;p&gt;フィルタのサイズや畳み込みかプーリングかを考えるのが難しいので、1層内で複数のフィルタサイズで畳み込みやプーリングして、スタックしたものを出力する手法がある。
これをする部分をInception moduleという。
計算コストが大きくなるので、最初に1×1畳み込みで圧縮してからその後の畳み込みをする。
1×1畳み込みの部分でデータがいったん小さくなるので、そこをボトルネック層(Bottleneck layer)と呼ぶ。&lt;/p&gt;

&lt;p&gt;ボトルネック層によって、精度に影響が出ることはない。&lt;/p&gt;

&lt;p&gt;Inception moduleを組み合わせたネットワークをInception Networkという。
Inception Networkの例の一つがGoogLeNet。
GoogLeNetは中間層から全結合層・ソフトマックス層につなげる支流をもっていて、中間層まででうまく学習できているかを見れて、過学習を防げるようになっている。&lt;/p&gt;

&lt;p&gt;因みにInceptionという名前は映画のInceptionから来ている。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;実践&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;既存の実装の利用&lt;/p&gt;

&lt;p&gt;モダンなCNNは複雑すぎて、エキスパートが論文を読み込んでも再現することが難しい。
が、普通は論文の著者がOSSで実装を公開するのでそれを使ったりベースにしたりすべし。&lt;/p&gt;

&lt;p&gt;学習済みのモデルもあることがあるので、転移学習にも使える。
ソフトマックス層だけ入れ替えて、そこのWだけ学習させて自分の問題に使うなど。
色んな深層学習フレームワークが転移学習をサポートしてる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;データ合成(Data augmentation)&lt;/p&gt;

&lt;p&gt;画像認識の分野では基本的にデータが沢山要るけどデータが手に入りにくい。
ので合成するのが効果的。&lt;/p&gt;

&lt;p&gt;左右判定とか、切り抜きとか、回したり、歪めたりとかは、有効だけどあんまりやられない。
若干編集が複雑なので。&lt;/p&gt;

&lt;p&gt;色相を変える(Color shifting)のがよくやられる。赤味を増やしたり。
色を選ぶときには主成分分析(PCA)が使える。(PCA Color Augmentation)&lt;/p&gt;

&lt;p&gt;一つのCPUスレッドに元画像のロードと合成をやらせて、別のスレッドで並列に学習を処理するのが一般的な実装。&lt;/p&gt;

&lt;p&gt;データ合成するにも、どの程度変化させるかというハイパーパラメータが付きまとうので、既存の実装やアイデアを使うのがいい。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;画像認識の現状&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;データ vs hand engineering&lt;/p&gt;

&lt;p&gt;データが沢山ある分野の問題だと、でかいネットワークを適当に組んで学習させれば上手く解ける。
データがあんまりないと、色々工夫(hand engineering)が必要になってくる。
特徴量を選んだり、アーキテクチャを工夫したり。
例えば物体検知は画像認識よりかなりデータが少ない。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ベンチマークやコンペで上手くやるコツ&lt;/p&gt;

&lt;p&gt;研究者は、論文を通しやすくするため、ベンチマークやコンペのデータに対して頑張る。
ベンチマークに対して上手くやるコツ:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;アンサンブル(Ensembling)&lt;/p&gt;

&lt;p&gt;複数のNNを独立に訓練して、それらの出力の平均を使う。
1,2%の性能向上が見込める。
けど計算コストが高いので、普通プロダクションでは使わない。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Multi-crop at test time&lt;/p&gt;

&lt;p&gt;テスト時にテストデータを色んな感じに切り抜いて、それらに対する予測値を平均する。
10-crop。
アンサンブルに比べ、訓練時の計算コストが少ないし、予測時に1つのモデルを保持すればいいのでメモリ使用量が少ない。
若干の性能向上が見込め、プロダクションでも使われることがある。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;オープンソースコードの利用&lt;/p&gt;

&lt;p&gt;だれかが考えたアーキテクチャを使え。&lt;/p&gt;

&lt;p&gt;OSS実装を使え。&lt;/p&gt;

&lt;p&gt;なんなら訓練済みモデルを使え。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Kerasのチュートリアル&lt;/p&gt;

&lt;p&gt;というほど解説してくれるわけではないけど。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Kerasで50層のResNetを実装&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;3週目&lt;/p&gt;

&lt;p&gt;物体認識(Object detection)。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;位置特定(Localization)&lt;/p&gt;

&lt;p&gt;画像を与えられて単にラベルを付けるのは分類。
ラベルの物体の位置を示すのが位置特定。
分類したあとさらに位置特定したい。&lt;/p&gt;

&lt;p&gt;分類する画像は、普通一つの画像の中に一つの物体が大きく映っている。
一方、物体認識は、一つの画像の中に複数の物体があったりする、もう少し複雑な問題。&lt;/p&gt;

&lt;p&gt;位置特定するには、ソフトマックス層に、クラス以外に4つの出力をさせる。
すなわち物体の中心点のx座標、y座標、それと物体を囲む枠の高さ、幅。
それぞれ0～1の値で、画像全体に対する割合を示す。&lt;/p&gt;

&lt;p&gt;また、物体があるかないかという予測値Pcも出力する。
この予測値が0のときは、損失関数でそれいがいの出力を計算に入れない。&lt;/p&gt;

&lt;p&gt;より一般的には、物体の位置を示す任意の数のランドマーク(Landmark)の座標を出力させる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;物体認識&lt;/p&gt;

&lt;p&gt;スライディングウィンドウ認識(Sliding windows detection)する。
すなわち、小さい枠をずらしながら画像の切り抜きをたくさん作って、それぞれ分類する。
ウィンドウサイズを変えてもやる。
計算コストがかかる。&lt;/p&gt;

&lt;p&gt;のでCNNでやる。&lt;/p&gt;

&lt;p&gt;まず、全結合層は、数学的に等価な畳み込み層で置き換えられる。
5×5×16の入力を受け取って、5×5の400個のフィルタで畳み込むと、
1×1×400の出力が得られ、これは400ノードの全結合層と一緒。
最後に1×1の4個のフィルタで畳み込むと、4つの出力をするソフトマックス層みたいになる。
こういう、全部畳み込み層のNNをFCN(Fully Convolutional Networks)という。&lt;/p&gt;

&lt;p&gt;で、入力のサイズ(高さと幅)を広げてやると、中間層と出力もちょっと広がる。
このCNNは、入力の一部を5×5のウィンドウで切り抜いた部分の分類結果が、出力の1ピクセルに対応するようなものになる。
なので一回CNNに通せば、一回の計算でスライディングウィンドウできる。&lt;/p&gt;

&lt;p&gt;けどこれは、物体を囲む枠が正確でないという欠点がある。
実際は長方形であるべきだったりするので。
これを解決するのがYOLO(You Only Look Once)アルゴリズム。&lt;/p&gt;

&lt;p&gt;YOLOでは、まず入力画像をグリッド状に分割して、それぞれについて分類と位置特定する。
複数のセルに物体がまたがっている場合は、物体の中心があるセルだけにあるものとする。
それぞれのセルの出力をスタックして、3次元の出力にする。
つまり、グリッドが3×3なら、3×3×(もとのyベクトルの次元)とする。
(普通はもっと細かいグリッドにする。)
で、CNNをこういう形の出力をするように組む。&lt;/p&gt;

&lt;p&gt;YOLOの論文はかなりむずい。&lt;/p&gt;

&lt;p&gt;位置特定の評価をするのに、IoU(Intersection over Union)という指標がある。
これは、2つの領域の重なり具合を示すもので、2つの領域が重なった部分の面積を、2つの領域全体の面積で割った値。
モデルが特定した枠と期待する枠とで、IoUが0.5以上だとよしとすることが多い。&lt;/p&gt;

&lt;p&gt;YOLOを使うと、複数のセルで同じ物体を認識してしまうことが多い。
これを一つに絞るのがNMS(Non-max suppression)。
ざっくり言うと、それぞれのセルの確度(Pc)を見て、一番でかいの以外を無効化する。&lt;/p&gt;

&lt;p&gt;詳しく言うとまず、Pcがある閾値(e.g.0.6)以下のものを無効化する。
で、残ったものの中から、最大のPcを選び、それとおおきくかぶっている枠(IoUが0.5以上など)を無効化する。
で、また残ったのものの中から最大のPcを選び、同じことを繰り返していく。
クラスが複数あったら、これをクラスごとにやる。&lt;/p&gt;

&lt;p&gt;一つのセルに複数の物体があったらどうか。
境界ボックス(Anchor box)を使う。
事前に複数の形の枠(境界ボックス)を用意しておいて、それぞれについての予測を出力ベクトルに並べる。
で、一番IoUが高い境界ボックスを採用する。&lt;/p&gt;

&lt;p&gt;境界ボックスは手動で作ったり、k平均法で作ったりする。&lt;/p&gt;

&lt;p&gt;スライディングウィンドウは、明らかに何もない部分の計算もしちゃうのでちょっと無駄。
なので、そういう部分はスキップしようというのがR-CNN(Regions with CNN)。
これはRegion proposalsとCNNを組み合わせたもの。
Region proposalsは、セグメンテーション(Segmentation)アルゴリズムで画像をざっくり区分けして、それっぽい部分を処理対象にするもの。&lt;/p&gt;

&lt;p&gt;R-CNNはすごく遅いので、あまり使われていないし、Andrew先生も好んで使わない。
Fast R-CNN、Faster R-CNNってのもあるけど、まだ遅い。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;KerasでYOLOv2モデルを実装。&lt;/p&gt;

&lt;p&gt;CNN部分は訓練済みのモデルを使って、出力をフィルタリングする部分を作る。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;4週目&lt;/p&gt;

&lt;p&gt;顔認識(Face recognition)とNeural style transfer。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;顔認識&lt;/p&gt;

&lt;p&gt;顔認証(Face authentication)には、顔認識と、生きた人間かの判定(Liveness detection)が要るけど、前者を主に学ぶ。&lt;/p&gt;

&lt;p&gt;顔認識は顔検証(Face verification)の難しい版。
後者は顔画像と名前を与えて、正しい組み合わせかを判定する。
前者は顔画像を与えて、DBからその人を探す。&lt;/p&gt;

&lt;p&gt;顔認識は一般的に、One-shot learning問題に対応する必要がある。
つまり一つの訓練データから学習しないといけない。
DBに一人のひとについて何個も画像があるというケースは少ない。&lt;/p&gt;

&lt;p&gt;CNNに顔画像を入力して、ソフトマックス層で分類するのは、訓練データが少なすぎるのでうまくいかない。
代わりに類似関数(Similarity function)を学習する。
つまり、二つの画像を入力として、異なる度合いを出力するもの。
で、その出力が閾値以下だったら同一人物と判定する。
これをDBに入っている画像それぞれについてやる。&lt;/p&gt;

&lt;p&gt;類似関数にはシャム(Siamese)ネットワークをつかう。&lt;/p&gt;

&lt;p&gt;CNNの最後の全結合層の出力ベクトルは、入力画像をエンコードしたものだと考えられる。
二つの画像を、別々に同じCNNにいれて、二つの出力ベクトルを得たら、それらのユークリッド距離の二乗を差として出力する。
これがシャムネットワーク。
二つの画像が同一人物ならユークリッド距離が小さくなるように、違うなら大きくなるように訓練する。&lt;/p&gt;

&lt;p&gt;損失関数にはTriplet loss関数を使う。
同一人物を比較するとき、Anchor画像とPositive画像の比較、違う人物の比較はAnchor画像とNegative画像の比較と呼ぶ。
AnchorとPositiveのユークリッド距離がAnchorとNegativeのユークリッド距離以下になってほしい。
つまり前者マイナス後者がゼロ以下になって欲しい。
ただこれだと、CNNが全ての画像について同じ出力をするように学習してしまうかもしれないので、&lt;code&gt;0-α&lt;/code&gt;以下になるように訓練する。
αはマージンと呼ばれるハイパーパラメータ。&lt;/p&gt;

&lt;p&gt;AnchorとPositiveとNegativeの一組をTripletと呼ぶ。
Negativeをランダムに選ぶと、全然違う顔の組み合わせが多くなって、類似関数があまり学習しない。
ので、似てるひとを組み合わせたTripletを多く作ってやると効率よく学習する。&lt;/p&gt;

&lt;p&gt;シャムネットワークの二つの出力ベクトルをロジスティック回帰ユニットに入れて、同一人物か否かの二値分類する方法もある。
ベクトル間の距離も、ユークリッド距離の他、カイ二乗値(χ square similarity)ってのもある。&lt;/p&gt;

&lt;p&gt;DBには、顔画像そのものよりも、エンコードしたベクトルを入れておくと計算量を省けるし、DBサイズも抑えられる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ニューラル画風変換(NST: Neural style transfer)&lt;/p&gt;

&lt;p&gt;Content画像&amp;copy;とStyle画像(S)から、あらたな画像(G)を生成するCNN。
(CNNは別途訓練済みのものを使うので、転移学習の一種。)&lt;/p&gt;

&lt;p&gt;CNNの可視化ができる。
画像の一部を入力して、ある層の一つのユニットの出力が最大になるものを選んで集めると、そのユニットがどのような特徴を抽出しているかがわかる。
深い層ほど広い範囲を見て、複雑なパターンを学ぶ。&lt;/p&gt;

&lt;p&gt;コスト関数&lt;code&gt;J(G)&lt;/code&gt;を最小化する。
&lt;code&gt;J(G)&lt;/code&gt;はコンテントコスト&lt;code&gt;Jcontent(C, G)×α&lt;/code&gt;とスタイルコスト&lt;code&gt;Jstyle(S, G)×β&lt;/code&gt;の和。
前者はCとGの類似関数で、後者はSとGの類似関数。
αとβはハイパーパラメータ。
Gをランダムに初期化して、最急降下法でGを調整していく。&lt;/p&gt;

&lt;p&gt;訓練済みのCNN(VGGなど)を使って、中間層lを選ぶ。
Cを入力してlから出てきた値と、Gを入力してlから出てきた値が似てたら、CとGを似ているとする。
つまりそれらをベクトルにアンロールしたものの二乗誤差が&lt;code&gt;Jcontent(C, G)&lt;/code&gt;。
lが浅ければ浅いほど、CとGは似たものになる。&lt;/p&gt;

&lt;p&gt;スタイルは、ある層lの出力のx座標とy座標の各点について、チャネル間で値の関連性を見る。
あるチャネルのあるニューロンが縦縞を検出していて、ほかのチャネルのあるニューロンがオレンジ色を検出していたとしたら、画像にオレンジの縦縞がよく現れるなら両者は関連が高く、そうでなければ低い。
SとGとの間でこの関連性が似てれば、スタイルが似ていると言える。&lt;/p&gt;

&lt;p&gt;スタイル行列(Style matrix)で表す。
ある層のスタイル行列は&lt;code&gt;nc×nc&lt;/code&gt;で、チャネル間の関連度を表す。
行列の一つの値は、二つのチャネルの各アクティベーションの掛け合わせものの合計。
関連性が強いとこの掛け合わせは大きくなる。
(対角成分は同じチャネル同士の積になって、そのスタイルがどれだけ全体的に出ているかを示す。)
この行列は代数学ではグラム行列(Gram matrix)と呼ばれる。&lt;/p&gt;

&lt;p&gt;このスタイル行列をSとGで計算して、それらの二乗誤差をl層のスタイルコストとする。
で、これにハイパーパラメータλlをかけたものを層ごとに計算して、足し合わせたものを全体のスタイルコストとする。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2D以外の画像の処理&lt;/p&gt;

&lt;p&gt;心電図のデータとかの1Dデータや、CTスキャンみたいな3Dデータでも、それに合わせた次元のフィルタを使えば畳み込める。
1DデータはRNNでもできるけど、CNNでもできて、それぞれメリットデメリットがある。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;TensorFlowでニューラル画風変換を実装&lt;/p&gt;

&lt;p&gt;VGG-19をImageNetのデータで訓練したものを使う。
ルーブル美術館の写真をContent画像に、モネの絵をStyle画像に使って画像を生成。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Kerasで顔認識モデルを実装&lt;/p&gt;

&lt;p&gt;現時点でtriplet_lossの採点にバグがある。
対策はフォーラム参照。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    
      
        <item>
          <title>CourseraのDeep Learning SpecializationのStructuring Machine Learning Projectsコースを修了した</title>
          <link>https://www.kaitoy.xyz/2018/01/16/coursera-deep-learning-ml-strategy/</link>
          <pubDate>Tue, 16 Jan 2018 07:56:43 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2018/01/16/coursera-deep-learning-ml-strategy/</guid>
          <description>&lt;p&gt;&lt;a href=&#34;https://www.kaitoy.xyz/2018/01/12/coursera-deep-learning-improving-deep-neural-networks/&#34;&gt;CourseraのDeep Learning SpecializationのImproving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了した&lt;/a&gt;のに続き、&lt;a href=&#34;https://www.coursera.org/learn/machine-learning-projects&#34;&gt;Structuring Machine Learning Projectsコース&lt;/a&gt;を修了した。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;このコースは、深層学習プロジェクトの進め方のコツや問題への対処方法などについて学べる2週間のコース。
今回はプログラミング課題がない。
動画は今のところ全部英語。&lt;/p&gt;

&lt;p&gt;ちょっと動画編集ミスが多かった。
同じことを二回言ったり、無音無絵の時間があったり、マイクテストしてたり。&lt;/p&gt;

&lt;p&gt;2018/1/13に始めて、1/15に完了。
3日間かかった。
修了したらまた&lt;a href=&#34;https://www.coursera.org/account/accomplishments/certificate/7MHFMLHP67C4&#34;&gt;Certifacate&lt;/a&gt;もらえた。&lt;/p&gt;

&lt;p&gt;以下、2週分の内容をメモ程度に書いておく。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1週目&lt;/p&gt;

&lt;p&gt;モデルの改善をするのに、データを増やしたりハイパーパラメータを変えたり色々な手法がある。
一つを試すのに下手すると数か月とかかかるので、効率よく手法の取捨選択し、モデルを改善していくための戦略について学ぶ。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;直交化(Orthogonalization)&lt;/p&gt;

&lt;p&gt;一つの要素で複数の制御をしようとすると難しいので、一つの制御だけするようにする。
具体的には、以下のことを別々に扱う。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;訓練データに目標の精度までフィットさせる。&lt;/li&gt;
&lt;li&gt;devデータに目標の精度までフィットさせる。&lt;/li&gt;
&lt;li&gt;テストデータに目標の精度までフィットさせる。&lt;/li&gt;
&lt;li&gt;現実のデータでうまく動くようにする。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;それぞれの目的について、チューニングすべき要素は別々になる。&lt;/p&gt;

&lt;p&gt;早期終了は直行化の原則に反しているので、ほかの方法があるならそっちをやったほうがいい。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;指標(Goal)の設定&lt;/p&gt;

&lt;p&gt;モデルの改善はイテレーティブなプロセスなので、サイクルを速く回したい。
そのため、モデルを評価する単一の数値があるといい。
F1スコアとか。平均とか&lt;/p&gt;

&lt;p&gt;単一の指標にまとめるのがむずいときもある。
精度と速さとか。
そんなときは一つ以外の指標を足切りだけに使う。
ある閾値以上の速さが出てるもののなかで精度をくらべるなど。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;データの分け方&lt;/p&gt;

&lt;p&gt;devデータとテストデータの分布(と評価指標)は同じ感じにしないといけない。
そのために、いったん全データをシャッフルしてから分割する。
訓練データの分布は異なってても問題ない。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;訓練:テスト = 70:30&lt;/code&gt;とか、&lt;code&gt;訓練:dev:テスト = 60:20:20&lt;/code&gt;とかいう比率は、1万くらいのデータなら適当。
けど100万くらいなら、98:1:1くらいが妥当。&lt;/p&gt;

&lt;p&gt;テストデータはモデルの最終評価をするためのものなので、どれだけ評価したいかによってサイズを変える。
0もありがちだけど、非推奨。&lt;/p&gt;

&lt;p&gt;猫の画像のなかにエロ画像が混じっちゃうようなモデルはだめ。
猫判定率が多少下がっても、エロ画像が含まれないほうがまし。
こういう場合は評価指標を変える必要がある。
エロ画像を猫と判定した場合にペナルティを大きくするなど。&lt;/p&gt;

&lt;p&gt;直行化の観点で言うと、指標を決めるのと、その指標に従って最適化するのは、別のタスクとして扱うべき。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;人並性能(Human-level performance)との比較&lt;/p&gt;

&lt;p&gt;人並性能とは、人が手動で達成できる精度。そのエラー率、つまり人並誤差(Human-level error)はベイズ誤差(Bayes optimal error)に近い。&lt;/p&gt;

&lt;p&gt;モデルを改良していくと、人並性能を超え、その後改善速度は鈍化し、人並誤差はベイズ誤差に漸近していく。
鈍化する理由は、人並性能がベイズ誤差に近いのと、人以上の精度に人がチューニングするのが無理があるから。
人手でラベル付きデータを作れないし、エラー分析もできなくなるので。&lt;/p&gt;

&lt;p&gt;人並誤差より訓練データでのエラー率が結構高いなら、高バイアス対策をする。
人並誤差と訓練データでのエラー率が近くて、devデータでのエラー率が結構高いなら、高バリアンス対策をする。
人並誤差と訓練データでのエラー率との差ををAndrew先生は可避バイアス(Avoidable bias)と名付けた。&lt;/p&gt;

&lt;p&gt;人並誤差はベイズ誤差の近似として使える。
人並誤差は、ある判別問題に関して、その道のエキスパート達が議論して解を出すみたいな、人類が全力を尽くしたうえでの誤差とする。
人並誤差が分かれば、訓練データとdevデータのエラー率を見て、高バイアスか高バリアンス化を判別できる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;モデルの性能改善手順&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;訓練データにフィットさせ、可避バイアスを最小化する。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;モデルを大きくする。&lt;/li&gt;
&lt;li&gt;最適化アルゴリズムを高度なものにするか、長く訓練する。&lt;/li&gt;
&lt;li&gt;NNのレイヤを深くしたり隠れ層のノードを増やしたり、CNNとかRNNとかの高度なアーキテクチャにする。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;dev・テストデータで評価し、バリアンスを下げる。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;データを増やす。&lt;/li&gt;
&lt;li&gt;正則化する。&lt;/li&gt;
&lt;li&gt;ハイパーパラメータをいじる。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Andrej Karpathyへのインタビュー&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2週目&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;エラー分析&lt;/p&gt;

&lt;p&gt;エラー率が高いときに、エラーが起きたdevデータのサンプルを見て原因を分析する。
天井分析(Ceiling analysis)も併せてやる。
例えば、猫判定器で、犬を猫と判定したサンプルがあったとして、犬の問題に取り組むかどうかは、全体のエラーサンプル数に対する犬のエラーサンプル数の割合を見て、その取り組みで最大どれだけの効果を得るかを分析する。&lt;/p&gt;

&lt;p&gt;天井分析を複数の問題点に対してやれば、どれに時間をかける価値があるかの指標を得られる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ラベリングミスへの対処&lt;/p&gt;

&lt;p&gt;ディープラーニングは、ランダムエラーに対して堅牢で、訓練データに多少のラベリングミスがあっても問題なく動く。&lt;/p&gt;

&lt;p&gt;devデータにミスがあった場合、エラー分析の際にそれも数えておいて、対処すべきかどうかを判断する。
他の問題によるエラーの割合と比べて、ラベリングミスによるものの割合が大きければ対処すればいいし、そうでなければほっておく。
また、エラー全体に対するラベリングミスの割合が大きくなると、モデルの性能比較に支障が出てくるので、そうなったらラベリングミスに対処する必要が高まってくる。&lt;/p&gt;

&lt;p&gt;devデータのラベリングミスを直すときは、テストデータも同時に直し、分布に違いが出ないようにする。
また、エラーなサンプルだけじゃなく、正答したサンプルも見直すべし。
けど、訓練データは直さなくてもいい。数も多いし、devデータと分布が違っていても問題ないし。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;新しいディープラーニングシステムを作るときのガイドライン&lt;/p&gt;

&lt;p&gt;あまり経験のない分野のシステムを新たに作るなら、早く立ち上げてイテレーションを回すべし。
具体的には、&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;dev・テストデータと、指標を用意する。&lt;/li&gt;
&lt;li&gt;さっさとシステムを実装する。&lt;/li&gt;
&lt;li&gt;バイアス/バリアンス分析やエラー分析をして、次のアクションを決める。&lt;/li&gt;
&lt;li&gt;システムを改善する。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;というのを速く回す。
経験が深かったり、確かな研究結果がすでにあったりするなら、最初から凝ったシステムにしてもいい。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;訓練データとdev・テストデータの分布のミスマッチ&lt;/p&gt;

&lt;p&gt;ディープラーニングでは訓練データは大抵不足するから、現実的に、訓練データはいろんなデータのかき集めで、dev・テストデータとは分布が異なってくる。&lt;/p&gt;

&lt;p&gt;例えば、実際のデータが10000個で、かき集めのが200000個あったら、全部合わせてシャッフルしてデータ分割するのは、dev・テストデータの質が悪くなるのでダメ。
dev・テストデータには実際のデータだけ使って、かき集めのは全部訓練データに使うべし。&lt;/p&gt;

&lt;p&gt;分布がミスマッチになると、バイアス/バリアンス分析がむずくなる。
devデータでエラーが増えても、単にdevデータのほうが判別が難しいデータなのかもしれない。&lt;/p&gt;

&lt;p&gt;分析をしやすくするため、訓練devデータを作る。
これは、訓練データと同じ分布のデータで訓練に使わないデータ。訓練データのサブセット。&lt;/p&gt;

&lt;p&gt;訓練データと訓練devデータのエラー率の差が大きければオーバーフィット。
訓練データと訓練devデータのエラー率の差が小さくて、devデータのエラー率が高いなら、データミスマッチ問題(Data missmatch problem)。&lt;/p&gt;

&lt;p&gt;あんまり発生しないけど、devデータよりテストデータのエラー率が結構大きいなら、devデータにオーバーフィットしてるので、devデータサイズを増やしたりする必要がある。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;データミスマッチ問題への対応&lt;/p&gt;

&lt;p&gt;データミスマッチ問題が発覚したら、訓練データをdevデータに近づけたり、devデータに近いものを訓練データに加えたりすることを考える。
例えば、車内の音声認識のためのモデルを開発していて、devデータに車の雑音が入っていることが分かったら、訓練データにそういう雑音を合成してやるなど。
ただし、その場合、同じ雑音をすべての訓練データに適用すると、その雑音にモデルがオーバーフィットするリスクがあるので注意。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;転移学習(Transfer learning)&lt;/p&gt;

&lt;p&gt;ある問題に対して作ったモデルを、別の問題に再利用する。
(但し入力は同種のデータ。画像なら画像、音声なら音声。)
その際、NNの、出力層だけのパラメータをランダム初期化したり、層を足したりして、あたらしい訓練データで学習させる。
新たな訓練データが少ない場合は、後ろの1,2層だけを再学習させ、データがたくさんあったら全体を再学習させる。&lt;/p&gt;

&lt;p&gt;最初の学習を事前学習(Pre-training)、新たなデータでの学習をファインチューニング(Fine tuning)という。
画像認識の分野でよく使われる。NNの浅い層のほうが、汎用的な特徴(エッジ検出など)を学習するので再利用できると考えられているが、詳しい原理は判明していない。&lt;/p&gt;

&lt;p&gt;事前学習するデータより、ファインチューニングに使えるデータが少ないときに効果的。
データ量が同じくらいだったり、後者のほうが多い場合は、最初から目的のデータで学習させたほうがいい。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;マルチタスク学習(Multi-task learning)&lt;/p&gt;

&lt;p&gt;一度に複数の判別をさせるモデルをつくる。
ソフトマックス層つかった多クラス分類に似てるけど、一つのサンプルから複数のクラスを検出する。
例えば車と歩行者と標識など。
物体検知によく使われる。&lt;/p&gt;

&lt;p&gt;一つ一つのクラスのデータが少なくても低層が学んだ共通特徴を活用できるので、一つのタスクをするNNを複数訓練するより性能が良くなることがある。&lt;/p&gt;

&lt;p&gt;ラベルが歯抜けでも学習できる。&lt;/p&gt;

&lt;p&gt;複数タスクをこなすため、大き目なネットワークにする必要がある。&lt;/p&gt;

&lt;p&gt;それぞれのクラスのデータが十分あるときはあまり使われない手法。
目的のクラスのデータが少ないとき、転移学習のほうがよく使われる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;End-to-end学習&lt;/p&gt;

&lt;p&gt;従来、ある入力から解となる出力を得るのに、パイプラインで複数の処理をしていたが、これを全部一つのNNで処理する手法。
データ量が多いと上手くいく。&lt;/p&gt;

&lt;p&gt;例えば、人の認識をするときパイプラインでは、顔検出して、拡大してクロップしてからNNにかける。
こっちのほうが個々のタスクがシンプルで、それぞれのデータも手に入りやすいので性能を出しやすい。
けどもし、end-to-endのラベル付きデータが十分にあればend-to-end学習でもいける。&lt;/p&gt;

&lt;p&gt;翻訳タスクの場合、現実的に大量のデータが手に入るので、end-to-endで上手くいく。&lt;/p&gt;

&lt;p&gt;データさえたくさんあれば、パイプラインのコンポーネント的なところも勝手に学んでくれるので、ヒトが考え付くよりもいい特徴を学んでくれるかもしれない。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Ruslan Salakhutdinovへのインタビュー&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    
      
        <item>
          <title>CourseraのDeep Learning SpecializationのImproving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了した</title>
          <link>https://www.kaitoy.xyz/2018/01/12/coursera-deep-learning-improving-deep-neural-networks/</link>
          <pubDate>Fri, 12 Jan 2018 23:41:57 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2018/01/12/coursera-deep-learning-improving-deep-neural-networks/</guid>
          <description>&lt;p&gt;&lt;a href=&#34;https://www.kaitoy.xyz/2018/01/05/coursera-deep-learning-neural-networks-and-deep-learning/&#34;&gt;CourseraのDeep Learning SpecializationのNeural Networks and Deep Learningコースを修了した&lt;/a&gt;のに続き、&lt;a href=&#34;https://www.coursera.org/learn/deep-neural-network&#34;&gt;Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコース&lt;/a&gt;を修了した。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;このコースは、ディープニューラルネットワークのチューニングなどについて学べる3週間のコース。
今のところ全部英語。&lt;/p&gt;

&lt;p&gt;2018/1/5に始めて、1/12に完了。
8日間かかった。
修了したらまた&lt;a href=&#34;https://www.coursera.org/account/accomplishments/certificate/5VS9EJJ6TJ3A&#34;&gt;Certifacate&lt;/a&gt;もらえた。&lt;/p&gt;

&lt;p&gt;以下、3週分の内容をメモ程度に書いておく。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1週目&lt;/p&gt;

&lt;p&gt;OverfittingやUnderfittingを防ぐテクニックについて。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;データ分割&lt;/p&gt;

&lt;p&gt;深層学習のモデル構築は、検討(Idea)、実装(Code)、検証(Experiment)というサイクルの繰り返し(Iterative process)。
取り組む課題、課題のドメイン、データ量、マシンの構成などにより、ハイパーパラメータは変わるので、経験をもって事前に予測することは無理なので、サイクルをどれだけ速く回せるかが鍵。&lt;/p&gt;

&lt;p&gt;データは、訓練データ(Training set)、Devデータ(Development set))(a.k.a. Cross-validation set)、テストデータ(Test set)に分割する。
訓練データで学習し、Devデータでハイパーパラメータを評価し、テストデータで最終的な評価と性能見積をする。
テストデータは無くてもいい。&lt;/p&gt;

&lt;p&gt;サンプル数が1万くらいだった時代は、6:2:2くらいで分割してたけど、近年は数百万とかのデータを扱い、交差検証データやテストデータの割合はもっと小さくするのがトレンド。
98:1:1など。&lt;/p&gt;

&lt;p&gt;Devデータとテストデータは同じようなものを使うべき。
訓練データは必ずしも同様でなくてもいい。訓練データは沢山要るので、別のデータソースからとることもある。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;バイアス vs バリアンス&lt;/p&gt;

&lt;p&gt;でかいネットワークで正則化して大量データで学習させるのが吉。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;正則化&lt;/p&gt;

&lt;p&gt;過学習(Overfitting)を防ぐため、コスト関数を正則化(Regularization)すべし。&lt;/p&gt;

&lt;p&gt;ロジスティック回帰ではL2ノルム(L2 norm)を使ったL2正則化が一般的。
L1正則化はあまり使われない。
L1正則化をすると、wがスパース行列になってモデルを圧縮できると言われることがあるが、経験上その効果はほとんどない。&lt;/p&gt;

&lt;p&gt;正則化パラメータλはハイパーパラメータで、Devデータで評価する。&lt;/p&gt;

&lt;p&gt;ニューラルネットワークでは正則化項にフロベニウスノルム(Frobenius norm)を使う。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Dropout(Inverted Dropout)&lt;/p&gt;

&lt;p&gt;ランダムにノードを無効化しながら学習することで過学習を防ぐ。
画像処理の分野では、特徴量の数が多く学習データが少ない傾向があるので、ほぼ常に使われる。&lt;/p&gt;

&lt;p&gt;コスト関数が計算できなくなるのが欠点。
計算する必要があるときにはDropoutを無効化する。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;データ拡張(Data augmentation)&lt;/p&gt;

&lt;p&gt;データを加工して増やせば、高バリアンス対策になる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;早期終了(Early stopping)&lt;/p&gt;

&lt;p&gt;過学習するまえに学習を止めるテクニック。
訓練データとDevデータについてコストをプロットして、Devデータのものが上がる前に止める。&lt;/p&gt;

&lt;p&gt;これは、直交化(Orthogonalization)の原則、つまり一度に一つのことを考慮すべきという原則に反していて、コストを最小化するという問題と、過学習を避けるという問題に同時に対処することになるので微妙。&lt;/p&gt;

&lt;p&gt;普通は代わりにL2正則化使えばいいけど、λを最適化する手間を省きたいときには選択肢になりうる、というか実現場ではちょくちょく選択肢になる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;訓練データの正規化(Normalization)&lt;/p&gt;

&lt;p&gt;訓練データの各特徴量について平均を0にして分散を1にすると学習が速くなる。&lt;/p&gt;

&lt;p&gt;訓練データを正規化したらテストデータも正規化する。
その際、正規化パラメータは訓練データのものを使う。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;勾配消失(Vanishing gradient)、勾配爆発(Exploding gradient)&lt;/p&gt;

&lt;p&gt;ニューラルネットワークの層が深くなると、層の出力や勾配が指数関数的に大きくなったり小さくなったりして、学習が難しくなる問題。
長年ディープニューラルネットワークの発展を妨げてきた問題。&lt;/p&gt;

&lt;p&gt;パラメータのランダム初期化をすると防げる。
ガウス分布で作ったパラメータに特定の値を掛けてを分散が1/n(ReLUの時は2/n)になるように調整して、活性化関数に入力する値を約1に抑える。
掛ける値は活性化関数ごとにだいたい決まっていて((e.g. Xavier Initialization)、その値をハイパーパラメータとして調整するのはそれほど優先度は高くない。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gradient checking&lt;/p&gt;

&lt;p&gt;順伝播・逆伝播が正確に実装できているかを、数値計算手法で概算した勾配と逆伝播で出した勾配を比べてチェックするテクニック。
計算コストが高くなるので、デバッグ時にのみ使う。&lt;/p&gt;

&lt;p&gt;Dropoutしてるときには使えない。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;初期化&lt;/p&gt;

&lt;p&gt;ゼロ初期化、大きい値でのランダム初期化、He初期化(Xavier初期化っぽいやつ)を実装して性能を比べる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;正則化&lt;/p&gt;

&lt;p&gt;正則化無し、L2正則化、Dropoutの実装と比較。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gradient checking&lt;/p&gt;

&lt;p&gt;Gradient checkingの実装と、その結果を利用した逆伝播のデバッグ。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Yoshua Bengioへのインタビュー&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2週目&lt;/p&gt;

&lt;p&gt;学習を速くするテクニックについて。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ミニバッチ勾配降下法(Mini-batch gradient descent)&lt;/p&gt;

&lt;p&gt;普通の勾配降下法(i.e. バッチ勾配降下法)よりかなり速いので、大規模データでよく使われるテクニック。
学習回数に対するコストのプロットはノイジーになる。&lt;/p&gt;

&lt;p&gt;ミニバッチサイズというハイパーパラメータが増える。
ミニバッチサイズがmならバッチ勾配降下法、1なら確率的勾配降下法(Stochastic gradient descent)になる。&lt;/p&gt;

&lt;p&gt;ミニバッチ勾配降下法と確率的勾配降下法は収束しない。&lt;/p&gt;

&lt;p&gt;バッチ勾配降下法は遅すぎる。
確率的勾配降下法はベクトル化の恩恵がなくなるという欠点がある。
ので、適当なミニバッチサイズにするのがよく、それが一番速い。&lt;/p&gt;

&lt;p&gt;2000個くらいのデータならバッチ勾配降下法。
それより多ければ、64～512位のミニバッチサイズがいい。
メモリ効率を考えると2の累乗数がいいし、CPU/GPUメモリサイズに乗るサイズにすべし。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;指数加重移動平均 (EWMA: Exponentially Weighted Moving Average)&lt;/p&gt;

&lt;p&gt;ノイズのあるデータから、よりスムーズなプロットを書く手法。
過去数日の平均をもとにプロットする。&lt;/p&gt;

&lt;p&gt;この手法だと、最初の方のデータが不当に小さくなってしまう。
これが問題になるなら、バイアス補正(Bias correction)をかける。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;モーメンタム(Momentum)付き勾配降下法&lt;/p&gt;

&lt;p&gt;パラメータを更新するときに、勾配そのままではなく、勾配の指数加重移動平均を使う手法。
勾配降下を滑らかに速くできる。
慣性(勢い)をつけて走り抜ける感じ。&lt;/p&gt;

&lt;p&gt;指数加重移動平均を計算するときのβが新たなハイパーパラメータになる。
普通0.9。この場合バイアス補正はそんなに効果ないので普通かけない。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;RMSprop&lt;/p&gt;

&lt;p&gt;パラメータを更新するときに、勾配そのままではなく、勾配の二乗平均平方根(RMS: Root Mean Square)を使う手法。
学習率を上げつつ、勾配降下を滑らかに速くできる。&lt;/p&gt;

&lt;p&gt;二乗平均平方根を計算するときのβと、ゼロ除算を防ぐためのεが新たなハイパーパラメータになる。
提唱者は、&lt;code&gt;β=0.999&lt;/code&gt;、&lt;code&gt;ε=10^-8&lt;/code&gt;を推奨しているし、これらをチューニングすることはあまりない。&lt;/p&gt;

&lt;p&gt;Couseraが広めたことで、よく使われるようになった。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Adam(Adaptive Moment Estimation)&lt;/p&gt;

&lt;p&gt;モーメンタムとRMSpropとバイアス補正を組み合わせた最適化アルゴリズム。
これをミニバッチ勾配降下法と一緒に使えばだいたい上手くいく。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;学習率減衰(Learning rate decay)&lt;/p&gt;

&lt;p&gt;ミニバッチ勾配降下を収束させるために、学習率を徐々に小さくする手法。
エポックごとに学習率を下げる。&lt;/p&gt;

&lt;p&gt;学習率αが、α0と 減衰率(Decay rate)とエポック番号から計算されるようになるので、α0と減衰率がハイパーパラメータ。&lt;/p&gt;

&lt;p&gt;学習率の計算方法にはいくつかある。
指数関数的に下げたり、階段状に下げたり。&lt;/p&gt;

&lt;p&gt;Andrew先生はあまり使わない手法。
学習率をよくチューニングすれば十分。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;局所最適解(Local optima)&lt;/p&gt;

&lt;p&gt;かつて、勾配が0になる点は、コストの谷、つまり局所最適解だと考えられていて、そこに嵌ることが問題だった。&lt;/p&gt;

&lt;p&gt;けど、ディープニューラルネットワークでは多くは尾根的なもの。
鞍の上みたいな部分なので鞍点(Saddle point)と呼ばれる。
特徴量が沢山あるので、ちょっと動かすとどれかの勾配は負になる。&lt;/p&gt;

&lt;p&gt;よって局所最適解はあまり恐れなくていい。
代わりに、鞍点の台地みたいな部分では勾配が小さいので学習効率が悪くなる。
ここを勢いよく抜けたいので、モーメンタムやRMSpropやAdamが有効になる。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ミニバッチ勾配降下法実装&lt;/p&gt;

&lt;p&gt;訓練データをシャッフルして、ミニバッチサイズに分割して(余りは余りでミニバッチにする)、forループで回す。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;モーメンタムとAdam実装&lt;/p&gt;

&lt;p&gt;単なるミニバッチ勾配降下法とモーメンタム付きとAdamを比較。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Yuanqing Linへのインタビュー&lt;/p&gt;

&lt;p&gt;中国の国立深層学習研究所のトップ。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2週目&lt;/p&gt;

&lt;p&gt;ハイパーパラメータのチューニング方法、バッチ正規化、ソフトマックス回帰、TensorFlow。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ハイパーパラメータのチューニング&lt;/p&gt;

&lt;p&gt;一番重要なのは学習率。
次はモーメンタムのβとかミニバッチサイズとか隠れ層のノード数。
その次がレイヤ数とか学習率減衰率。&lt;/p&gt;

&lt;p&gt;チューニングの際は、かつてはグリッドサーチ(Grid search)がよく使われたけど、これはハイパーパラメータが多くなるとつらい。
ランダムサーチ(Randomized search)がより効率的。&lt;/p&gt;

&lt;p&gt;グリッドサーチだとあるパラメータを固定して別のパラメータを変化させるけど、変化させたパラメータがどうでもいいものだった場合、その試行がほとんど無駄になるので。&lt;/p&gt;

&lt;p&gt;粗くランダムサーチして当たりをつけ、範囲を絞って細かいランダムサーチする。&lt;/p&gt;

&lt;p&gt;ランダムといってもいろいろあって、ユニット数なんかは一様にランダムでいいけど、学習率なんかはlogスケールの上でランダムにしたほうがいい。&lt;/p&gt;

&lt;p&gt;実運用では、計算リソースが少ない場合に採るパンダアプローチと、潤沢なリソースで複数のモデルを同時に訓練するキャビアアプローチがある。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;バッチ正規化(Batch normalization)&lt;/p&gt;

&lt;p&gt;深層学習の実用化において最も重要なアルゴリズムの一つ。
ハイパーパラメータの選定を簡単にして、ディープニューラルネットワークの訓練を簡単にする。&lt;/p&gt;

&lt;p&gt;バッチ正規化では、各層の入力を正規化する。
ミニバッチごとに平均を0、分散を1に正規化した後、βとγというパラメータでそれぞれを調整する。
aよりz(i.e. 活性化関数適用前)を正規化するのが普通。&lt;/p&gt;

&lt;p&gt;(ハイパーではない)パラメータとしてβとγが層ごとに増える。
これらもWとともに学習する。
βがbの役割をするので、bはいらなくなる。&lt;/p&gt;

&lt;p&gt;バッチ正規化は共変量シフト(Covariate shift)という問題に対応するもの。
共変量シフトは、訓練した後で入力の分散が変わると、また訓練しなおさないといけないという問題。
ニューラルネットワークの内部では、前のほうの層のWが学習を進めるたびに変わり、その層の出力が変わる。
つまり後のほうの層への入力が変わるので、後のほうの層の学習が進みにくい。
バッチ正規化は、この後のほうの層への入力の分散を一定範囲に抑えることで、後のほうの層の学習を効率化する。&lt;/p&gt;

&lt;p&gt;Dropoutと同様な論理(ノードへの依存が分散される)で正則化の効果もややある。&lt;/p&gt;

&lt;p&gt;訓練が終わったら、最後のミニバッチの平均μと分散σ^2を保存しておいて、予測時に使う。
μとσ^2は訓練データ全体から再計算してもよさそうだけど、普通はやらない。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ソフトマックス回帰(Softmax regression)&lt;/p&gt;

&lt;p&gt;ニューラルネットワークで多値分類(Multi-class classification)するアルゴリズム。
出力層(ソフトマックス層)のノード数をクラス数にして、活性化関数にソフトマックス関数を使う。
出力層の各ノードは、サンプルが各クラスに属する確率を出力する。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;TensorFlow&lt;/p&gt;

&lt;p&gt;ディープラーニングフレームワークはいろいろある: Caffe/Caffe2、CNTK、DL2J、Keras、Lasagne、mxnet、PaddlePaddle、TensorFlow、Theano、Torch。
プログラミングしやすいこと、訓練性能がいいこと、オープンであることが重要。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;TensorFlowの基本&lt;/p&gt;

&lt;p&gt;TensorFlowでのプログラムはだいたい以下のような手順で書く。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;テンソル(tensor)をつくる。これはまだ評価されない。&lt;/li&gt;
&lt;li&gt;テンソル間の計算式(計算グラフ)を書く。&lt;/li&gt;
&lt;li&gt;テンソルを初期化する。&lt;/li&gt;
&lt;li&gt;セッションを作る。&lt;/li&gt;
&lt;li&gt;セッションを実行する。ここで計算が実行される。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;後で(セッション実行時に)値を入れたい場合はプレースホルダ(placeholder)を使う。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;TensorFlowでのニューラルネットワーク実装&lt;/p&gt;

&lt;p&gt;画像を読み込んで多クラス分類するNNを作る。
以下、今回使った関数の一部。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;シグモイド関数: &lt;code&gt;tf.sigmoid(x)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;エントロピーコスト: &lt;code&gt;tf.nn.sigmoid_cross_entropy_with_logits(logits, labels)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;One-hotエンコーディング: &lt;code&gt;tf.one_hot(labels, depth, axis)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;最急降下法: &lt;code&gt;tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    
      
        <item>
          <title>CourseraのDeep Learning SpecializationのNeural Networks and Deep Learningコースを修了した</title>
          <link>https://www.kaitoy.xyz/2018/01/05/coursera-deep-learning-neural-networks-and-deep-learning/</link>
          <pubDate>Fri, 05 Jan 2018 15:20:23 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2018/01/05/coursera-deep-learning-neural-networks-and-deep-learning/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;https://www.kaitoy.xyz/2017/12/22/coursera-machine-learning/&#34;&gt;CourseraのMachine Learningコース&lt;/a&gt;に続いて、同じくAndrew先生による&lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;&gt;Deep Learning Specialization&lt;/a&gt;を受講中。&lt;/p&gt;

&lt;p&gt;これは深層学習の基本を学べるもので、以下の5つのコースからなる。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Neural Networks and Deep Learning&lt;/li&gt;
&lt;li&gt;Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization&lt;/li&gt;
&lt;li&gt;Structuring Machine Learning Projects&lt;/li&gt;
&lt;li&gt;Convolutional Neural Networks&lt;/li&gt;
&lt;li&gt;Sequence Models&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;この内、最初のNeural Networks and Deep Learningを修了したので、記念にブログしておく。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h1 id=&#34;deep-learning-specializationとは&#34;&gt;Deep Learning Specializationとは&lt;/h1&gt;

&lt;p&gt;Deep Learning Specializationは&lt;a href=&#34;https://learner.coursera.help/hc/en-us/articles/208280296&#34;&gt;Coursera Specialization&lt;/a&gt;のひとつ。
Coursera Specializationはサブスクリプションモデルで、つまりあるSpecializationのサブスクリプションを購入すると、受講完了するまで毎月定額の料金を支払うことになる。&lt;/p&gt;

&lt;p&gt;Deep Learning Specializationは月$49で、5コース合わせて13週+α分の内容。
(Sequence Modelsが2018年1月に公開予定となっていて、現時点でまだ公開されていないので内容が不明。)
最初の7日間はトライアルで無料なので、この間に全部終わらせられればタダ。
無理だけど。&lt;/p&gt;

&lt;p&gt;Deep Learning Specializationでは、PythonとTensorFlowでディープニューラルネットワーク、CNN、RNN、LSTM、Adam、Dropout、バッチ正規化、Xavier/He initializationなどを学べる。
Machine Learningコースと同じく、5分～15分くらいの動画による講義と、小テストと、プログラミング課題から構成されている。&lt;/p&gt;

&lt;p&gt;プログラミング課題は、coursera hubという、ホステッドJupyter Notebookで解いて提出できるので楽。&lt;/p&gt;

&lt;h1 id=&#34;neural-networks-and-deep-learningコースとは&#34;&gt;Neural Networks and Deep Learningコースとは&lt;/h1&gt;

&lt;p&gt;ディープニューラルネットワークの仕組みを学んで実装する4週間のコース。
また、深層学習の偉い人へのインタビューを見れる。
Machine Learningコースと被っている内容が少なくなく、かなり楽だったが、結構ペースが速いので、Machine Learningコースをやっていなかったら辛かったと思う。&lt;/p&gt;

&lt;p&gt;動画は大抵日本語字幕が付いている。
日本語字幕が付いていない奴は、英語字幕が機械生成したっぽいもので見辛い。&lt;/p&gt;

&lt;p&gt;2018/1/1に始めて、1/5に完了。
5日間かかった。
修了したら&lt;a href=&#34;https://www.coursera.org/account/accomplishments/certificate/G77XMU9TNEKX&#34;&gt;Certifacate&lt;/a&gt;もらえた。&lt;/p&gt;

&lt;p&gt;以下、4週分の内容をキーワードレベルで書いておく。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1週目&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;深層学習(Deep Learning)概要&lt;/p&gt;

&lt;p&gt;AIは次世代の電気。産業革命を起こす。
AIで今一番熱い分野が深層学習。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ニューラルネットワーク(Neural Network)。&lt;/li&gt;
&lt;li&gt;畳み込みニューラルネットワーク(CNN: Convolutional Neural Network)。&lt;/li&gt;
&lt;li&gt;再帰型ニューラルネットワーク(RNN: Recurrent Neural Network)。&lt;/li&gt;
&lt;li&gt;深層学習の適用分野・例。&lt;/li&gt;
&lt;li&gt;深層学習が実用化した背景。&lt;/li&gt;
&lt;li&gt;シグモイド関数(Sigmoid function) vs ReLU(Rectified Linear Unit)関数。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;深層学習のゴッドファーザー、Geoffrey Hintonへのインタビュー。&lt;/p&gt;

&lt;p&gt;ニューラルネットワークの黎明期を支え、ReLU関数の有効性を証明したりボルツマンマシンを発明したりした人。
自身が歩んできた深層学習の歴史や今取り組んでいる・注目している理論について、
高尚な話をしていたようだったが、高尚すぎてよくわからなかった。&lt;/p&gt;

&lt;p&gt;今日成果を出しているのは教師あり学習だけど、教師無し学習のほうが重要と考えているとのこと。&lt;/p&gt;

&lt;p&gt;これから機械学習に取り組む人へ、以下のアドバイスをしていた。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;文献を読みすぎるな。&lt;/li&gt;
&lt;li&gt;文献を読んで、間違っていると感じるところをみつけて、それに取り組め。&lt;/li&gt;
&lt;li&gt;人から何を言われても気にせず、自分の信念に従って研究しろ。&lt;/li&gt;
&lt;li&gt;誰かに無意味なことをしていると指摘されたら、むしろ価値のあることをしていると思え。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2週目&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ニューラルネットワークプログラミングの基礎&lt;/p&gt;

&lt;p&gt;ロジスティック回帰は小さい(1層1ノード)ニューラルネットワーク。
ロジスティック回帰の微分を逆伝播で計算する。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;二値分類(Binary classification)、ロジスティック回帰(Logistic regression)。&lt;/li&gt;
&lt;li&gt;損失関数(Loss function)、交差エントロピー(Cross entropy)、目的関数(Cost function)。&lt;/li&gt;
&lt;li&gt;最急降下法(Gradient descent)。&lt;/li&gt;
&lt;li&gt;微分(Derivatives)。&lt;/li&gt;
&lt;li&gt;逆伝播(Backpropagation)、計算グラフ(Computation graph)、連鎖律(Chain rule)。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Pythonとベクトル化(Vectorization)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;forループ vs ベクトル化。&lt;/li&gt;
&lt;li&gt;Jupyter Notebook、NumPy、ブロードキャスト(Broadcasting)。&lt;/li&gt;
&lt;li&gt;ロジスティック回帰のベクトル化。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ロジスティック回帰で猫の画像の判別。&lt;/li&gt;
&lt;li&gt;NumPy、h5py、Matplotlib、PIL、SciPy。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;深層学習とロボットの研究者、Pieter Abbeelへのインタビュー&lt;/p&gt;

&lt;p&gt;深層強化学習の有名な研究者。DQN。&lt;/p&gt;

&lt;p&gt;かつて、機械学習で成果を出すためには、取り組んでいる課題特有の分野の知識が必要だったが、2012年にGeoffreyが発表したAlexNetがそれを覆した。
Pieterはそのアイデアを深層強化学習に適用し発展させた。&lt;/p&gt;

&lt;p&gt;強化学習は、どこからデータ収集するのか、報酬の分配はどうするかといったところに課題がある。
また、安全性にも課題。学習の過程で失敗を繰り返すので、自動運転などをどう学習させるか、またどう学び続けさせるか。
ネガティブデータを集めるのがむずい。&lt;/p&gt;

&lt;p&gt;短時間の実験でうまくいっても、長時間の稼働でうまくいくとも限らない。&lt;/p&gt;

&lt;p&gt;強化学習は複雑すぎるので、アルゴリズム自体を学習できるようにしたい。
プログラムを自動で変更しながら学習するなど。&lt;/p&gt;

&lt;p&gt;強化学習は、アルゴリズムを変更しなくても様々なことを学べる。
けど、ゼロから学ぶと時間がかかるので、以前学んだことを活かして次の課題に取り組めるようにするのが最先端の研究。&lt;/p&gt;

&lt;p&gt;まずは教師あり学習で人の代わりができるようになり、その後目的を与えて、強化学習で改善していく、っていう感じのことができるとうれしい。&lt;/p&gt;

&lt;p&gt;これから機械学習に取り組む人へ、以下のアドバイスをしていた。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;需要が高まっているので、AIを始めるにはよい時期。&lt;/li&gt;
&lt;li&gt;オンライン講座がたくさんあるので、学びやすい。自分で試したり再現したりしてみることが重要。&lt;/li&gt;
&lt;li&gt;TensorFlow、Chainer、Theano、PyTorchなど、手軽に試せるツールが色々ある。&lt;/li&gt;
&lt;li&gt;専門的な教育を受けなくても、自己学習でトップクラスになれる。&lt;/li&gt;
&lt;li&gt;機械学習を学ぶのに、大学で研究すべきか大企業で仕事を得るべきかについては、どれくらいの指導を受けれるかによる。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;3週目&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;浅いニューラルネットワーク&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ロジスティック回帰を多重にして2層のニューラルネットワーク化。&lt;/li&gt;
&lt;li&gt;ニューラルネットワークアルゴリズムのベクトル化。&lt;/li&gt;
&lt;li&gt;活性化関数(Activation function)の選択: シグモイド vs tanh vs ReLU vs Leaky ReLU vs 線形活性化関数(Linear activation function)。&lt;/li&gt;
&lt;li&gt;順伝播(Forward propagation)、逆伝播(Backpropagation)。&lt;/li&gt;
&lt;li&gt;ランダム初期化(Random initialization)。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;二値分類するニューラルネットワークを実装。&lt;/li&gt;
&lt;li&gt;scikit-learn。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;GAN(Generative Adversarial Network)の発明者、Ian Goodfellowへのインタビュー&lt;/p&gt;

&lt;p&gt;GANは生成モデル(学習したデータに似たデータを生成するモデル)。
バーで飲んでいるときに思いつき、一晩で実装した。&lt;/p&gt;

&lt;p&gt;GANは今は繊細過ぎるのが課題で、安定性の向上に今取り組んでいる。&lt;/p&gt;

&lt;p&gt;機械学習のセキュリティにも興味がある。モデルをだまして想定外の動作をさせるような攻撃への対策など。&lt;/p&gt;

&lt;p&gt;これから機械学習に取り組む人へ、以下のアドバイスをしていた。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;機械学習のアルゴリズムよりも、線形代数や確率といった数学の基礎を習得することが重要。&lt;/li&gt;
&lt;li&gt;AIの道を進むのに、近年では博士号は必ずしも要らない。コードを書いてGitHubに上げろ。自身も実際に、オープンソース活動をしているひとの貢献を見て興味をもって採用したことがある。&lt;/li&gt;
&lt;li&gt;論文を公開するとよりいいけど、コードのほうが楽。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;4週目&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;深いニューラルネットワーク&lt;/p&gt;

&lt;p&gt;3層以上のニューラルネットワーク。その実装方法と有効性について。
ハイパーパラメータ(Hyperparameters): 学習率(Learning rate)、学習回数、レイヤ数、ノード数、活性化関数、等。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ディープニューラルネットワークの実装。&lt;/li&gt;
&lt;li&gt;再度猫画像の判別。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    

  </channel>
</rss>

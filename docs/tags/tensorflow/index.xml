<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tensorflow on To Be Decided</title>
    <link>https://www.kaitoy.xyz/tags/tensorflow/</link>
    <description>Recent content in tensorflow on To Be Decided</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© 2015 Kaito Yamada</copyright>
    <lastBuildDate>Sun, 25 Mar 2018 22:43:27 +0900</lastBuildDate>
    
	<atom:link href="https://www.kaitoy.xyz/tags/tensorflow/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>機械学習のHello World: MNISTの分類モデルをKerasで作ってみた</title>
      <link>https://www.kaitoy.xyz/2018/03/25/hello-world-to-ml-with-keras/</link>
      <pubDate>Sun, 25 Mar 2018 22:43:27 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/03/25/hello-world-to-ml-with-keras/</guid>
      <description>機械学習のHello WorldとしてよくやられるMNISTの分類モデルをKeras on TensorFlowで作ってみた話。
   (adsbygoogle = window.adsbygoogle || []).push({});  MNISTとは 手書き数字画像のラベル付きデータセット。 6万個の訓練データと1万個のテストデータからなる。 CC BY-SA 3.0で配布されているっぽい。
一つの画像は28×28ピクセルの白黒画像で、0から9のアラビア数字が書かれている。
画像とラベルがそれぞれ独特な形式でアーカイブされていて、画像一つ、ラベル一つ取り出すのも一苦労する。
Kerasとは Pythonのニューラルネットワークライブラリ。 バックエンドとしてTensorFlowかCNTKかTheanoを使う。 今回はTensorFlowを使った。
やったこと KerasのMNISTのAPIとかコードサンプルとかがあけどこれらはスルー。
MNISTのサイトにあるデータセットをダウンロードしてきて、サイトに書いてあるデータ形式の説明を見ながらサンプルを取り出すコードを書いた。 で、KerasでVGGっぽいCNNを書いて、学習させてモデルをダンプして、ダンプしたモデルをロードしてテストデータで評価するコードを書いた。 コードはGitHubに。
ネットワークアーキテクチャ 入力画像のサイズに合わせてVGGを小さくした感じのCNNを作った。
VGGは2014年に発表されたアーキテクチャで、各層に同じフィルタを使い、フィルタ数を線形増加させるシンプルな構造でありながら、性能がよく、今でもよく使われるっぽい。
VGGを図にすると以下の構造。
実際はバッチ正規化とかDropoutもやるのかも。 プーリング層は数えないで16層なので、VGG-16とも呼ばれる。 パラメータ数は1億3800万個くらいで、結構深めなアーキテクチャ。

VGG-16は244×244×3の画像を入力して1000クラスに分類するのに対し、MNISTは28×28×1を入れて10クラスに分類するので、以下のような7層版を作った。
これでパラメータ数は27万個くらい。 訓練データのサンプル数が6万個なので、パラメータ数が大分多い感じではある。
コードは以下。
inputs: Tensor = Input(shape=(IMAGE_NUM_ROWS, IMAGE_NUM_COLS, 1)) x: Tensor = Conv2D(filters=8, kernel_size=(2, 2), padding=&#39;same&#39;, activation=&#39;relu&#39;)(inputs) x = Conv2D(filters=8, kernel_size=(2, 2), padding=&#39;same&#39;, activation=&#39;relu&#39;)(x) x = MaxPooling2D(pool_size=(2, 2))(x) x = Conv2D(filters=16, kernel_size=(2, 2), padding=&#39;same&#39;, activation=&#39;relu&#39;)(x) x = Conv2D(filters=16, kernel_size=(2, 2), padding=&#39;same&#39;, activation=&#39;relu&#39;)(x) x = Conv2D(filters=16, kernel_size=(2, 2), padding=&#39;same&#39;, activation=&#39;relu&#39;)(x) x = MaxPooling2D(pool_size=(2, 2))(x) x = Flatten()(x) x = Dense(units=256, activation=&#39;relu&#39;)(x) x = Dense(units=256, activation=&#39;relu&#39;)(x) predictions: Tensor = Dense(NUM_CLASSES, activation=&#39;softmax&#39;)(x) model: Model = Model(inputs=inputs, outputs=predictions) model.</description>
    </item>
    
    <item>
      <title>CourseraのDeep Learning SpecializationのImproving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了した</title>
      <link>https://www.kaitoy.xyz/2018/01/12/coursera-deep-learning-improving-deep-neural-networks/</link>
      <pubDate>Fri, 12 Jan 2018 23:41:57 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/01/12/coursera-deep-learning-improving-deep-neural-networks/</guid>
      <description>CourseraのDeep Learning SpecializationのNeural Networks and Deep Learningコースを修了したのに続き、Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了した。
   (adsbygoogle = window.adsbygoogle || []).push({});  このコースは、ディープニューラルネットワークのチューニングなどについて学べる3週間のコース。 今のところ全部英語。
2018/1/5に始めて、1/12に完了。 8日間かかった。 修了したらまたCertifacateもらえた。
以下、3週分の内容をメモ程度に書いておく。
 1週目
OverfittingやUnderfittingを防ぐテクニックについて。
 動画
 データ分割
深層学習のモデル構築は、検討(Idea)、実装(Code)、検証(Experiment)というサイクルの繰り返し(Iterative process)。 取り組む課題、課題のドメイン、データ量、マシンの構成などにより、ハイパーパラメータは変わるので、経験をもって事前に予測することは無理なので、サイクルをどれだけ速く回せるかが鍵。
データは、訓練データ(Training set)、Devデータ(Development set))(a.k.a. Cross-validation set)、テストデータ(Test set)に分割する。 訓練データで学習し、Devデータでハイパーパラメータを評価し、テストデータで最終的な評価と性能見積をする。 テストデータは無くてもいい。
サンプル数が1万くらいだった時代は、6:2:2くらいで分割してたけど、近年は数百万とかのデータを扱い、交差検証データやテストデータの割合はもっと小さくするのがトレンド。 98:1:1など。
Devデータとテストデータは同じようなものを使うべき。 訓練データは必ずしも同様でなくてもいい。訓練データは沢山要るので、別のデータソースからとることもある。
 バイアス vs バリアンス
でかいネットワークで正則化して大量データで学習させるのが吉。
 正則化
過学習(Overfitting)を防ぐため、コスト関数を正則化(Regularization)すべし。
ロジスティック回帰ではL2ノルム(L2 norm)を使ったL2正則化が一般的。 L1正則化はあまり使われない。 L1正則化をすると、wがスパース行列になってモデルを圧縮できると言われることがあるが、経験上その効果はほとんどない。
正則化パラメータλはハイパーパラメータで、Devデータで評価する。
ニューラルネットワークでは正則化項にフロベニウスノルム(Frobenius norm)を使う。
 Dropout(Inverted Dropout)</description>
    </item>
    
  </channel>
</rss>
<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>To Be Decided </title>
    <link>https://www.kaitoy.xyz/tags/tensorflow/</link>
    <language>en-us</language>
    <author>Kaito Yamada</author>
    <rights>(C) 2018</rights>
    <updated>2018-03-25 22:43:27 &#43;0900 JST</updated>

    
      
        <item>
          <title>機械学習のHello World: MNISTの分類モデルをKerasで作ってみた</title>
          <link>https://www.kaitoy.xyz/2018/03/25/hello-world-to-ml-with-keras/</link>
          <pubDate>Sun, 25 Mar 2018 22:43:27 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2018/03/25/hello-world-to-ml-with-keras/</guid>
          <description>

&lt;p&gt;機械学習のHello Worldとしてよくやられる&lt;a href=&#34;http://yann.lecun.com/exdb/mnist/&#34;&gt;MNIST&lt;/a&gt;の分類モデルを&lt;a href=&#34;https://keras.io/ja/&#34;&gt;Keras&lt;/a&gt; on &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;TensorFlow&lt;/a&gt;で作ってみた話。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h2 id=&#34;mnistとは&#34;&gt;MNISTとは&lt;/h2&gt;

&lt;p&gt;手書き数字画像のラベル付きデータセット。
6万個の訓練データと1万個のテストデータからなる。
&lt;a href=&#34;https://creativecommons.org/licenses/by-sa/3.0/&#34;&gt;CC BY-SA 3.0&lt;/a&gt;で&lt;a href=&#34;http://www.pymvpa.org/datadb/mnist.html&#34;&gt;配布されているっぽい&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;一つの画像は28×28ピクセルの白黒画像で、0から9のアラビア数字が書かれている。&lt;/p&gt;

&lt;p&gt;画像とラベルがそれぞれ独特な形式でアーカイブされていて、画像一つ、ラベル一つ取り出すのも一苦労する。&lt;/p&gt;

&lt;h2 id=&#34;kerasとは&#34;&gt;Kerasとは&lt;/h2&gt;

&lt;p&gt;Pythonのニューラルネットワークライブラリ。
バックエンドとしてTensorFlowかCNTKかTheanoを使う。
今回はTensorFlowを使った。&lt;/p&gt;

&lt;h2 id=&#34;やったこと&#34;&gt;やったこと&lt;/h2&gt;

&lt;p&gt;KerasのMNISTの&lt;a href=&#34;https://keras.io/ja/datasets/#mnist&#34;&gt;API&lt;/a&gt;とか&lt;a href=&#34;https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py&#34;&gt;コードサンプル&lt;/a&gt;とかがあけどこれらはスルー。&lt;/p&gt;

&lt;p&gt;MNISTのサイトにあるデータセットをダウンロードしてきて、サイトに書いてあるデータ形式の説明を見ながらサンプルを取り出すコードを書いた。
で、KerasでVGGっぽいCNNを書いて、学習させてモデルをダンプして、ダンプしたモデルをロードしてテストデータで評価するコードを書いた。
コードは&lt;a href=&#34;https://github.com/kaitoy/ml-mnist&#34;&gt;GitHub&lt;/a&gt;に。&lt;/p&gt;

&lt;h2 id=&#34;ネットワークアーキテクチャ&#34;&gt;ネットワークアーキテクチャ&lt;/h2&gt;

&lt;p&gt;入力画像のサイズに合わせてVGGを小さくした感じのCNNを作った。&lt;/p&gt;

&lt;p&gt;VGGは2014年に発表されたアーキテクチャで、各層に同じフィルタを使い、フィルタ数を線形増加させるシンプルな構造でありながら、性能がよく、今でもよく使われるっぽい。&lt;/p&gt;

&lt;p&gt;VGGを図にすると以下の構造。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/hello-world-to-ml-with-keras/vgg16.png&#34; alt=&#34;vgg16.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;実際はバッチ正規化とかDropoutもやるのかも。
プーリング層は数えないで16層なので、VGG-16とも呼ばれる。
パラメータ数は1億3800万個くらいで、結構深めなアーキテクチャ。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;VGG-16は244×244×3の画像を入力して1000クラスに分類するのに対し、MNISTは28×28×1を入れて10クラスに分類するので、以下のような7層版を作った。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/hello-world-to-ml-with-keras/vgg7.png&#34; alt=&#34;vgg7.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;これでパラメータ数は27万個くらい。
訓練データのサンプル数が6万個なので、パラメータ数が大分多い感じではある。&lt;/p&gt;

&lt;p&gt;コードは以下。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;inputs: Tensor = Input(shape=(IMAGE_NUM_ROWS, IMAGE_NUM_COLS, 1))

x: Tensor = Conv2D(filters=8, kernel_size=(2, 2), padding=&#39;same&#39;, activation=&#39;relu&#39;)(inputs)
x = Conv2D(filters=8, kernel_size=(2, 2), padding=&#39;same&#39;, activation=&#39;relu&#39;)(x)
x = MaxPooling2D(pool_size=(2, 2))(x)

x = Conv2D(filters=16, kernel_size=(2, 2), padding=&#39;same&#39;, activation=&#39;relu&#39;)(x)
x = Conv2D(filters=16, kernel_size=(2, 2), padding=&#39;same&#39;, activation=&#39;relu&#39;)(x)
x = Conv2D(filters=16, kernel_size=(2, 2), padding=&#39;same&#39;, activation=&#39;relu&#39;)(x)
x = MaxPooling2D(pool_size=(2, 2))(x)

x = Flatten()(x)
x = Dense(units=256, activation=&#39;relu&#39;)(x)
x = Dense(units=256, activation=&#39;relu&#39;)(x)
predictions: Tensor = Dense(NUM_CLASSES, activation=&#39;softmax&#39;)(x)

model: Model = Model(inputs=inputs, outputs=predictions)
model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;訓練-評価&#34;&gt;訓練・評価&lt;/h2&gt;

&lt;p&gt;上記モデルを6万個のサンプルでバッチサイズ512で一周(1エポック)学習させると、Intel Core i5-6300HQプロセッサー、メモリ16GBのノートPCで28秒前後かかる。&lt;/p&gt;

&lt;p&gt;とりあえず3エポック学習させてみる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;(ml-mnist) C:\workspace\ml-mnist&amp;gt;python bin\ml_mnist.py train
Using TensorFlow backend.
Epoch 1/3
60000/60000 [==============================] - 28s 465us/step - loss: 0.7813 - acc: 0.7702
Epoch 2/3
60000/60000 [==============================] - 27s 453us/step - loss: 0.1607 - acc: 0.9496
Epoch 3/3
60000/60000 [==============================] - 27s 448us/step - loss: 0.1041 - acc: 0.9681
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ロスが0.1041で正答率が96.81%という結果になった。&lt;/p&gt;

&lt;p&gt;このモデルをテストデータで評価する。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;(ml-mnist) C:\workspace\ml-mnist&amp;gt;python bin\ml_mnist.py eval
Using TensorFlow backend.
10000/10000 [==============================] - 2s 165us/step
loss: 0.08780829641819, acc: 0.9717000002861023
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正答率97.17%。
そんなに良くないけど、過学習はしていない模様。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;次に10エポック学習させて評価してみる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;(ml-mnist) C:\workspace\ml-mnist&amp;gt;python bin\ml_mnist.py train
Using TensorFlow backend.
Epoch 1/10
60000/60000 [==============================] - 29s 486us/step - loss: 0.5814 - acc: 0.8297
Epoch 2/10
60000/60000 [==============================] - 28s 470us/step - loss: 0.1130 - acc: 0.9651
Epoch 3/10
60000/60000 [==============================] - 28s 469us/step - loss: 0.0711 - acc: 0.9777
Epoch 4/10
60000/60000 [==============================] - 28s 468us/step - loss: 0.0561 - acc: 0.9821
Epoch 5/10
60000/60000 [==============================] - 28s 469us/step - loss: 0.0476 - acc: 0.9852
Epoch 6/10
60000/60000 [==============================] - 28s 473us/step - loss: 0.0399 - acc: 0.9879
Epoch 7/10
60000/60000 [==============================] - 28s 467us/step - loss: 0.0338 - acc: 0.9892
Epoch 8/10
60000/60000 [==============================] - 28s 467us/step - loss: 0.0283 - acc: 0.9909
Epoch 9/10
60000/60000 [==============================] - 29s 490us/step - loss: 0.0230 - acc: 0.9925
Epoch 10/10
60000/60000 [==============================] - 28s 471us/step - loss: 0.0223 - acc: 0.9928

(ml-mnist) C:\workspace\ml-mnist&amp;gt;python bin\ml_mnist.py eval
Using TensorFlow backend.
10000/10000 [==============================] - 2s 171us/step
loss: 0.040611953073740006, acc: 0.9866999998092651
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;テストデータでの正答率98.67%。
ちょっと改善した。&lt;/p&gt;

&lt;h2 id=&#34;モデル改善&#34;&gt;モデル改善&lt;/h2&gt;

&lt;p&gt;試しにバッチ正規化層を入れてみる。
ReLUの前に入れるべきという情報があったけど、それだとちょっと修正が面倒なので、単にプーリング層の後に入れてみた。&lt;/p&gt;

&lt;p&gt;3エポックで学習・評価。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;(ml-mnist) C:\workspace\ml-mnist&amp;gt;python bin\ml_mnist.py train
Using TensorFlow backend.
Epoch 1/3
60000/60000 [==============================] - 45s 746us/step - loss: 0.2157 - acc: 0.9336
Epoch 2/3
60000/60000 [==============================] - 44s 737us/step - loss: 0.0513 - acc: 0.9838
Epoch 3/3
60000/60000 [==============================] - 47s 777us/step - loss: 0.0340 - acc: 0.9896

(ml-mnist) C:\workspace\ml-mnist&amp;gt;python bin\ml_mnist.py eval
Using TensorFlow backend.
10000/10000 [==============================] - 2s 246us/step
loss: 0.051704482543468475, acc: 0.9844999995231628
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;98.45%。
1エポックの学習時間は45秒前後に伸びたけど、速く学習できるようにはなった。&lt;/p&gt;

&lt;p&gt;10エポックではどうか。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;(ml-mnist) C:\workspace\ml-mnist&amp;gt;python bin\ml_mnist.py train
Using TensorFlow backend.
Epoch 1/10
60000/60000 [==============================] - 47s 776us/step - loss: 0.2318 - acc: 0.9265
Epoch 2/10
60000/60000 [==============================] - 47s 790us/step - loss: 0.0596 - acc: 0.9811
Epoch 3/10
60000/60000 [==============================] - 47s 778us/step - loss: 0.0370 - acc: 0.9884
Epoch 4/10
60000/60000 [==============================] - 48s 801us/step - loss: 0.0259 - acc: 0.9917
Epoch 5/10
60000/60000 [==============================] - 47s 785us/step - loss: 0.0182 - acc: 0.9942
Epoch 6/10
60000/60000 [==============================] - 48s 794us/step - loss: 0.0132 - acc: 0.9961
Epoch 7/10
60000/60000 [==============================] - 46s 765us/step - loss: 0.0108 - acc: 0.9965
Epoch 8/10
60000/60000 [==============================] - 45s 751us/step - loss: 0.0107 - acc: 0.9965
Epoch 9/10
60000/60000 [==============================] - 45s 749us/step - loss: 0.0055 - acc: 0.9984
Epoch 10/10
60000/60000 [==============================] - 45s 754us/step - loss: 0.0035 - acc: 0.9991

(ml-mnist) C:\workspace\ml-mnist&amp;gt;python bin\ml_mnist.py eval
Using TensorFlow backend.
10000/10000 [==============================] - 2s 243us/step
loss: 0.034382903814315795, acc: 0.9893999994277954
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;98.94%。
バッチ正規化無し版よりも0.27%改善してるけど、誤差の範囲かも。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;MNISTのサイトに載ってるので一番いいのが99.77%。
どうしたらそんなによくなるのか。&lt;/p&gt;

&lt;h2 id=&#34;エラー分析&#34;&gt;エラー分析&lt;/h2&gt;

&lt;p&gt;一番正答率が高かったモデルについて、エラー分析をしてみた。&lt;/p&gt;

&lt;p&gt;まず、エラーが多かった数字を調べる。
数字をエラー数順に並べると、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;ラベル: エラー数
9: 18
7: 18
5: 15
4: 14
6: 14
3: 8
8: 8
2: 7
1: 2
0: 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;9と7が一番分類むずくて、4、5、6がそれらに次いでむずいことがわかる。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;次にエラーのパターンを見てみる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;(予測した数字, ラベル): 出現数
(9, 4): 10
(3, 5): 9
(1, 7): 7
(2, 7): 6
(7, 9): 6
(9, 7): 5
(7, 2): 4
(8, 5): 4
(1, 6): 4
(1, 9): 4
(0, 6): 4
(8, 9): 3
(9, 3): 3
(4, 9): 3
(8, 3): 2
(2, 4): 2
(2, 8): 2
(1, 2): 2
(8, 4): 2
(5, 3): 2
(5, 9): 2
(8, 6): 2
(2, 6): 2
(5, 6): 1
(1, 8): 1
(6, 8): 1
(0, 8): 1
(2, 3): 1
(4, 6): 1
(2, 1): 1
(3, 8): 1
(8, 1): 1
(7, 0): 1
(4, 8): 1
(6, 0): 1
(5, 8): 1
(6, 5): 1
(9, 2): 1
(0, 5): 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4を9と、5を3と、7を1か2と、9を7と間違えたパターンが多い。
特に4と7がむずい模様。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;次に、実際に間違えた画像を見てみる。
多かったパターンについて見てみる。&lt;/p&gt;

&lt;p&gt;4を9と間違えたパターン:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/hello-world-to-ml-with-keras/9-4.png&#34; alt=&#34;9-4.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;なんだこれは。
これはかなり9にも見える。
ちょっと角ばってるとこと、線が右にはみ出しているとこで判断するのか。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;5を3と間違えたパターン:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/hello-world-to-ml-with-keras/3-5.png&#34; alt=&#34;3-5.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;これはもうちょっとモデルに頑張ってほしい。
これを3としてしまうのは残念。
なにがだめだったのかは分からないけど。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;7を1と間違えたパターン:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/hello-world-to-ml-with-keras/1-7.png&#34; alt=&#34;1-7.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;これは1でもいいような…&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;7を2と間違えたパターン:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/hello-world-to-ml-with-keras/2-7.png&#34; alt=&#34;2-7.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;これはちょっと面白い。
7の真ん中に線をいれるパターンを訓練データに足せば対応できそう。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;9を7と間違えたパターン:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/hello-world-to-ml-with-keras/7-9.png&#34; alt=&#34;7-9.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;これもモデルに頑張ってほしかったパターン。
左上のごみが悪さしたんだろうか。
ごみがあるパターンを訓練データに増やすと対応できるかも。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;あと、ちょっと噴いたやつ:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/hello-world-to-ml-with-keras/2-4.png&#34; alt=&#34;2-4.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;これは4。
モデルは2と間違えた。
むずい。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/hello-world-to-ml-with-keras/2-8.png&#34; alt=&#34;2-8.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;これは8。
モデルは2と間違えた。
こんなのテストデータにいれないで…&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>CourseraのDeep Learning SpecializationのImproving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了した</title>
          <link>https://www.kaitoy.xyz/2018/01/12/coursera-deep-learning-improving-deep-neural-networks/</link>
          <pubDate>Fri, 12 Jan 2018 23:41:57 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2018/01/12/coursera-deep-learning-improving-deep-neural-networks/</guid>
          <description>&lt;p&gt;&lt;a href=&#34;https://www.kaitoy.xyz/2018/01/05/coursera-deep-learning-neural-networks-and-deep-learning/&#34;&gt;CourseraのDeep Learning SpecializationのNeural Networks and Deep Learningコースを修了した&lt;/a&gt;のに続き、&lt;a href=&#34;https://www.coursera.org/learn/deep-neural-network&#34;&gt;Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコース&lt;/a&gt;を修了した。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;このコースは、ディープニューラルネットワークのチューニングなどについて学べる3週間のコース。
今のところ全部英語。&lt;/p&gt;

&lt;p&gt;2018/1/5に始めて、1/12に完了。
8日間かかった。
修了したらまた&lt;a href=&#34;https://www.coursera.org/account/accomplishments/certificate/5VS9EJJ6TJ3A&#34;&gt;Certifacate&lt;/a&gt;もらえた。&lt;/p&gt;

&lt;p&gt;以下、3週分の内容をメモ程度に書いておく。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1週目&lt;/p&gt;

&lt;p&gt;OverfittingやUnderfittingを防ぐテクニックについて。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;データ分割&lt;/p&gt;

&lt;p&gt;深層学習のモデル構築は、検討(Idea)、実装(Code)、検証(Experiment)というサイクルの繰り返し(Iterative process)。
取り組む課題、課題のドメイン、データ量、マシンの構成などにより、ハイパーパラメータは変わるので、経験をもって事前に予測することは無理なので、サイクルをどれだけ速く回せるかが鍵。&lt;/p&gt;

&lt;p&gt;データは、訓練データ(Training set)、Devデータ(Development set))(a.k.a. Cross-validation set)、テストデータ(Test set)に分割する。
訓練データで学習し、Devデータでハイパーパラメータを評価し、テストデータで最終的な評価と性能見積をする。
テストデータは無くてもいい。&lt;/p&gt;

&lt;p&gt;サンプル数が1万くらいだった時代は、6:2:2くらいで分割してたけど、近年は数百万とかのデータを扱い、交差検証データやテストデータの割合はもっと小さくするのがトレンド。
98:1:1など。&lt;/p&gt;

&lt;p&gt;Devデータとテストデータは同じようなものを使うべき。
訓練データは必ずしも同様でなくてもいい。訓練データは沢山要るので、別のデータソースからとることもある。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;バイアス vs バリアンス&lt;/p&gt;

&lt;p&gt;でかいネットワークで正則化して大量データで学習させるのが吉。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;正則化&lt;/p&gt;

&lt;p&gt;過学習(Overfitting)を防ぐため、コスト関数を正則化(Regularization)すべし。&lt;/p&gt;

&lt;p&gt;ロジスティック回帰ではL2ノルム(L2 norm)を使ったL2正則化が一般的。
L1正則化はあまり使われない。
L1正則化をすると、wがスパース行列になってモデルを圧縮できると言われることがあるが、経験上その効果はほとんどない。&lt;/p&gt;

&lt;p&gt;正則化パラメータλはハイパーパラメータで、Devデータで評価する。&lt;/p&gt;

&lt;p&gt;ニューラルネットワークでは正則化項にフロベニウスノルム(Frobenius norm)を使う。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Dropout(Inverted Dropout)&lt;/p&gt;

&lt;p&gt;ランダムにノードを無効化しながら学習することで過学習を防ぐ。
画像処理の分野では、特徴量の数が多く学習データが少ない傾向があるので、ほぼ常に使われる。&lt;/p&gt;

&lt;p&gt;コスト関数が計算できなくなるのが欠点。
計算する必要があるときにはDropoutを無効化する。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;データ拡張(Data augmentation)&lt;/p&gt;

&lt;p&gt;データを加工して増やせば、高バリアンス対策になる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;早期終了(Early stopping)&lt;/p&gt;

&lt;p&gt;過学習するまえに学習を止めるテクニック。
訓練データとDevデータについてコストをプロットして、Devデータのものが上がる前に止める。&lt;/p&gt;

&lt;p&gt;これは、直交化(Orthogonalization)の原則、つまり一度に一つのことを考慮すべきという原則に反していて、コストを最小化するという問題と、過学習を避けるという問題に同時に対処することになるので微妙。&lt;/p&gt;

&lt;p&gt;普通は代わりにL2正則化使えばいいけど、λを最適化する手間を省きたいときには選択肢になりうる、というか実現場ではちょくちょく選択肢になる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;訓練データの正規化(Normalization)&lt;/p&gt;

&lt;p&gt;訓練データの各特徴量について平均を0にして分散を1にすると学習が速くなる。&lt;/p&gt;

&lt;p&gt;訓練データを正規化したらテストデータも正規化する。
その際、正規化パラメータは訓練データのものを使う。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;勾配消失(Vanishing gradient)、勾配爆発(Exploding gradient)&lt;/p&gt;

&lt;p&gt;ニューラルネットワークの層が深くなると、層の出力や勾配が指数関数的に大きくなったり小さくなったりして、学習が難しくなる問題。
長年ディープニューラルネットワークの発展を妨げてきた問題。&lt;/p&gt;

&lt;p&gt;パラメータのランダム初期化をすると防げる。
ガウス分布で作ったパラメータに特定の値を掛けてを分散が1/n(ReLUの時は2/n)になるように調整して、活性化関数に入力する値を約1に抑える。
掛ける値は活性化関数ごとにだいたい決まっていて((e.g. Xavier Initialization)、その値をハイパーパラメータとして調整するのはそれほど優先度は高くない。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gradient checking&lt;/p&gt;

&lt;p&gt;順伝播・逆伝播が正確に実装できているかを、数値計算手法で概算した勾配と逆伝播で出した勾配を比べてチェックするテクニック。
計算コストが高くなるので、デバッグ時にのみ使う。&lt;/p&gt;

&lt;p&gt;Dropoutしてるときには使えない。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;初期化&lt;/p&gt;

&lt;p&gt;ゼロ初期化、大きい値でのランダム初期化、He初期化(Xavier初期化っぽいやつ)を実装して性能を比べる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;正則化&lt;/p&gt;

&lt;p&gt;正則化無し、L2正則化、Dropoutの実装と比較。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gradient checking&lt;/p&gt;

&lt;p&gt;Gradient checkingの実装と、その結果を利用した逆伝播のデバッグ。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Yoshua Bengioへのインタビュー&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2週目&lt;/p&gt;

&lt;p&gt;学習を速くするテクニックについて。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ミニバッチ勾配降下法(Mini-batch gradient descent)&lt;/p&gt;

&lt;p&gt;普通の勾配降下法(i.e. バッチ勾配降下法)よりかなり速いので、大規模データでよく使われるテクニック。
学習回数に対するコストのプロットはノイジーになる。&lt;/p&gt;

&lt;p&gt;ミニバッチサイズというハイパーパラメータが増える。
ミニバッチサイズがmならバッチ勾配降下法、1なら確率的勾配降下法(Stochastic gradient descent)になる。&lt;/p&gt;

&lt;p&gt;ミニバッチ勾配降下法と確率的勾配降下法は収束しない。&lt;/p&gt;

&lt;p&gt;バッチ勾配降下法は遅すぎる。
確率的勾配降下法はベクトル化の恩恵がなくなるという欠点がある。
ので、適当なミニバッチサイズにするのがよく、それが一番速い。&lt;/p&gt;

&lt;p&gt;2000個くらいのデータならバッチ勾配降下法。
それより多ければ、64～512位のミニバッチサイズがいい。
メモリ効率を考えると2の累乗数がいいし、CPU/GPUメモリサイズに乗るサイズにすべし。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;指数加重移動平均 (EWMA: Exponentially Weighted Moving Average)&lt;/p&gt;

&lt;p&gt;ノイズのあるデータから、よりスムーズなプロットを書く手法。
過去数日の平均をもとにプロットする。&lt;/p&gt;

&lt;p&gt;この手法だと、最初の方のデータが不当に小さくなってしまう。
これが問題になるなら、バイアス補正(Bias correction)をかける。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;モーメンタム(Momentum)付き勾配降下法&lt;/p&gt;

&lt;p&gt;パラメータを更新するときに、勾配そのままではなく、勾配の指数加重移動平均を使う手法。
勾配降下を滑らかに速くできる。
慣性(勢い)をつけて走り抜ける感じ。&lt;/p&gt;

&lt;p&gt;指数加重移動平均を計算するときのβが新たなハイパーパラメータになる。
普通0.9。この場合バイアス補正はそんなに効果ないので普通かけない。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;RMSprop&lt;/p&gt;

&lt;p&gt;パラメータを更新するときに、勾配そのままではなく、勾配の二乗平均平方根(RMS: Root Mean Square)を使う手法。
学習率を上げつつ、勾配降下を滑らかに速くできる。&lt;/p&gt;

&lt;p&gt;二乗平均平方根を計算するときのβと、ゼロ除算を防ぐためのεが新たなハイパーパラメータになる。
提唱者は、&lt;code&gt;β=0.999&lt;/code&gt;、&lt;code&gt;ε=10^-8&lt;/code&gt;を推奨しているし、これらをチューニングすることはあまりない。&lt;/p&gt;

&lt;p&gt;Couseraが広めたことで、よく使われるようになった。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Adam(Adaptive Moment Estimation)&lt;/p&gt;

&lt;p&gt;モーメンタムとRMSpropとバイアス補正を組み合わせた最適化アルゴリズム。
これをミニバッチ勾配降下法と一緒に使えばだいたい上手くいく。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;学習率減衰(Learning rate decay)&lt;/p&gt;

&lt;p&gt;ミニバッチ勾配降下を収束させるために、学習率を徐々に小さくする手法。
エポックごとに学習率を下げる。&lt;/p&gt;

&lt;p&gt;学習率αが、α0と 減衰率(Decay rate)とエポック番号から計算されるようになるので、α0と減衰率がハイパーパラメータ。&lt;/p&gt;

&lt;p&gt;学習率の計算方法にはいくつかある。
指数関数的に下げたり、階段状に下げたり。&lt;/p&gt;

&lt;p&gt;Andrew先生はあまり使わない手法。
学習率をよくチューニングすれば十分。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;局所最適解(Local optima)&lt;/p&gt;

&lt;p&gt;かつて、勾配が0になる点は、コストの谷、つまり局所最適解だと考えられていて、そこに嵌ることが問題だった。&lt;/p&gt;

&lt;p&gt;けど、ディープニューラルネットワークでは多くは尾根的なもの。
鞍の上みたいな部分なので鞍点(Saddle point)と呼ばれる。
特徴量が沢山あるので、ちょっと動かすとどれかの勾配は負になる。&lt;/p&gt;

&lt;p&gt;よって局所最適解はあまり恐れなくていい。
代わりに、鞍点の台地みたいな部分では勾配が小さいので学習効率が悪くなる。
ここを勢いよく抜けたいので、モーメンタムやRMSpropやAdamが有効になる。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ミニバッチ勾配降下法実装&lt;/p&gt;

&lt;p&gt;訓練データをシャッフルして、ミニバッチサイズに分割して(余りは余りでミニバッチにする)、forループで回す。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;モーメンタムとAdam実装&lt;/p&gt;

&lt;p&gt;単なるミニバッチ勾配降下法とモーメンタム付きとAdamを比較。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Yuanqing Linへのインタビュー&lt;/p&gt;

&lt;p&gt;中国の国立深層学習研究所のトップ。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2週目&lt;/p&gt;

&lt;p&gt;ハイパーパラメータのチューニング方法、バッチ正規化、ソフトマックス回帰、TensorFlow。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ハイパーパラメータのチューニング&lt;/p&gt;

&lt;p&gt;一番重要なのは学習率。
次はモーメンタムのβとかミニバッチサイズとか隠れ層のノード数。
その次がレイヤ数とか学習率減衰率。&lt;/p&gt;

&lt;p&gt;チューニングの際は、かつてはグリッドサーチ(Grid search)がよく使われたけど、これはハイパーパラメータが多くなるとつらい。
ランダムサーチ(Randomized search)がより効率的。&lt;/p&gt;

&lt;p&gt;グリッドサーチだとあるパラメータを固定して別のパラメータを変化させるけど、変化させたパラメータがどうでもいいものだった場合、その試行がほとんど無駄になるので。&lt;/p&gt;

&lt;p&gt;粗くランダムサーチして当たりをつけ、範囲を絞って細かいランダムサーチする。&lt;/p&gt;

&lt;p&gt;ランダムといってもいろいろあって、ユニット数なんかは一様にランダムでいいけど、学習率なんかはlogスケールの上でランダムにしたほうがいい。&lt;/p&gt;

&lt;p&gt;実運用では、計算リソースが少ない場合に採るパンダアプローチと、潤沢なリソースで複数のモデルを同時に訓練するキャビアアプローチがある。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;バッチ正規化(Batch normalization)&lt;/p&gt;

&lt;p&gt;深層学習の実用化において最も重要なアルゴリズムの一つ。
ハイパーパラメータの選定を簡単にして、ディープニューラルネットワークの訓練を簡単にする。&lt;/p&gt;

&lt;p&gt;バッチ正規化では、各層の入力を正規化する。
ミニバッチごとに平均を0、分散を1に正規化した後、βとγというパラメータでそれぞれを調整する。
aよりz(i.e. 活性化関数適用前)を正規化するのが普通。&lt;/p&gt;

&lt;p&gt;(ハイパーではない)パラメータとしてβとγが層ごとに増える。
これらもWとともに学習する。
βがbの役割をするので、bはいらなくなる。&lt;/p&gt;

&lt;p&gt;バッチ正規化は共変量シフト(Covariate shift)という問題に対応するもの。
共変量シフトは、訓練した後で入力の分散が変わると、また訓練しなおさないといけないという問題。
ニューラルネットワークの内部では、前のほうの層のWが学習を進めるたびに変わり、その層の出力が変わる。
つまり後のほうの層への入力が変わるので、後のほうの層の学習が進みにくい。
バッチ正規化は、この後のほうの層への入力の分散を一定範囲に抑えることで、後のほうの層の学習を効率化する。&lt;/p&gt;

&lt;p&gt;Dropoutと同様な論理(ノードへの依存が分散される)で正則化の効果もややある。&lt;/p&gt;

&lt;p&gt;訓練が終わったら、最後のミニバッチの平均μと分散σ^2を保存しておいて、予測時に使う。
μとσ^2は訓練データ全体から再計算してもよさそうだけど、普通はやらない。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ソフトマックス回帰(Softmax regression)&lt;/p&gt;

&lt;p&gt;ニューラルネットワークで多値分類(Multi-class classification)するアルゴリズム。
出力層(ソフトマックス層)のノード数をクラス数にして、活性化関数にソフトマックス関数を使う。
出力層の各ノードは、サンプルが各クラスに属する確率を出力する。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;TensorFlow&lt;/p&gt;

&lt;p&gt;ディープラーニングフレームワークはいろいろある: Caffe/Caffe2、CNTK、DL2J、Keras、Lasagne、mxnet、PaddlePaddle、TensorFlow、Theano、Torch。
プログラミングしやすいこと、訓練性能がいいこと、オープンであることが重要。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;TensorFlowの基本&lt;/p&gt;

&lt;p&gt;TensorFlowでのプログラムはだいたい以下のような手順で書く。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;テンソル(tensor)をつくる。これはまだ評価されない。&lt;/li&gt;
&lt;li&gt;テンソル間の計算式(計算グラフ)を書く。&lt;/li&gt;
&lt;li&gt;テンソルを初期化する。&lt;/li&gt;
&lt;li&gt;セッションを作る。&lt;/li&gt;
&lt;li&gt;セッションを実行する。ここで計算が実行される。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;後で(セッション実行時に)値を入れたい場合はプレースホルダ(placeholder)を使う。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;TensorFlowでのニューラルネットワーク実装&lt;/p&gt;

&lt;p&gt;画像を読み込んで多クラス分類するNNを作る。
以下、今回使った関数の一部。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;シグモイド関数: &lt;code&gt;tf.sigmoid(x)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;エントロピーコスト: &lt;code&gt;tf.nn.sigmoid_cross_entropy_with_logits(logits, labels)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;One-hotエンコーディング: &lt;code&gt;tf.one_hot(labels, depth, axis)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;最急降下法: &lt;code&gt;tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    

  </channel>
</rss>

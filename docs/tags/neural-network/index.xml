<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>To Be Decided </title>
    <link>https://www.kaitoy.xyz/tags/neural-network/</link>
    <language>en-us</language>
    <author>Kaito Yamada</author>
    <rights>(C) 2018</rights>
    <updated>2018-03-25 22:43:27 &#43;0900 JST</updated>

    
      
        <item>
          <title>機械学習のHello World: MNISTの分類モデルをKerasで作ってみた</title>
          <link>https://www.kaitoy.xyz/2018/03/25/hello-world-to-ml-with-keras/</link>
          <pubDate>Sun, 25 Mar 2018 22:43:27 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2018/03/25/hello-world-to-ml-with-keras/</guid>
          <description>

&lt;p&gt;機械学習のHello Worldとしてよくやられる&lt;a href=&#34;http://yann.lecun.com/exdb/mnist/&#34;&gt;MNIST&lt;/a&gt;の分類モデルを&lt;a href=&#34;https://keras.io/ja/&#34;&gt;Keras&lt;/a&gt; on &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;TensorFlow&lt;/a&gt;で作ってみた話。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h2 id=&#34;mnistとは&#34;&gt;MNISTとは&lt;/h2&gt;

&lt;p&gt;手書き数字画像のラベル付きデータセット。
6万個の訓練データと1万個のテストデータからなる。
&lt;a href=&#34;https://creativecommons.org/licenses/by-sa/3.0/&#34;&gt;CC BY-SA 3.0&lt;/a&gt;で&lt;a href=&#34;http://www.pymvpa.org/datadb/mnist.html&#34;&gt;配布されているっぽい&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;一つの画像は28×28ピクセルの白黒画像で、0から9のアラビア数字が書かれている。&lt;/p&gt;

&lt;p&gt;画像とラベルがそれぞれ独特な形式でアーカイブされていて、画像一つ、ラベル一つ取り出すのも一苦労する。&lt;/p&gt;

&lt;h2 id=&#34;kerasとは&#34;&gt;Kerasとは&lt;/h2&gt;

&lt;p&gt;Pythonのニューラルネットワークライブラリ。
バックエンドとしてTensorFlowかCNTKかTheanoを使う。
今回はTensorFlowを使った。&lt;/p&gt;

&lt;h2 id=&#34;やったこと&#34;&gt;やったこと&lt;/h2&gt;

&lt;p&gt;KerasのMNISTの&lt;a href=&#34;https://keras.io/ja/datasets/#mnist&#34;&gt;API&lt;/a&gt;とか&lt;a href=&#34;https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py&#34;&gt;コードサンプル&lt;/a&gt;とかがあけどこれらはスルー。&lt;/p&gt;

&lt;p&gt;MNISTのサイトにあるデータセットをダウンロードしてきて、サイトに書いてあるデータ形式の説明を見ながらサンプルを取り出すコードを書いた。
で、KerasでVGGっぽいCNNを書いて、学習させてモデルをダンプして、ダンプしたモデルをロードしてテストデータで評価するコードを書いた。
コードは&lt;a href=&#34;https://github.com/kaitoy/ml-mnist&#34;&gt;GitHub&lt;/a&gt;に。&lt;/p&gt;

&lt;h2 id=&#34;ネットワークアーキテクチャ&#34;&gt;ネットワークアーキテクチャ&lt;/h2&gt;

&lt;p&gt;入力画像のサイズに合わせてVGGを小さくした感じのCNNを作った。&lt;/p&gt;

&lt;p&gt;VGGは2014年に発表されたアーキテクチャで、各層に同じフィルタを使い、フィルタ数を線形増加させるシンプルな構造でありながら、性能がよく、今でもよく使われるっぽい。&lt;/p&gt;

&lt;p&gt;VGGを図にすると以下の構造。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/hello-world-to-ml-with-keras/vgg16.png&#34; alt=&#34;vgg16.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;実際はバッチ正規化とかDropoutもやるのかも。
プーリング層は数えないで16層なので、VGG-16とも呼ばれる。
パラメータ数は1億3800万個くらいで、結構深めなアーキテクチャ。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;VGG-16は244×244×3の画像を入力して1000クラスに分類するのに対し、MNISTは28×28×1を入れて10クラスに分類するので、以下のような7層版を作った。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/hello-world-to-ml-with-keras/vgg7.png&#34; alt=&#34;vgg7.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;これでパラメータ数は27万個くらい。
訓練データのサンプル数が6万個なので、パラメータ数が大分多い感じではある。&lt;/p&gt;

&lt;p&gt;コードは以下。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;inputs: Tensor = Input(shape=(IMAGE_NUM_ROWS, IMAGE_NUM_COLS, 1))

x: Tensor = Conv2D(filters=8, kernel_size=(2, 2), padding=&#39;same&#39;, activation=&#39;relu&#39;)(inputs)
x = Conv2D(filters=8, kernel_size=(2, 2), padding=&#39;same&#39;, activation=&#39;relu&#39;)(x)
x = MaxPooling2D(pool_size=(2, 2))(x)

x = Conv2D(filters=16, kernel_size=(2, 2), padding=&#39;same&#39;, activation=&#39;relu&#39;)(x)
x = Conv2D(filters=16, kernel_size=(2, 2), padding=&#39;same&#39;, activation=&#39;relu&#39;)(x)
x = Conv2D(filters=16, kernel_size=(2, 2), padding=&#39;same&#39;, activation=&#39;relu&#39;)(x)
x = MaxPooling2D(pool_size=(2, 2))(x)

x = Flatten()(x)
x = Dense(units=256, activation=&#39;relu&#39;)(x)
x = Dense(units=256, activation=&#39;relu&#39;)(x)
predictions: Tensor = Dense(NUM_CLASSES, activation=&#39;softmax&#39;)(x)

model: Model = Model(inputs=inputs, outputs=predictions)
model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;訓練-評価&#34;&gt;訓練・評価&lt;/h2&gt;

&lt;p&gt;上記モデルを6万個のサンプルでバッチサイズ512で一周(1エポック)学習させると、Intel Core i5-6300HQプロセッサー、メモリ16GBのノートPCで28秒前後かかる。&lt;/p&gt;

&lt;p&gt;とりあえず3エポック学習させてみる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;(ml-mnist) C:\workspace\ml-mnist&amp;gt;python bin\ml_mnist.py train
Using TensorFlow backend.
Epoch 1/3
60000/60000 [==============================] - 28s 465us/step - loss: 0.7813 - acc: 0.7702
Epoch 2/3
60000/60000 [==============================] - 27s 453us/step - loss: 0.1607 - acc: 0.9496
Epoch 3/3
60000/60000 [==============================] - 27s 448us/step - loss: 0.1041 - acc: 0.9681
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ロスが0.1041で正答率が96.81%という結果になった。&lt;/p&gt;

&lt;p&gt;このモデルをテストデータで評価する。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;(ml-mnist) C:\workspace\ml-mnist&amp;gt;python bin\ml_mnist.py eval
Using TensorFlow backend.
10000/10000 [==============================] - 2s 165us/step
loss: 0.08780829641819, acc: 0.9717000002861023
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正答率97.17%。
そんなに良くないけど、過学習はしていない模様。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;次に10エポック学習させて評価してみる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;(ml-mnist) C:\workspace\ml-mnist&amp;gt;python bin\ml_mnist.py train
Using TensorFlow backend.
Epoch 1/10
60000/60000 [==============================] - 29s 486us/step - loss: 0.5814 - acc: 0.8297
Epoch 2/10
60000/60000 [==============================] - 28s 470us/step - loss: 0.1130 - acc: 0.9651
Epoch 3/10
60000/60000 [==============================] - 28s 469us/step - loss: 0.0711 - acc: 0.9777
Epoch 4/10
60000/60000 [==============================] - 28s 468us/step - loss: 0.0561 - acc: 0.9821
Epoch 5/10
60000/60000 [==============================] - 28s 469us/step - loss: 0.0476 - acc: 0.9852
Epoch 6/10
60000/60000 [==============================] - 28s 473us/step - loss: 0.0399 - acc: 0.9879
Epoch 7/10
60000/60000 [==============================] - 28s 467us/step - loss: 0.0338 - acc: 0.9892
Epoch 8/10
60000/60000 [==============================] - 28s 467us/step - loss: 0.0283 - acc: 0.9909
Epoch 9/10
60000/60000 [==============================] - 29s 490us/step - loss: 0.0230 - acc: 0.9925
Epoch 10/10
60000/60000 [==============================] - 28s 471us/step - loss: 0.0223 - acc: 0.9928

(ml-mnist) C:\workspace\ml-mnist&amp;gt;python bin\ml_mnist.py eval
Using TensorFlow backend.
10000/10000 [==============================] - 2s 171us/step
loss: 0.040611953073740006, acc: 0.9866999998092651
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;テストデータでの正答率98.67%。
ちょっと改善した。&lt;/p&gt;

&lt;h2 id=&#34;モデル改善&#34;&gt;モデル改善&lt;/h2&gt;

&lt;p&gt;試しにバッチ正規化層を入れてみる。
ReLUの前に入れるべきという情報があったけど、それだとちょっと修正が面倒なので、単にプーリング層の後に入れてみた。&lt;/p&gt;

&lt;p&gt;3エポックで学習・評価。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;(ml-mnist) C:\workspace\ml-mnist&amp;gt;python bin\ml_mnist.py train
Using TensorFlow backend.
Epoch 1/3
60000/60000 [==============================] - 45s 746us/step - loss: 0.2157 - acc: 0.9336
Epoch 2/3
60000/60000 [==============================] - 44s 737us/step - loss: 0.0513 - acc: 0.9838
Epoch 3/3
60000/60000 [==============================] - 47s 777us/step - loss: 0.0340 - acc: 0.9896

(ml-mnist) C:\workspace\ml-mnist&amp;gt;python bin\ml_mnist.py eval
Using TensorFlow backend.
10000/10000 [==============================] - 2s 246us/step
loss: 0.051704482543468475, acc: 0.9844999995231628
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;98.45%。
1エポックの学習時間は45秒前後に伸びたけど、速く学習できるようにはなった。&lt;/p&gt;

&lt;p&gt;10エポックではどうか。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;(ml-mnist) C:\workspace\ml-mnist&amp;gt;python bin\ml_mnist.py train
Using TensorFlow backend.
Epoch 1/10
60000/60000 [==============================] - 47s 776us/step - loss: 0.2318 - acc: 0.9265
Epoch 2/10
60000/60000 [==============================] - 47s 790us/step - loss: 0.0596 - acc: 0.9811
Epoch 3/10
60000/60000 [==============================] - 47s 778us/step - loss: 0.0370 - acc: 0.9884
Epoch 4/10
60000/60000 [==============================] - 48s 801us/step - loss: 0.0259 - acc: 0.9917
Epoch 5/10
60000/60000 [==============================] - 47s 785us/step - loss: 0.0182 - acc: 0.9942
Epoch 6/10
60000/60000 [==============================] - 48s 794us/step - loss: 0.0132 - acc: 0.9961
Epoch 7/10
60000/60000 [==============================] - 46s 765us/step - loss: 0.0108 - acc: 0.9965
Epoch 8/10
60000/60000 [==============================] - 45s 751us/step - loss: 0.0107 - acc: 0.9965
Epoch 9/10
60000/60000 [==============================] - 45s 749us/step - loss: 0.0055 - acc: 0.9984
Epoch 10/10
60000/60000 [==============================] - 45s 754us/step - loss: 0.0035 - acc: 0.9991

(ml-mnist) C:\workspace\ml-mnist&amp;gt;python bin\ml_mnist.py eval
Using TensorFlow backend.
10000/10000 [==============================] - 2s 243us/step
loss: 0.034382903814315795, acc: 0.9893999994277954
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;98.94%。
バッチ正規化無し版よりも0.27%改善してるけど、誤差の範囲かも。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;MNISTのサイトに載ってるので一番いいのが99.77%。
どうしたらそんなによくなるのか。&lt;/p&gt;

&lt;h2 id=&#34;エラー分析&#34;&gt;エラー分析&lt;/h2&gt;

&lt;p&gt;一番正答率が高かったモデルについて、エラー分析をしてみた。&lt;/p&gt;

&lt;p&gt;まず、エラーが多かった数字を調べる。
数字をエラー数順に並べると、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ラベル: エラー数
9: 18
7: 18
5: 15
4: 14
6: 14
3: 8
8: 8
2: 7
1: 2
0: 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;9と7が一番分類むずくて、4、5、6がそれらに次いでむずいことがわかる。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;次にエラーのパターンを見てみる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(予測した数字, ラベル): 出現数
(9, 4): 10
(3, 5): 9
(1, 7): 7
(2, 7): 6
(7, 9): 6
(9, 7): 5
(7, 2): 4
(8, 5): 4
(1, 6): 4
(1, 9): 4
(0, 6): 4
(8, 9): 3
(9, 3): 3
(4, 9): 3
(8, 3): 2
(2, 4): 2
(2, 8): 2
(1, 2): 2
(8, 4): 2
(5, 3): 2
(5, 9): 2
(8, 6): 2
(2, 6): 2
(5, 6): 1
(1, 8): 1
(6, 8): 1
(0, 8): 1
(2, 3): 1
(4, 6): 1
(2, 1): 1
(3, 8): 1
(8, 1): 1
(7, 0): 1
(4, 8): 1
(6, 0): 1
(5, 8): 1
(6, 5): 1
(9, 2): 1
(0, 5): 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4を9と、5を3と、7を1か2と、9を7と間違えたパターンが多い。
特に4と7がむずい模様。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;次に、実際に間違えた画像を見てみる。
多かったパターンについて見てみる。&lt;/p&gt;

&lt;p&gt;4を9と間違えたパターン:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/hello-world-to-ml-with-keras/9-4.png&#34; alt=&#34;9-4.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;なんだこれは。
これはかなり9にも見える。
ちょっと角ばってるとこと、線が右にはみ出しているとこで判断するのか。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;5を3と間違えたパターン:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/hello-world-to-ml-with-keras/3-5.png&#34; alt=&#34;3-5.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;これはもうちょっとモデルに頑張ってほしい。
これを3としてしまうのは残念。
なにがだめだったのかは分からないけど。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;7を1と間違えたパターン:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/hello-world-to-ml-with-keras/1-7.png&#34; alt=&#34;1-7.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;これは1でもいいような…&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;7を2と間違えたパターン:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/hello-world-to-ml-with-keras/2-7.png&#34; alt=&#34;2-7.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;これはちょっと面白い。
7の真ん中に線をいれるパターンを訓練データに足せば対応できそう。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;9を7と間違えたパターン:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/hello-world-to-ml-with-keras/7-9.png&#34; alt=&#34;7-9.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;これもモデルに頑張ってほしかったパターン。
左上のごみが悪さしたんだろうか。
ごみがあるパターンを訓練データに増やすと対応できるかも。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;あと、ちょっと噴いたやつ:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/hello-world-to-ml-with-keras/2-4.png&#34; alt=&#34;2-4.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;これは4。
モデルは2と間違えた。
むずい。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/hello-world-to-ml-with-keras/2-8.png&#34; alt=&#34;2-8.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;これは8。
モデルは2と間違えた。
こんなのテストデータにいれないで…&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>CourseraのDeep Learning SpecializationのSequence Modelsコースを修了した</title>
          <link>https://www.kaitoy.xyz/2018/02/27/coursera-deep-learning-sequence-models/</link>
          <pubDate>Tue, 27 Feb 2018 00:49:05 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2018/02/27/coursera-deep-learning-sequence-models/</guid>
          <description>&lt;p&gt;&lt;a href=&#34;https://www.kaitoy.xyz/2018/02/06/coursera-deep-learning-convolutional-neural-networks/&#34;&gt;CourseraのDeep Learning SpecializationのConvolutional Neural Networksコースを修了した&lt;/a&gt;のに続き、&lt;a href=&#34;https://www.coursera.org/learn/nlp-sequence-models&#34;&gt;Sequence Modelsコース&lt;/a&gt;を修了した。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;このコースは、RNNの原理、代表的なアーキテクチャ、自然言語処理などについて学べる3週間のコース。
生成モデルが色々出てきて面白い。
動画は今のところ全部英語。&lt;/p&gt;

&lt;p&gt;2018/2/6に始めて、2/27に完了。
22日間かかった。
修了したらまた&lt;a href=&#34;https://www.coursera.org/account/accomplishments/certificate/NCW69X7UASJ6&#34;&gt;Certifacate&lt;/a&gt;もらえた。&lt;/p&gt;

&lt;p&gt;また、これでDeep Learning Specializationのすべてのコースを修了したので、全部まとめた&lt;a href=&#34;https://www.coursera.org/account/accomplishments/specialization/certificate/4487DSN9ARXN&#34;&gt;Certifacate&lt;/a&gt;ももらえた。
結局2ヶ月ほどかかり、1万円以上課金してしまった…&lt;/p&gt;

&lt;p&gt;以下、3週分の内容をメモ程度に書いておく。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1週目&lt;/p&gt;

&lt;p&gt;連続データを扱うシーケンス(Sequence)モデルについて学ぶ。
RNN、LSTM、GRU、BRNN。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;再帰型ニューラルネットワーク(Recurrent Neural Network)&lt;/p&gt;

&lt;p&gt;シーケンスモデルにはRNNなどがあって、音声認識(Speech recognition)や自然言語処理(Natural language processing)に使われる。
音楽生成(Music generation)、感情分類(Sentiment classification)、DNA解析(DNA sequence analysis)、動画行動認識(Video Activity Recognition)、固有表現抽出(Named entity recognition)なんてのも。&lt;/p&gt;

&lt;p&gt;入力だけが連続データだったり、出力だけが連続データだったり、両方だったり。&lt;/p&gt;

&lt;p&gt;自然言語処理では、ボキャブラリ(Vocabulary)を使って単語をone hotベクトルにして扱う。
ボキャブラリは普通5万次元くらいのベクトル。
ボキャブラリにない単語はそれ用(unknown)の次元に割り当てる。&lt;/p&gt;

&lt;p&gt;入力や出力の次元がサンプルごとに違うので、普通のNNは使えない。
また、普通のNNだと、文のある個所から学んだ特徴を他の箇所と共有しない。
また、普通のNNだと、入力サイズが大きすぎて、パラメータが多くなりすぎる。
RNNはこうした問題を持たない。&lt;/p&gt;

&lt;p&gt;RNNは、最初の単語xを受け取り、層で計算し、最初の出力yとアクティベーションaを出し、そのaと次のxを同じ層で受け取り、次のyとaをだす、ということを繰り返す。
xにかける重みをWax、aにかける重みをWaa、yにかける重みをWyaと呼ぶ。
あとaとyを計算するときに足すバイアスがあって、それぞれba、by。
あるxの計算をするときに、その前のxも使うので、連続データ処理に向いてる。
けど、後のxを考慮しないところが欠点。
この欠点に対処したのがBRNN(Bidirectional RNN)。&lt;/p&gt;

&lt;p&gt;RNNのaの活性化関数にはtanhがよく使われる。
ReLUもあり。
yには二値分類だったらシグモイドだし、そうでなければソフトマックスとか。&lt;/p&gt;

&lt;p&gt;損失関数は普通に交差エントロピーでいいけど、yがベクトルなので、その各要素について交差エントロピーを計算して、足し合わせたものが損失になる。
ここから逆伝播するんだけど、その際に連続データを過去にさかのぼるので、時をかける逆伝播(BPTT: Backpropagation through time)と呼ばれる。&lt;/p&gt;

&lt;p&gt;上で説明したRNNは、入力と出力が同じ長さだけど、そうでない問題のほうが多い。
感情分類なんかは、任意の長さの文を入力して、5段階評価とかするので、最後のxまで入力した後で一つだけyを出すようにする。
音楽生成なんかは入力が一つで出力が多いので、入力がない部分は前の出力を代わりに入力する。
翻訳みたいに入力と出力の長さが違うときは、前半入力だけして、後半出力だけする。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;言語モデル(Language model)&lt;/p&gt;

&lt;p&gt;ある文のあとに、どんな分が続くかを確率で示してくれるモデル。&lt;/p&gt;

&lt;p&gt;訓練データは、文をトークンに分解してone-hotベクトルにして、最後にEOSトークンを加えて作る。&lt;/p&gt;

&lt;p&gt;モデルは、RNNの出力をボキャブラリと同じサイズのソフトマックスにして、どの単語の確率が高いかを出力させる。
最初に0ベクトルを入力し、その出力を次の入力にして、それを繰り返す。&lt;/p&gt;

&lt;p&gt;単語じゃなくて文字単位でやるモデルもあるけど、あんまり使われない。&lt;/p&gt;

&lt;p&gt;このモデルを使うと、学習した文章に似た雰囲気の分を生成できる。
このとき、出力したベクトルから、各単語の確率にしたがって単語をサンプリングし、それを次の入力にする。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;RNNの勾配消失&lt;/p&gt;

&lt;p&gt;英語の文だと、主語が単数だと動詞の形が変わるんだけど、主語と動詞がすごい離れていることがありうる。
最初のほうの単語である主語は浅い層(初期のステップ)で処理されて、一方動詞は深い層(あとのほうのステップ)で処理されることになる。
すると、勾配消失により、深い層の単語が浅い層から受ける影響が小さくなってしまって、動詞の形をいい感じに学習できない。
これがナイーブなRNNの欠点。&lt;/p&gt;

&lt;p&gt;勾配爆発も起こり得るけど、Gradient clipping、つまり勾配の値を計算した後に値が閾値を超えていたら修正する手法を使えば比較的簡単に回避できるので、勾配消失が深刻。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;GRU(Gated Recurrent Unit)&lt;/p&gt;

&lt;p&gt;RNNにMemory cell&amp;copy;というアイデアを加えたもの。
cは浅い層の情報を深い層に伝える役目をして、勾配消失問題を緩和する。
cの候補は毎回、前回のcとxの線形変換をtanhに入れたものとして生成され、それを、0か1を返すゲートΓu(シグモイドな感じの関数)で実際にcとして使うかを決めて、cを更新していく。
このゲートを更新ゲート(Update gate)という。
cはソフトマックスに入れてyを出力したり、次のステップのaにする。&lt;/p&gt;

&lt;p&gt;実際には、もう一つ関連ゲート(Relevance gate)Γrってのがあって、cの候補を計算するときに前回のcに掛ける。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;LSTM(Long short term memory)&lt;/p&gt;

&lt;p&gt;RNNの勾配消失に対処するまた別のアイデア。
GRUよりパワフル。
けどGRUより古くからあるもので、GRUがそれのシンプル版という関係。&lt;/p&gt;

&lt;p&gt;LSTMの論文はかなりむずい。&lt;/p&gt;

&lt;p&gt;GRUと比べると、まずΓrはない。&lt;/p&gt;

&lt;p&gt;で、GRUはΓuが1だったらcを更新して、0だったら前のを保持するという感じだったけど、LSTMでは忘却ゲート(Forget gate)Γfに前回のcをかけて、捨てるかどうかを決める。&lt;/p&gt;

&lt;p&gt;また、出力ゲート(Output gate)Γoが追加されて、単にcを次のaにするんじゃなくて、&lt;code&gt;Γo*c&lt;/code&gt;をaにする。&lt;/p&gt;

&lt;p&gt;各ゲートは前のaと今回のxの線型結合にバイアスを加えたものをシグモイドして計算する。
ゲートの計算にcもいれることがあって、のぞき穴接続 (Peephole connection)と呼ばれる。&lt;/p&gt;

&lt;p&gt;基本的にはLSTM使えばいいけど、GRUのほうが計算コストが少なくて大きなネットワーク作りやすいから、GRUのほうがいいこともある。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;BRNN(Bidirectional RNN)&lt;/p&gt;

&lt;p&gt;普通にRNNやった後、後ろの入力から順番に逆向きにRNNする。
yは、順向きのaと逆向きのaとバイアスを線形計算して非線形変換したものになる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Deep RNN&lt;/p&gt;

&lt;p&gt;単にyを出力するんじゃなくて、そのyを入力とする別のRNNを積み上げていくとdeepになる。
RNNは時間軸の方向にすでに深いので、出力方向には普通は2、3個だけ積み上げる。&lt;/p&gt;

&lt;p&gt;出力方向にRNNを積み上げる代わりに、出力を普通のNNにいれるってのもある。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;p&gt;3つもある…&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ベーシックRNNとLSTMの順伝播をNumPyで実装&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;恐竜の名前を生成する言語モデルをNumPyで実装&lt;/p&gt;

&lt;p&gt;Gradient clippingとサンプリングを実装して、モデルを訓練しながら、恐竜の名前をいい感じに生成できるようになっていく様を観察する。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;LSTMの音楽生成モデルをKerasで実装&lt;/p&gt;

&lt;p&gt;Jazzの曲の断片を学習させて、それっぽい曲を生成してみる。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2週目&lt;/p&gt;

&lt;p&gt;自然言語処理。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;単語埋め込み(Word embedding)&lt;/p&gt;

&lt;p&gt;1週目でやったように、単語をone-hotベクトルで表すと、単語同士の積が0になって、単語間の関係(距離)が表せられない。
代わりに、単語を特徴のベクトルにして、各次元に特徴量をもたせる、特徴付き表現(Featurized representation)がある。&lt;/p&gt;

&lt;p&gt;単語の特徴は数百とかにするけど、可視化するために2Dにすることがある。
このための代表的なアルゴリズムがt-SNE。
t-SNEは複雑で非線形な処理をするので、後述の類推には使えないけど、似たような単語のクラスタを観察できる。&lt;/p&gt;

&lt;p&gt;特徴の分布を表すN次元の空間に単語を埋め込むため、単語埋め込みという。
このN次元ベクトルを単語の数だけ結合したものをEで表す。&lt;/p&gt;

&lt;p&gt;この表現形式にすると、大量の適当なテキストデータで学習させたり、学習済みのモデルをダウンロードしたりしたあと、特定のタスクのために転移学習させることができる。&lt;/p&gt;

&lt;p&gt;また、類推(Analogical reasoning)が可能になる。
単語のペアが二つあって、ペア内の単語ベクトル間の差を計算して、ペア間でその値が近ければ、それらのペアは同じような関係の組み合わせだと言える。
例えば、&lt;code&gt;男 - 女&lt;/code&gt;は&lt;code&gt;王 - 女王&lt;/code&gt;に近くなる。
類似度の計算にはコサイン類似度(Cosine similarity)がよく使われる。
ユークリッド距離(Euclidean distance)でもいいけど、コサイン類似度のほうが一般的。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;単語埋め込みの学習&lt;/p&gt;

&lt;p&gt;Eにone-hotベクトルをかけると、そのone-hotベクトルが表す単語の特徴ベクトルが得られる。&lt;/p&gt;

&lt;p&gt;数単語の後に続く単語を予測するニューラル言語モデルを考えると、与えられたそれぞれの単語を表すone-hotベクトルを入力して、Eをかける層があって、その結果を全結合層にいれて、その結果をボキャブラリサイズのベクトルを出力するソフトマックス層にいれる。
ソフトマックス層の出力で、一番大きい値の単語が予測する単語になる。
Eをパラメータにしておくと、このモデルを訓練するとEが学習される。
予測精度自体はそんなによくならなくても問題ない。&lt;/p&gt;

&lt;p&gt;予測する単語の前だけじゃなくて、前後の単語を使って学習させたりも。
1単語だけで予測しても結構いい結果になる。&lt;/p&gt;

&lt;p&gt;上記のモデルはSkip-Gramモデルと呼ばれる。
実際には、文の中からターゲット単語を選び、前後ウィンドウサイズ(5とか)以内のコンテクスト単語を一つ選び、それらをペアにして一つの訓練データを作る。
コンテクスト単語は完全にランダムに選ぶと、aとかtheとかofとかが多くなっちゃうので、ヒューリスティックにバランスよく選ぶ必要がある。&lt;/p&gt;

&lt;p&gt;ソフトマックス層は、ボキャブラリサイズのベクトルを出力するため、計算コストがでかい。
その対策として、階層的ソフトマックス(Hierarchical softmax)ってのがあって、これは木構造で分類を表す手法。
もう一つ負例サンプリング(Negative sampling)という手法があって、こっちのほうがシンプルで効果的。&lt;/p&gt;

&lt;p&gt;Skip-Gramモデルのほか、CBOW(Continuous Bag Of Words)というモデルもある。
これはターゲット単語とその前後数単語を訓練データにするもの。
これらのモデルをWord2Vecモデルといったり、それを使ってEを学習する手法をWord2Vecアルゴリズムといったりする。&lt;/p&gt;

&lt;p&gt;負例サンプリングではまず、コンテクスト単語に対して別の単語を与え、ターゲット単語なら1、違うなら0というラベル付き訓練データを作る。
ターゲット単語とコンテクスト単語の組はSkip-Gramと同様に選んで、0になる単語はランダムに選ぶ。
コンテクスト単語一つに対し、yが1になるデータを一つ、0になるデータをk個(2～20くらい)作る。
kはデータセットが大きいほど少なくする。
で、Skip-Gramモデルのソフトマックス層を、ボキャブラリの数だけの二値分類ノードに変える。
これらのノードは毎回全部計算するんじゃなくて、k+1個だけを計算するので計算コストが小さい。&lt;/p&gt;

&lt;p&gt;単語埋め込みの学習アルゴリズムとして、Word2Vecじゃないものだと、GloVe(Global Vectors)アルゴリズムってのがある。
GloVeでは、単語iと単語jが近くに現れることが何回あるかをxijで表す。
xijをボキャブラリのすべての単語の組み合わせで数えて、それらと何かの二乗誤差にヒューリスティックな重み付けをしたコスト関数を作って最適化して単語埋め込みを学習させる。
細かいことはよくわからなかった…&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;感情分類&lt;/p&gt;

&lt;p&gt;Yelpみたいなサービスで、コメントから星の数を推定する。&lt;/p&gt;

&lt;p&gt;感情分類は、教師データがあまり得られないことが多いのが課題だけど、よく訓練したEがあれば上手く分類できる。&lt;/p&gt;

&lt;p&gt;コメントを単語に分解したら、それぞれのone-hotベクトルをEとかけて単語ベクトルを作って、単語間の平均を計算して、ソフトマックス層に入れて、出力を星の数だけ作る。
というのがシンプルなモデル。
これだと語順を考慮しないので、RNNに単語ごとに入力して、最後の出力をソフトマックスするといい感じになる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;単語埋め込みからのバイアス除去(Debias)&lt;/p&gt;

&lt;p&gt;単語埋め込みに性別とか人種のバイアスがかかってると、いいモデルができない。
例えば、&lt;code&gt;プログラマ - 男 + 女 = 主婦&lt;/code&gt;みたいになってるとまずい。
こういうバイアスは、学習データのテキストのバイアス、つまりそれを書いた人のバイアスからくる。&lt;/p&gt;

&lt;p&gt;単語空間のバイアスの方向を調べるには、例えば性別のバイアスなら、&lt;code&gt;he - she&lt;/code&gt;とか&lt;code&gt;male - female&lt;/code&gt;とかの平均をとる。
するとある1次元のベクトルが得られて、これがバイアスの方向になる。
この上にある値を他の次元の射影に変換することでバイアスを削減(Neutralize)できる。&lt;/p&gt;

&lt;p&gt;実際には、特異値分解(SVD: Singular Value Decomposition)でもっとシステマチックにバイアスを計算する。
バイアスも数次元のベクトルになりうる。&lt;/p&gt;

&lt;p&gt;Neutralizeしたらさらに、バイアスをなくす方向に単語の組の位置をずらす(Equalize pairs)。
例えば、男と女の組を、ベビーシッターからの距離に差がなくなるようにずらす。&lt;/p&gt;

&lt;p&gt;Neutralizeすべき単語は、線形分類で決めることができる(?)。
Equalizeすべき単語は手で選ぶ。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;訓練済みのGloVeの単語埋め込みをロードして、コサイン類似度計算を実装して、類推を実行。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;NeutralizationとEqualizationを実装。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;感情分類(文に自動で絵文字を付ける)の、シンプルな実装と2層LSTMでの実装。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;3週目&lt;/p&gt;

&lt;p&gt;連続データを入力して連続データを出力するRNNアーキテクチャを学ぶ。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Sequence to sequence(Seq2Seq)モデル&lt;/p&gt;

&lt;p&gt;翻訳などに使うモデル。
エンコーダ(encoder)ネットワークで元の単語を読み込み、デコーダ(decoder)ネットワークが翻訳を吐き出す。&lt;/p&gt;

&lt;p&gt;画像を読んでその説明文(caption)を吐くのにもつかわれる。
この場合、CNNで画像を読んで、最後の全結合層の出力をRNNに入れる。&lt;/p&gt;

&lt;p&gt;小説を生成するような言語モデルと異なるのは、ランダムに生成してほしいのではなくて、ベストな結果を生成してほしいところ。
デコーダネットワークの部分は言語モデルと一緒。
入力がエンコーダネットワークの出力か0ベクトルかの違いだけ。
このようなのを条件付き言語モデル(Conditional language model)と呼ぶ。&lt;/p&gt;

&lt;p&gt;言語モデルだと、次に出力する単語をランダムで選んでたけど、翻訳ではそうはいかない。
貪欲法(Greedy algorithm)みたいに、毎回一番確率が高いものを選んでもうまくいかない。
最終的に出力する全単語の確率の掛け合わせが最大になるやつを選びたい。&lt;/p&gt;

&lt;p&gt;ビームサーチ(Beam search)を代わりに使う。
ビーム幅(Beamwidth)Bを決めて、最初にBの数だけ単語の候補を確率の高い順に選ぶ。
で、それらを次の入力にしてB個の出力ベクトルを得たら、最初の単語の確立をそれらのベクトルの各要素にかけて、大きい順にまたB個単語を選ぶ。
あとはこれの繰り返し。&lt;/p&gt;

&lt;p&gt;ビームサーチをもう少し改良できる。
その一つが長さ正規化(Length normalization)。
条件付確率を計算するときに、単語が増えてくると1未満の数を何回もかけることになり、アンダーフローや丸め誤差が発生してしまう。
ので、各単語の確率をかけ合わせる代わりに、各確率のlogを足し合わせる。
これだとまだ、確率が1未満なのでlogは常に負の値になり、単語の数が少ない程総和は大きくなるので、翻訳結果に短い文が選ばれがちになっちゃう。
ので単語数で割る。
または単語数をα(0～1)乗したもので割る。
この式(目的関数)をNormalized log probability objectiveとかNormalized log likelihood objectiveとか呼ぶ。&lt;/p&gt;

&lt;p&gt;Bはどう選ぶか。
Bが小さいと計算コストが低く、最適解を見つける可能性が低い。
Bが大きいとその逆。
プロダクション環境では、10～100くらいが一般的。
研究では数千とかも。
試行錯誤していい値を見つけるしかない。&lt;/p&gt;

&lt;p&gt;深さ優先探索(DFS: Depth First Search)、幅優先探索(BFS: Breadth First Search)に比べて、ビームサーチは最適解を見つけられないかもしれないけど、リーズナブル。&lt;/p&gt;

&lt;p&gt;ビームサーチはヒューリスティックなアルゴリズムなので、いつもいい結果を出すとは限らない。
だめだったときはエラー分析をする。
ダメな翻訳が出力されたとき、RNNの訓練が足らないのか、ビームサーチのBが小さすぎるのかを切り分けたい。&lt;/p&gt;

&lt;p&gt;まずは、翻訳中のある単語について、期待する出力とモデルの出力の確率をそれぞれ見てみる。
前者が大きければ、モデルは正しい出力してるけど、ビームサーチが間違ったものを選択している。
逆ならモデルに問題がある。
(長さ正規化してたらその目的関数を比べる。)
これを色んなサンプルで試して、より多くのサンプルでダメだったほうの改善に努めるべし。&lt;/p&gt;

&lt;p&gt;いい感じの訳が複数あったらどうする?
Bleu(BiLingual Evaluation Understudy)スコアで評価する。
Bleuスコアは、生成した訳が期待する訳(リファレンス)のいずれかに近ければ高くなる。&lt;/p&gt;

&lt;p&gt;precisionは、生成した訳の単語のいくつが、リファレンスにも出現するかを示す。
これだと、例えば&lt;code&gt;the the the is the the&lt;/code&gt;みたいな訳で高い値(&lt;code&gt;6/6&lt;/code&gt;)をとれちゃう。
のでmodified precisionを代わりに使う。
modified precisionでは、それぞれの単語について、一つのリファレンスに最大何回出現するかをcreditと定義する。
で、&lt;code&gt;the&lt;/code&gt;のcreditが2、&lt;code&gt;is&lt;/code&gt;のcreditが1なら、上記訳のmodified precisionは&lt;code&gt;3/6&lt;/code&gt;になる。&lt;/p&gt;

&lt;p&gt;Bleuスコアは、Nグラム(N-Gram)についてmodified precisionを計算する。
あるN-Gramについてのmodified precisionをPn、N-Gramの数をNとすると、Bleuスコアは&lt;code&gt;exp((ΣPn)/N) * BP&lt;/code&gt;。
BPはbrevity penaltyで、短い訳で高いスコアを簡単に取れないようにするためのもの。
出力長がリファレンス長より長ければ1で、そうでなければ&lt;code&gt;exp(1 - 出力長 / リファレンス長)&lt;/code&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Attentionモデル&lt;/p&gt;

&lt;p&gt;Seq2Seqを改良したもの。&lt;/p&gt;

&lt;p&gt;長い文の場合でも、Seq2Seqは原文を全部読んで、それをactivationに記憶して、そこから翻訳を出力する。
けど人が翻訳するときは、短い文や節に区切って出力していく。
実際、長い原文を記憶するのは難しく、原文の単語数が増えていくにしたがってSeq2Seqは性能が落ちる。
Attentionモデルは人と同様な翻訳の仕方をするので、原文の単語数が増えても性能を保てる。&lt;/p&gt;

&lt;p&gt;Attentionモデルでは、エンコーダはBRNN。
で、エンコーダはデコーダに対し、t番目の出力に際してどの入力単語に注目すべきかという重みづけαから生成するコンテクストcを入力する。
エンコーダの順方向と逆方向のアクティベーションを結合したものをaとすると、&lt;code&gt;c = Σαa&lt;/code&gt;。
デコーダはそのcと、1ステップ前のアクティベーションsを使って一単語を出力する。
1ステップ前のsとaを小さいシンプルなNNにいれて入力単語ごとに計算したeを、入力単語に渡って足すと1になるようにスケールしたものがα。&lt;/p&gt;

&lt;p&gt;Attentionモデルは、画像を読んで説明文を出力するときにも使われる。
説明文は画像の特定の箇所に注目した説明の集まりなので。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;音声認識(Speech recognition)&lt;/p&gt;

&lt;p&gt;Seq2Seqモデルで、音声データを読んで字幕を出力する。
従来、音声を音素(Phoneme)に分解して処理していたが、深層学習ではその必要が無い。
但し300～100000時間くらいの音声データが要る。&lt;/p&gt;

&lt;p&gt;普通に音声データをエンコーダに時系列に従って入力してデコーダに文字を出力させてもいいけど、それだと、入力データのステップ数は出力ステップ数よりはるかに大きくなっちゃう。
ので、CTC(Connectionist temporal classification)モデルは、RNNに入力と同じだけ出力をさせて、その出力を圧縮して最終的な字幕を生成する。
例えば&lt;code&gt;the&lt;/code&gt;について、&lt;code&gt;ttt_h_eee___&lt;/code&gt;みたいな出力をさせる。
&lt;code&gt;_&lt;/code&gt;はブランクという特殊な出力で、単語の切れ目のスペースとはまた別のもの。&lt;/p&gt;

&lt;p&gt;「OK Google」みたいなトリガーワードを識別するシステムに使うアルゴリズムはまだ発展途上で、これといったものはない。
例えば、トリガーワードを含む音声を入力して、トリガーワードの終わりの部分の出力を1、それ以外を0として学習させる方法がある。
終わりの瞬間だけ1にすると0ばっかりになっちゃうので、終わりから一定時間1にする。
トリガーワードを聞いた瞬間に検知するようにしたいので、BRNNじゃなくて単方向のRNNを使う。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Kerasで日付を特定のフォーマットに変換するAttentionモデルを作る。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;トリガーワード検知システムを作る。&lt;/p&gt;

&lt;p&gt;トリガーワードとそれ以外の言葉とノイズを別々に録音して、合成してラベルを付けて訓練データを作る。
で、Kerasで1D畳み込み層がひとつ、GRUが2層のDeep RNNモデルを作って学習させる。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    
      
        <item>
          <title>CourseraのDeep Learning SpecializationのConvolutional Neural Networksコースを修了した</title>
          <link>https://www.kaitoy.xyz/2018/02/06/coursera-deep-learning-convolutional-neural-networks/</link>
          <pubDate>Tue, 06 Feb 2018 00:37:11 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2018/02/06/coursera-deep-learning-convolutional-neural-networks/</guid>
          <description>&lt;p&gt;&lt;a href=&#34;https://www.kaitoy.xyz/2018/01/16/coursera-deep-learning-ml-strategy/&#34;&gt;CourseraのDeep Learning SpecializationのStructuring Machine Learning Projectsコースを修了した&lt;/a&gt;のに続き、&lt;a href=&#34;https://www.coursera.org/learn/convolutional-neural-networks&#34;&gt;Convolutional Neural Networksコース&lt;/a&gt;を修了した。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;このコースは、CNNの原理、代表的なアーキテクチャ、応用などについて学べる4週間のコース。
動画は今のところ全部英語。
プログラミング課題は初のKeras。&lt;/p&gt;

&lt;p&gt;このコースは結構難しくて、特に3週目と4週目は理解に苦しんだ。
というか理解しきれなかったような。
けどNST面白かった。&lt;/p&gt;

&lt;p&gt;2018/1/16に始めて、2/6に完了。
22日間かかった。
修了したらまた&lt;a href=&#34;https://www.coursera.org/account/accomplishments/certificate/MVNK5ZA5CDKA&#34;&gt;Certifacate&lt;/a&gt;もらえた。&lt;/p&gt;

&lt;p&gt;以下、4週分の内容をメモ程度に書いておく。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1週目&lt;/p&gt;

&lt;p&gt;畳み込みニューラルネットワーク(CNN: Convolutional neural network)の基本。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;畳み込み計算&lt;/p&gt;

&lt;p&gt;画像認識でよく使われるNNのアーキテクチャ。&lt;/p&gt;

&lt;p&gt;低層ではエッジを検出し、層が進むにつれて複雑な特徴を学習する。&lt;/p&gt;

&lt;p&gt;画像を特定の行列(普通は奇数の正方行列。3×3が多い。)で畳み込むことで、特定の方向のエッジを検出できる。
この行列をフィルタ(filter)という。カーネルと呼ばれることもある。
例えば縦なら以下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[[1, 0, -1],
 [1, 0, -1],
 [1, 0, -1]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;縦でもいろいろフィルタはあって、以下はSobelフィルタというもの。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[[1, 0, -1],
 [2, 0, -2],
 [1, 0, -1]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以下はScharrフィルタ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[[ 3, 0,  -3],
 [10, 0, -10],
 [ 3, 0,  -3]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;縦のフィルタを90度回転すると横のフィルタになる。&lt;/p&gt;

&lt;p&gt;深層学習では、フィルタもパラメータとして学習させる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;パディング(Padding)&lt;/p&gt;

&lt;p&gt;&lt;code&gt;n×n&lt;/code&gt;の行列を&lt;code&gt;f×f&lt;/code&gt;のフィルタで畳み込むと&lt;code&gt;n-f+1×n-f+1&lt;/code&gt;の行列になる。
つまり畳み込めば畳み込むほど画像が小さくなってしまう。
また、画像の端のほうはフィルタにかかる割合が小さいので、情報量が小さくなってしまう。
これらを解決するテクニックがパディング(Padding)。
行列の周囲を0でパディングして、サイズを大きくしてから畳み込む。
パディングがないのをValidな畳み込み、出力が入力と同じサイズになるようにパディングするのをSameな畳み込みという。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Strided畳み込み&lt;/p&gt;

&lt;p&gt;畳み込むときにフィルタをずらす幅を1より大きくする。
パディングサイズがpでストライドがsのとき、&lt;code&gt;n×n&lt;/code&gt;の行列を&lt;code&gt;f×f&lt;/code&gt;のフィルタで畳み込むと&lt;code&gt;(n+2p-f)/s+1×(n+2p-f)/s+1&lt;/code&gt;の行列になる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;3次元(カラー画像)の畳み込み&lt;/p&gt;

&lt;p&gt;カラー画像は3次元の行列、つまり&lt;code&gt;n×n×c&lt;/code&gt;の行列で、それを畳み込むのは&lt;code&gt;f×f×c&lt;/code&gt;のフィルタで、出力は&lt;code&gt;n-f+1×n-f+1&lt;/code&gt;の行列になる。
チャネルごとにフィルタを設定して、色ごとにエッジ検出できる。
フィルタごとの出力は全部スタックして、最終的な出力は3次元になる。&lt;/p&gt;

&lt;p&gt;畳み込み層はフィルタの要素数がパラメータ数になる。
入力画像の大きさに依存しないので、パラメータ数が少なくて済み、過学習しにくい。&lt;/p&gt;

&lt;p&gt;入力を複数の畳み込み層に通したら、最終的に3次元の出力をなべてベクトルにして、後ろの層に渡す。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プーリング層(Pooling layer)&lt;/p&gt;

&lt;p&gt;計算量を減らすため、また特徴の抽出のために、畳み込み層のあとに使われる層。
基本Max poolingが使われるけど、Average poolingというのもある。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Max pooling: フィルタをかけた部分を畳み込む代わりに、最大値を出力とする。大きな値が特徴が大きく出ているところだから、特徴を凝縮するイメージだけど、経験的にこれで上手くいくことが分かっているだけで、なぜ上手くいくかは判明していない。この層はパラメータを持たない。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Average pooling: フィルタをかけた部分を畳み込む代わりに、平均を出力とする。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;プーリング層のフィルタは大抵、サイズが&lt;code&gt;2×2&lt;/code&gt;でパディングが0でストライドは2。&lt;/p&gt;

&lt;p&gt;普通、畳み込み層とプーリング層とセットで1層と数える。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;全結合層(Fully connected layer)&lt;/p&gt;

&lt;p&gt;全ノードがメッシュ状につながった普通の層。
畳み込み層とプーリング層のセットがいくつかあって、その出力をベクトルになべて、全結合層につなぐ。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;一般的なCNN&lt;/p&gt;

&lt;p&gt;畳み込み層は、普通nhとnwを縮め、ncを増やす。
また、全体として、層が浅くなるほど出力が減るのが多い。&lt;/p&gt;

&lt;p&gt;CNNはハイパーパラメータが多すぎるので、アーキテクチャは自分で考えるんではなく、論文呼んで自分の問題に合いそうなのを探すべし。&lt;/p&gt;

&lt;p&gt;畳み込み層は全結合層に比べてパラメータ数がかなり少なくて済むのがいいところ。
これはパラメーター共有(Parameter sharing)という、画像のある個所で上手く動いたフィルタ(e.g. 縦エッジ検出器)は、その画像の他の箇所でも上手く働くという考え方がベース。&lt;/p&gt;

&lt;p&gt;また、層間の接続がまばらなのもパラメータを減らす要因。
つまり出力のあるピクセルは、入力のうちフィルタ分のサイズのピクセルとしか関連していない。&lt;/p&gt;

&lt;p&gt;CNNは空間変化の不変性(Translation invariance)に強い。
つまり画像の中の物体の位置が変わってもよく検出できる。
これは同じフィルタを画像全体に適用するから。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;p&gt;CNNの順伝播をNumPyで実装。&lt;/p&gt;

&lt;p&gt;CNNによる画像の分類をTensorFlowで実装。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2週目&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ケーススタディ&lt;/p&gt;

&lt;p&gt;畳み込み層とかプーリング層をどう組み合わせるといいかは、事例を見ていくことで雰囲気をつかめる。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;古いやつ&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;LeNet-5&lt;/p&gt;

&lt;p&gt;1980年代にできたふるいやつ。
モノクロ画像(32×32)の手書き数字認識。&lt;/p&gt;

&lt;p&gt;当時はソフトマックスもReLUもなかった。
けど、畳み込み層とプーリング層のセットを繰り返して入力をチャネル方向に引き伸ばし、全結合層に流し込むアーキテクチャは、モダンなCNNにも通じる。&lt;/p&gt;

&lt;p&gt;5層(内2層が全結合層)の浅いネットワークで、比較的パラメータが少なく、6万個くらい。
モダンなのだとこの1000倍くらいあるのが普通。&lt;/p&gt;

&lt;p&gt;LeNet-5は、チャネルごとに違うフィルタを使っているが、今日では普通同じのを使う。&lt;/p&gt;

&lt;p&gt;また、プーリング層のあとに活性化関数(シグモイド)かけてるのも特殊。
(モダンなアーキテクチャではプーリング層の前にかける?)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;AlexNet&lt;/p&gt;

&lt;p&gt;227×227×3のカラー画像
8層(内3層が全結合層)でパラメータは6千万個くらい。
活性化関数にReLU。&lt;/p&gt;

&lt;p&gt;Local Response Normalizationという正規化層がある。
昨今ではあまり使われない。&lt;/p&gt;

&lt;p&gt;論文が比較的読みやすい。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;VGG-16&lt;/p&gt;

&lt;p&gt;2014年に発表。&lt;/p&gt;

&lt;p&gt;各層に同じフィルタを使い、フィルタ数も線形増加させるシンプルなアーキテクチャ。
16層(内2層が全結合層)で、1億3800万個のパラメータ。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;モダンなやつ&lt;/p&gt;

&lt;p&gt;理論的にはネットワークを深くすると精度が高くなるけど、現実的にそうはいかない。
深いネットワークは勾配消失や勾配爆発で訓練しにくいので。
モダンなアーキテクチャはこの問題に対応。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ResNet(Residual Network)&lt;/p&gt;

&lt;p&gt;残差ブロック(Residual block)を持つ。
このブロックでは、浅い層からの出力を深い層のReLUの入力に足し合わせる。
この深い層からの依存はショートカット(short cut)とかskip connectionとか呼ばれる。&lt;/p&gt;

&lt;p&gt;ショートカットのおかげで深い層の学習が効率的になり、層を152まで深くできた。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Network in Network&lt;/p&gt;

&lt;p&gt;畳み込み層に1×1のフィルタを使う。
1×1畳み込み(one by one convolution)、またはNetwork in Networkと呼ばれる。&lt;/p&gt;

&lt;p&gt;これを使うと、入力のhとwを変えずに、チャネル数を減らして計算量を減らしたり、非線形性を追加することができる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Inception Network (GoogleNet)&lt;/p&gt;

&lt;p&gt;フィルタのサイズや畳み込みかプーリングかを考えるのが難しいので、1層内で複数のフィルタサイズで畳み込みやプーリングして、スタックしたものを出力する手法がある。
これをする部分をInception moduleという。
計算コストが大きくなるので、最初に1×1畳み込みで圧縮してからその後の畳み込みをする。
1×1畳み込みの部分でデータがいったん小さくなるので、そこをボトルネック層(Bottleneck layer)と呼ぶ。&lt;/p&gt;

&lt;p&gt;ボトルネック層によって、精度に影響が出ることはない。&lt;/p&gt;

&lt;p&gt;Inception moduleを組み合わせたネットワークをInception Networkという。
Inception Networkの例の一つがGoogLeNet。
GoogLeNetは中間層から全結合層・ソフトマックス層につなげる支流をもっていて、中間層まででうまく学習できているかを見れて、過学習を防げるようになっている。&lt;/p&gt;

&lt;p&gt;因みにInceptionという名前は映画のInceptionから来ている。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;実践&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;既存の実装の利用&lt;/p&gt;

&lt;p&gt;モダンなCNNは複雑すぎて、エキスパートが論文を読み込んでも再現することが難しい。
が、普通は論文の著者がOSSで実装を公開するのでそれを使ったりベースにしたりすべし。&lt;/p&gt;

&lt;p&gt;学習済みのモデルもあることがあるので、転移学習にも使える。
ソフトマックス層だけ入れ替えて、そこのWだけ学習させて自分の問題に使うなど。
色んな深層学習フレームワークが転移学習をサポートしてる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;データ合成(Data augmentation)&lt;/p&gt;

&lt;p&gt;画像認識の分野では基本的にデータが沢山要るけどデータが手に入りにくい。
ので合成するのが効果的。&lt;/p&gt;

&lt;p&gt;左右判定とか、切り抜きとか、回したり、歪めたりとかは、有効だけどあんまりやられない。
若干編集が複雑なので。&lt;/p&gt;

&lt;p&gt;色相を変える(Color shifting)のがよくやられる。赤味を増やしたり。
色を選ぶときには主成分分析(PCA)が使える。(PCA Color Augmentation)&lt;/p&gt;

&lt;p&gt;一つのCPUスレッドに元画像のロードと合成をやらせて、別のスレッドで並列に学習を処理するのが一般的な実装。&lt;/p&gt;

&lt;p&gt;データ合成するにも、どの程度変化させるかというハイパーパラメータが付きまとうので、既存の実装やアイデアを使うのがいい。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;画像認識の現状&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;データ vs hand engineering&lt;/p&gt;

&lt;p&gt;データが沢山ある分野の問題だと、でかいネットワークを適当に組んで学習させれば上手く解ける。
データがあんまりないと、色々工夫(hand engineering)が必要になってくる。
特徴量を選んだり、アーキテクチャを工夫したり。
例えば物体検知は画像認識よりかなりデータが少ない。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ベンチマークやコンペで上手くやるコツ&lt;/p&gt;

&lt;p&gt;研究者は、論文を通しやすくするため、ベンチマークやコンペのデータに対して頑張る。
ベンチマークに対して上手くやるコツ:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;アンサンブル(Ensembling)&lt;/p&gt;

&lt;p&gt;複数のNNを独立に訓練して、それらの出力の平均を使う。
1,2%の性能向上が見込める。
けど計算コストが高いので、普通プロダクションでは使わない。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Multi-crop at test time&lt;/p&gt;

&lt;p&gt;テスト時にテストデータを色んな感じに切り抜いて、それらに対する予測値を平均する。
10-crop。
アンサンブルに比べ、訓練時の計算コストが少ないし、予測時に1つのモデルを保持すればいいのでメモリ使用量が少ない。
若干の性能向上が見込め、プロダクションでも使われることがある。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;オープンソースコードの利用&lt;/p&gt;

&lt;p&gt;だれかが考えたアーキテクチャを使え。&lt;/p&gt;

&lt;p&gt;OSS実装を使え。&lt;/p&gt;

&lt;p&gt;なんなら訓練済みモデルを使え。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Kerasのチュートリアル&lt;/p&gt;

&lt;p&gt;というほど解説してくれるわけではないけど。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Kerasで50層のResNetを実装&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;3週目&lt;/p&gt;

&lt;p&gt;物体認識(Object detection)。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;位置特定(Localization)&lt;/p&gt;

&lt;p&gt;画像を与えられて単にラベルを付けるのは分類。
ラベルの物体の位置を示すのが位置特定。
分類したあとさらに位置特定したい。&lt;/p&gt;

&lt;p&gt;分類する画像は、普通一つの画像の中に一つの物体が大きく映っている。
一方、物体認識は、一つの画像の中に複数の物体があったりする、もう少し複雑な問題。&lt;/p&gt;

&lt;p&gt;位置特定するには、ソフトマックス層に、クラス以外に4つの出力をさせる。
すなわち物体の中心点のx座標、y座標、それと物体を囲む枠の高さ、幅。
それぞれ0～1の値で、画像全体に対する割合を示す。&lt;/p&gt;

&lt;p&gt;また、物体があるかないかという予測値Pcも出力する。
この予測値が0のときは、損失関数でそれいがいの出力を計算に入れない。&lt;/p&gt;

&lt;p&gt;より一般的には、物体の位置を示す任意の数のランドマーク(Landmark)の座標を出力させる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;物体認識&lt;/p&gt;

&lt;p&gt;スライディングウィンドウ認識(Sliding windows detection)する。
すなわち、小さい枠をずらしながら画像の切り抜きをたくさん作って、それぞれ分類する。
ウィンドウサイズを変えてもやる。
計算コストがかかる。&lt;/p&gt;

&lt;p&gt;のでCNNでやる。&lt;/p&gt;

&lt;p&gt;まず、全結合層は、数学的に等価な畳み込み層で置き換えられる。
5×5×16の入力を受け取って、5×5の400個のフィルタで畳み込むと、
1×1×400の出力が得られ、これは400ノードの全結合層と一緒。
最後に1×1の4個のフィルタで畳み込むと、4つの出力をするソフトマックス層みたいになる。
こういう、全部畳み込み層のNNをFCN(Fully Convolutional Networks)という。&lt;/p&gt;

&lt;p&gt;で、入力のサイズ(高さと幅)を広げてやると、中間層と出力もちょっと広がる。
このCNNは、入力の一部を5×5のウィンドウで切り抜いた部分の分類結果が、出力の1ピクセルに対応するようなものになる。
なので一回CNNに通せば、一回の計算でスライディングウィンドウできる。&lt;/p&gt;

&lt;p&gt;けどこれは、物体を囲む枠が正確でないという欠点がある。
実際は長方形であるべきだったりするので。
これを解決するのがYOLO(You Only Look Once)アルゴリズム。&lt;/p&gt;

&lt;p&gt;YOLOでは、まず入力画像をグリッド状に分割して、それぞれについて分類と位置特定する。
複数のセルに物体がまたがっている場合は、物体の中心があるセルだけにあるものとする。
それぞれのセルの出力をスタックして、3次元の出力にする。
つまり、グリッドが3×3なら、3×3×(もとのyベクトルの次元)とする。
(普通はもっと細かいグリッドにする。)
で、CNNをこういう形の出力をするように組む。&lt;/p&gt;

&lt;p&gt;YOLOの論文はかなりむずい。&lt;/p&gt;

&lt;p&gt;位置特定の評価をするのに、IoU(Intersection over Union)という指標がある。
これは、2つの領域の重なり具合を示すもので、2つの領域が重なった部分の面積を、2つの領域全体の面積で割った値。
モデルが特定した枠と期待する枠とで、IoUが0.5以上だとよしとすることが多い。&lt;/p&gt;

&lt;p&gt;YOLOを使うと、複数のセルで同じ物体を認識してしまうことが多い。
これを一つに絞るのがNMS(Non-max suppression)。
ざっくり言うと、それぞれのセルの確度(Pc)を見て、一番でかいの以外を無効化する。&lt;/p&gt;

&lt;p&gt;詳しく言うとまず、Pcがある閾値(e.g.0.6)以下のものを無効化する。
で、残ったものの中から、最大のPcを選び、それとおおきくかぶっている枠(IoUが0.5以上など)を無効化する。
で、また残ったのものの中から最大のPcを選び、同じことを繰り返していく。
クラスが複数あったら、これをクラスごとにやる。&lt;/p&gt;

&lt;p&gt;一つのセルに複数の物体があったらどうか。
境界ボックス(Anchor box)を使う。
事前に複数の形の枠(境界ボックス)を用意しておいて、それぞれについての予測を出力ベクトルに並べる。
で、一番IoUが高い境界ボックスを採用する。&lt;/p&gt;

&lt;p&gt;境界ボックスは手動で作ったり、k平均法で作ったりする。&lt;/p&gt;

&lt;p&gt;スライディングウィンドウは、明らかに何もない部分の計算もしちゃうのでちょっと無駄。
なので、そういう部分はスキップしようというのがR-CNN(Regions with CNN)。
これはRegion proposalsとCNNを組み合わせたもの。
Region proposalsは、セグメンテーション(Segmentation)アルゴリズムで画像をざっくり区分けして、それっぽい部分を処理対象にするもの。&lt;/p&gt;

&lt;p&gt;R-CNNはすごく遅いので、あまり使われていないし、Andrew先生も好んで使わない。
Fast R-CNN、Faster R-CNNってのもあるけど、まだ遅い。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;KerasでYOLOv2モデルを実装。&lt;/p&gt;

&lt;p&gt;CNN部分は訓練済みのモデルを使って、出力をフィルタリングする部分を作る。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;4週目&lt;/p&gt;

&lt;p&gt;顔認識(Face recognition)とNeural style transfer。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;顔認識&lt;/p&gt;

&lt;p&gt;顔認証(Face authentication)には、顔認識と、生きた人間かの判定(Liveness detection)が要るけど、前者を主に学ぶ。&lt;/p&gt;

&lt;p&gt;顔認識は顔検証(Face verification)の難しい版。
後者は顔画像と名前を与えて、正しい組み合わせかを判定する。
前者は顔画像を与えて、DBからその人を探す。&lt;/p&gt;

&lt;p&gt;顔認識は一般的に、One-shot learning問題に対応する必要がある。
つまり一つの訓練データから学習しないといけない。
DBに一人のひとについて何個も画像があるというケースは少ない。&lt;/p&gt;

&lt;p&gt;CNNに顔画像を入力して、ソフトマックス層で分類するのは、訓練データが少なすぎるのでうまくいかない。
代わりに類似関数(Similarity function)を学習する。
つまり、二つの画像を入力として、異なる度合いを出力するもの。
で、その出力が閾値以下だったら同一人物と判定する。
これをDBに入っている画像それぞれについてやる。&lt;/p&gt;

&lt;p&gt;類似関数にはシャム(Siamese)ネットワークをつかう。&lt;/p&gt;

&lt;p&gt;CNNの最後の全結合層の出力ベクトルは、入力画像をエンコードしたものだと考えられる。
二つの画像を、別々に同じCNNにいれて、二つの出力ベクトルを得たら、それらのユークリッド距離の二乗を差として出力する。
これがシャムネットワーク。
二つの画像が同一人物ならユークリッド距離が小さくなるように、違うなら大きくなるように訓練する。&lt;/p&gt;

&lt;p&gt;損失関数にはTriplet loss関数を使う。
同一人物を比較するとき、Anchor画像とPositive画像の比較、違う人物の比較はAnchor画像とNegative画像の比較と呼ぶ。
AnchorとPositiveのユークリッド距離がAnchorとNegativeのユークリッド距離以下になってほしい。
つまり前者マイナス後者がゼロ以下になって欲しい。
ただこれだと、CNNが全ての画像について同じ出力をするように学習してしまうかもしれないので、&lt;code&gt;0-α&lt;/code&gt;以下になるように訓練する。
αはマージンと呼ばれるハイパーパラメータ。&lt;/p&gt;

&lt;p&gt;AnchorとPositiveとNegativeの一組をTripletと呼ぶ。
Negativeをランダムに選ぶと、全然違う顔の組み合わせが多くなって、類似関数があまり学習しない。
ので、似てるひとを組み合わせたTripletを多く作ってやると効率よく学習する。&lt;/p&gt;

&lt;p&gt;シャムネットワークの二つの出力ベクトルをロジスティック回帰ユニットに入れて、同一人物か否かの二値分類する方法もある。
ベクトル間の距離も、ユークリッド距離の他、カイ二乗値(χ square similarity)ってのもある。&lt;/p&gt;

&lt;p&gt;DBには、顔画像そのものよりも、エンコードしたベクトルを入れておくと計算量を省けるし、DBサイズも抑えられる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ニューラル画風変換(NST: Neural style transfer)&lt;/p&gt;

&lt;p&gt;Content画像&amp;copy;とStyle画像(S)から、あらたな画像(G)を生成するCNN。
(CNNは別途訓練済みのものを使うので、転移学習の一種。)&lt;/p&gt;

&lt;p&gt;CNNの可視化ができる。
画像の一部を入力して、ある層の一つのユニットの出力が最大になるものを選んで集めると、そのユニットがどのような特徴を抽出しているかがわかる。
深い層ほど広い範囲を見て、複雑なパターンを学ぶ。&lt;/p&gt;

&lt;p&gt;コスト関数&lt;code&gt;J(G)&lt;/code&gt;を最小化する。
&lt;code&gt;J(G)&lt;/code&gt;はコンテントコスト&lt;code&gt;Jcontent(C, G)×α&lt;/code&gt;とスタイルコスト&lt;code&gt;Jstyle(S, G)×β&lt;/code&gt;の和。
前者はCとGの類似関数で、後者はSとGの類似関数。
αとβはハイパーパラメータ。
Gをランダムに初期化して、最急降下法でGを調整していく。&lt;/p&gt;

&lt;p&gt;訓練済みのCNN(VGGなど)を使って、中間層lを選ぶ。
Cを入力してlから出てきた値と、Gを入力してlから出てきた値が似てたら、CとGを似ているとする。
つまりそれらをベクトルにアンロールしたものの二乗誤差が&lt;code&gt;Jcontent(C, G)&lt;/code&gt;。
lが浅ければ浅いほど、CとGは似たものになる。&lt;/p&gt;

&lt;p&gt;スタイルは、ある層lの出力のx座標とy座標の各点について、チャネル間で値の関連性を見る。
あるチャネルのあるニューロンが縦縞を検出していて、ほかのチャネルのあるニューロンがオレンジ色を検出していたとしたら、画像にオレンジの縦縞がよく現れるなら両者は関連が高く、そうでなければ低い。
SとGとの間でこの関連性が似てれば、スタイルが似ていると言える。&lt;/p&gt;

&lt;p&gt;スタイル行列(Style matrix)で表す。
ある層のスタイル行列は&lt;code&gt;nc×nc&lt;/code&gt;で、チャネル間の関連度を表す。
行列の一つの値は、二つのチャネルの各アクティベーションの掛け合わせものの合計。
関連性が強いとこの掛け合わせは大きくなる。
(対角成分は同じチャネル同士の積になって、そのスタイルがどれだけ全体的に出ているかを示す。)
この行列は代数学ではグラム行列(Gram matrix)と呼ばれる。&lt;/p&gt;

&lt;p&gt;このスタイル行列をSとGで計算して、それらの二乗誤差をl層のスタイルコストとする。
で、これにハイパーパラメータλlをかけたものを層ごとに計算して、足し合わせたものを全体のスタイルコストとする。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2D以外の画像の処理&lt;/p&gt;

&lt;p&gt;心電図のデータとかの1Dデータや、CTスキャンみたいな3Dデータでも、それに合わせた次元のフィルタを使えば畳み込める。
1DデータはRNNでもできるけど、CNNでもできて、それぞれメリットデメリットがある。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;TensorFlowでニューラル画風変換を実装&lt;/p&gt;

&lt;p&gt;VGG-19をImageNetのデータで訓練したものを使う。
ルーブル美術館の写真をContent画像に、モネの絵をStyle画像に使って画像を生成。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Kerasで顔認識モデルを実装&lt;/p&gt;

&lt;p&gt;現時点でtriplet_lossの採点にバグがある。
対策はフォーラム参照。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    
      
        <item>
          <title>CourseraのDeep Learning SpecializationのStructuring Machine Learning Projectsコースを修了した</title>
          <link>https://www.kaitoy.xyz/2018/01/16/coursera-deep-learning-ml-strategy/</link>
          <pubDate>Tue, 16 Jan 2018 07:56:43 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2018/01/16/coursera-deep-learning-ml-strategy/</guid>
          <description>&lt;p&gt;&lt;a href=&#34;https://www.kaitoy.xyz/2018/01/12/coursera-deep-learning-improving-deep-neural-networks/&#34;&gt;CourseraのDeep Learning SpecializationのImproving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了した&lt;/a&gt;のに続き、&lt;a href=&#34;https://www.coursera.org/learn/machine-learning-projects&#34;&gt;Structuring Machine Learning Projectsコース&lt;/a&gt;を修了した。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;このコースは、深層学習プロジェクトの進め方のコツや問題への対処方法などについて学べる2週間のコース。
今回はプログラミング課題がない。
動画は今のところ全部英語。&lt;/p&gt;

&lt;p&gt;ちょっと動画編集ミスが多かった。
同じことを二回言ったり、無音無絵の時間があったり、マイクテストしてたり。&lt;/p&gt;

&lt;p&gt;2018/1/13に始めて、1/15に完了。
3日間かかった。
修了したらまた&lt;a href=&#34;https://www.coursera.org/account/accomplishments/certificate/7MHFMLHP67C4&#34;&gt;Certifacate&lt;/a&gt;もらえた。&lt;/p&gt;

&lt;p&gt;以下、2週分の内容をメモ程度に書いておく。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1週目&lt;/p&gt;

&lt;p&gt;モデルの改善をするのに、データを増やしたりハイパーパラメータを変えたり色々な手法がある。
一つを試すのに下手すると数か月とかかかるので、効率よく手法の取捨選択し、モデルを改善していくための戦略について学ぶ。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;直交化(Orthogonalization)&lt;/p&gt;

&lt;p&gt;一つの要素で複数の制御をしようとすると難しいので、一つの制御だけするようにする。
具体的には、以下のことを別々に扱う。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;訓練データに目標の精度までフィットさせる。&lt;/li&gt;
&lt;li&gt;devデータに目標の精度までフィットさせる。&lt;/li&gt;
&lt;li&gt;テストデータに目標の精度までフィットさせる。&lt;/li&gt;
&lt;li&gt;現実のデータでうまく動くようにする。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;それぞれの目的について、チューニングすべき要素は別々になる。&lt;/p&gt;

&lt;p&gt;早期終了は直行化の原則に反しているので、ほかの方法があるならそっちをやったほうがいい。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;指標(Goal)の設定&lt;/p&gt;

&lt;p&gt;モデルの改善はイテレーティブなプロセスなので、サイクルを速く回したい。
そのため、モデルを評価する単一の数値があるといい。
F1スコアとか。平均とか&lt;/p&gt;

&lt;p&gt;単一の指標にまとめるのがむずいときもある。
精度と速さとか。
そんなときは一つ以外の指標を足切りだけに使う。
ある閾値以上の速さが出てるもののなかで精度をくらべるなど。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;データの分け方&lt;/p&gt;

&lt;p&gt;devデータとテストデータの分布(と評価指標)は同じ感じにしないといけない。
そのために、いったん全データをシャッフルしてから分割する。
訓練データの分布は異なってても問題ない。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;訓練:テスト = 70:30&lt;/code&gt;とか、&lt;code&gt;訓練:dev:テスト = 60:20:20&lt;/code&gt;とかいう比率は、1万くらいのデータなら適当。
けど100万くらいなら、98:1:1くらいが妥当。&lt;/p&gt;

&lt;p&gt;テストデータはモデルの最終評価をするためのものなので、どれだけ評価したいかによってサイズを変える。
0もありがちだけど、非推奨。&lt;/p&gt;

&lt;p&gt;猫の画像のなかにエロ画像が混じっちゃうようなモデルはだめ。
猫判定率が多少下がっても、エロ画像が含まれないほうがまし。
こういう場合は評価指標を変える必要がある。
エロ画像を猫と判定した場合にペナルティを大きくするなど。&lt;/p&gt;

&lt;p&gt;直行化の観点で言うと、指標を決めるのと、その指標に従って最適化するのは、別のタスクとして扱うべき。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;人並性能(Human-level performance)との比較&lt;/p&gt;

&lt;p&gt;人並性能とは、人が手動で達成できる精度。そのエラー率、つまり人並誤差(Human-level error)はベイズ誤差(Bayes optimal error)に近い。&lt;/p&gt;

&lt;p&gt;モデルを改良していくと、人並性能を超え、その後改善速度は鈍化し、人並誤差はベイズ誤差に漸近していく。
鈍化する理由は、人並性能がベイズ誤差に近いのと、人以上の精度に人がチューニングするのが無理があるから。
人手でラベル付きデータを作れないし、エラー分析もできなくなるので。&lt;/p&gt;

&lt;p&gt;人並誤差より訓練データでのエラー率が結構高いなら、高バイアス対策をする。
人並誤差と訓練データでのエラー率が近くて、devデータでのエラー率が結構高いなら、高バリアンス対策をする。
人並誤差と訓練データでのエラー率との差ををAndrew先生は可避バイアス(Avoidable bias)と名付けた。&lt;/p&gt;

&lt;p&gt;人並誤差はベイズ誤差の近似として使える。
人並誤差は、ある判別問題に関して、その道のエキスパート達が議論して解を出すみたいな、人類が全力を尽くしたうえでの誤差とする。
人並誤差が分かれば、訓練データとdevデータのエラー率を見て、高バイアスか高バリアンス化を判別できる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;モデルの性能改善手順&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;訓練データにフィットさせ、可避バイアスを最小化する。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;モデルを大きくする。&lt;/li&gt;
&lt;li&gt;最適化アルゴリズムを高度なものにするか、長く訓練する。&lt;/li&gt;
&lt;li&gt;NNのレイヤを深くしたり隠れ層のノードを増やしたり、CNNとかRNNとかの高度なアーキテクチャにする。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;dev・テストデータで評価し、バリアンスを下げる。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;データを増やす。&lt;/li&gt;
&lt;li&gt;正則化する。&lt;/li&gt;
&lt;li&gt;ハイパーパラメータをいじる。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Andrej Karpathyへのインタビュー&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2週目&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;エラー分析&lt;/p&gt;

&lt;p&gt;エラー率が高いときに、エラーが起きたdevデータのサンプルを見て原因を分析する。
天井分析(Ceiling analysis)も併せてやる。
例えば、猫判定器で、犬を猫と判定したサンプルがあったとして、犬の問題に取り組むかどうかは、全体のエラーサンプル数に対する犬のエラーサンプル数の割合を見て、その取り組みで最大どれだけの効果を得るかを分析する。&lt;/p&gt;

&lt;p&gt;天井分析を複数の問題点に対してやれば、どれに時間をかける価値があるかの指標を得られる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ラベリングミスへの対処&lt;/p&gt;

&lt;p&gt;ディープラーニングは、ランダムエラーに対して堅牢で、訓練データに多少のラベリングミスがあっても問題なく動く。&lt;/p&gt;

&lt;p&gt;devデータにミスがあった場合、エラー分析の際にそれも数えておいて、対処すべきかどうかを判断する。
他の問題によるエラーの割合と比べて、ラベリングミスによるものの割合が大きければ対処すればいいし、そうでなければほっておく。
また、エラー全体に対するラベリングミスの割合が大きくなると、モデルの性能比較に支障が出てくるので、そうなったらラベリングミスに対処する必要が高まってくる。&lt;/p&gt;

&lt;p&gt;devデータのラベリングミスを直すときは、テストデータも同時に直し、分布に違いが出ないようにする。
また、エラーなサンプルだけじゃなく、正答したサンプルも見直すべし。
けど、訓練データは直さなくてもいい。数も多いし、devデータと分布が違っていても問題ないし。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;新しいディープラーニングシステムを作るときのガイドライン&lt;/p&gt;

&lt;p&gt;あまり経験のない分野のシステムを新たに作るなら、早く立ち上げてイテレーションを回すべし。
具体的には、&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;dev・テストデータと、指標を用意する。&lt;/li&gt;
&lt;li&gt;さっさとシステムを実装する。&lt;/li&gt;
&lt;li&gt;バイアス/バリアンス分析やエラー分析をして、次のアクションを決める。&lt;/li&gt;
&lt;li&gt;システムを改善する。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;というのを速く回す。
経験が深かったり、確かな研究結果がすでにあったりするなら、最初から凝ったシステムにしてもいい。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;訓練データとdev・テストデータの分布のミスマッチ&lt;/p&gt;

&lt;p&gt;ディープラーニングでは訓練データは大抵不足するから、現実的に、訓練データはいろんなデータのかき集めで、dev・テストデータとは分布が異なってくる。&lt;/p&gt;

&lt;p&gt;例えば、実際のデータが10000個で、かき集めのが200000個あったら、全部合わせてシャッフルしてデータ分割するのは、dev・テストデータの質が悪くなるのでダメ。
dev・テストデータには実際のデータだけ使って、かき集めのは全部訓練データに使うべし。&lt;/p&gt;

&lt;p&gt;分布がミスマッチになると、バイアス/バリアンス分析がむずくなる。
devデータでエラーが増えても、単にdevデータのほうが判別が難しいデータなのかもしれない。&lt;/p&gt;

&lt;p&gt;分析をしやすくするため、訓練devデータを作る。
これは、訓練データと同じ分布のデータで訓練に使わないデータ。訓練データのサブセット。&lt;/p&gt;

&lt;p&gt;訓練データと訓練devデータのエラー率の差が大きければオーバーフィット。
訓練データと訓練devデータのエラー率の差が小さくて、devデータのエラー率が高いなら、データミスマッチ問題(Data missmatch problem)。&lt;/p&gt;

&lt;p&gt;あんまり発生しないけど、devデータよりテストデータのエラー率が結構大きいなら、devデータにオーバーフィットしてるので、devデータサイズを増やしたりする必要がある。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;データミスマッチ問題への対応&lt;/p&gt;

&lt;p&gt;データミスマッチ問題が発覚したら、訓練データをdevデータに近づけたり、devデータに近いものを訓練データに加えたりすることを考える。
例えば、車内の音声認識のためのモデルを開発していて、devデータに車の雑音が入っていることが分かったら、訓練データにそういう雑音を合成してやるなど。
ただし、その場合、同じ雑音をすべての訓練データに適用すると、その雑音にモデルがオーバーフィットするリスクがあるので注意。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;転移学習(Transfer learning)&lt;/p&gt;

&lt;p&gt;ある問題に対して作ったモデルを、別の問題に再利用する。
(但し入力は同種のデータ。画像なら画像、音声なら音声。)
その際、NNの、出力層だけのパラメータをランダム初期化したり、層を足したりして、あたらしい訓練データで学習させる。
新たな訓練データが少ない場合は、後ろの1,2層だけを再学習させ、データがたくさんあったら全体を再学習させる。&lt;/p&gt;

&lt;p&gt;最初の学習を事前学習(Pre-training)、新たなデータでの学習をファインチューニング(Fine tuning)という。
画像認識の分野でよく使われる。NNの浅い層のほうが、汎用的な特徴(エッジ検出など)を学習するので再利用できると考えられているが、詳しい原理は判明していない。&lt;/p&gt;

&lt;p&gt;事前学習するデータより、ファインチューニングに使えるデータが少ないときに効果的。
データ量が同じくらいだったり、後者のほうが多い場合は、最初から目的のデータで学習させたほうがいい。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;マルチタスク学習(Multi-task learning)&lt;/p&gt;

&lt;p&gt;一度に複数の判別をさせるモデルをつくる。
ソフトマックス層つかった多クラス分類に似てるけど、一つのサンプルから複数のクラスを検出する。
例えば車と歩行者と標識など。
物体検知によく使われる。&lt;/p&gt;

&lt;p&gt;一つ一つのクラスのデータが少なくても低層が学んだ共通特徴を活用できるので、一つのタスクをするNNを複数訓練するより性能が良くなることがある。&lt;/p&gt;

&lt;p&gt;ラベルが歯抜けでも学習できる。&lt;/p&gt;

&lt;p&gt;複数タスクをこなすため、大き目なネットワークにする必要がある。&lt;/p&gt;

&lt;p&gt;それぞれのクラスのデータが十分あるときはあまり使われない手法。
目的のクラスのデータが少ないとき、転移学習のほうがよく使われる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;End-to-end学習&lt;/p&gt;

&lt;p&gt;従来、ある入力から解となる出力を得るのに、パイプラインで複数の処理をしていたが、これを全部一つのNNで処理する手法。
データ量が多いと上手くいく。&lt;/p&gt;

&lt;p&gt;例えば、人の認識をするときパイプラインでは、顔検出して、拡大してクロップしてからNNにかける。
こっちのほうが個々のタスクがシンプルで、それぞれのデータも手に入りやすいので性能を出しやすい。
けどもし、end-to-endのラベル付きデータが十分にあればend-to-end学習でもいける。&lt;/p&gt;

&lt;p&gt;翻訳タスクの場合、現実的に大量のデータが手に入るので、end-to-endで上手くいく。&lt;/p&gt;

&lt;p&gt;データさえたくさんあれば、パイプラインのコンポーネント的なところも勝手に学んでくれるので、ヒトが考え付くよりもいい特徴を学んでくれるかもしれない。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Ruslan Salakhutdinovへのインタビュー&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    
      
        <item>
          <title>CourseraのDeep Learning SpecializationのImproving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコースを修了した</title>
          <link>https://www.kaitoy.xyz/2018/01/12/coursera-deep-learning-improving-deep-neural-networks/</link>
          <pubDate>Fri, 12 Jan 2018 23:41:57 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2018/01/12/coursera-deep-learning-improving-deep-neural-networks/</guid>
          <description>&lt;p&gt;&lt;a href=&#34;https://www.kaitoy.xyz/2018/01/05/coursera-deep-learning-neural-networks-and-deep-learning/&#34;&gt;CourseraのDeep Learning SpecializationのNeural Networks and Deep Learningコースを修了した&lt;/a&gt;のに続き、&lt;a href=&#34;https://www.coursera.org/learn/deep-neural-network&#34;&gt;Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimizationコース&lt;/a&gt;を修了した。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;このコースは、ディープニューラルネットワークのチューニングなどについて学べる3週間のコース。
今のところ全部英語。&lt;/p&gt;

&lt;p&gt;2018/1/5に始めて、1/12に完了。
8日間かかった。
修了したらまた&lt;a href=&#34;https://www.coursera.org/account/accomplishments/certificate/5VS9EJJ6TJ3A&#34;&gt;Certifacate&lt;/a&gt;もらえた。&lt;/p&gt;

&lt;p&gt;以下、3週分の内容をメモ程度に書いておく。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1週目&lt;/p&gt;

&lt;p&gt;OverfittingやUnderfittingを防ぐテクニックについて。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;データ分割&lt;/p&gt;

&lt;p&gt;深層学習のモデル構築は、検討(Idea)、実装(Code)、検証(Experiment)というサイクルの繰り返し(Iterative process)。
取り組む課題、課題のドメイン、データ量、マシンの構成などにより、ハイパーパラメータは変わるので、経験をもって事前に予測することは無理なので、サイクルをどれだけ速く回せるかが鍵。&lt;/p&gt;

&lt;p&gt;データは、訓練データ(Training set)、Devデータ(Development set))(a.k.a. Cross-validation set)、テストデータ(Test set)に分割する。
訓練データで学習し、Devデータでハイパーパラメータを評価し、テストデータで最終的な評価と性能見積をする。
テストデータは無くてもいい。&lt;/p&gt;

&lt;p&gt;サンプル数が1万くらいだった時代は、6:2:2くらいで分割してたけど、近年は数百万とかのデータを扱い、交差検証データやテストデータの割合はもっと小さくするのがトレンド。
98:1:1など。&lt;/p&gt;

&lt;p&gt;Devデータとテストデータは同じようなものを使うべき。
訓練データは必ずしも同様でなくてもいい。訓練データは沢山要るので、別のデータソースからとることもある。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;バイアス vs バリアンス&lt;/p&gt;

&lt;p&gt;でかいネットワークで正則化して大量データで学習させるのが吉。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;正則化&lt;/p&gt;

&lt;p&gt;過学習(Overfitting)を防ぐため、コスト関数を正則化(Regularization)すべし。&lt;/p&gt;

&lt;p&gt;ロジスティック回帰ではL2ノルム(L2 norm)を使ったL2正則化が一般的。
L1正則化はあまり使われない。
L1正則化をすると、wがスパース行列になってモデルを圧縮できると言われることがあるが、経験上その効果はほとんどない。&lt;/p&gt;

&lt;p&gt;正則化パラメータλはハイパーパラメータで、Devデータで評価する。&lt;/p&gt;

&lt;p&gt;ニューラルネットワークでは正則化項にフロベニウスノルム(Frobenius norm)を使う。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Dropout(Inverted Dropout)&lt;/p&gt;

&lt;p&gt;ランダムにノードを無効化しながら学習することで過学習を防ぐ。
画像処理の分野では、特徴量の数が多く学習データが少ない傾向があるので、ほぼ常に使われる。&lt;/p&gt;

&lt;p&gt;コスト関数が計算できなくなるのが欠点。
計算する必要があるときにはDropoutを無効化する。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;データ拡張(Data augmentation)&lt;/p&gt;

&lt;p&gt;データを加工して増やせば、高バリアンス対策になる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;早期終了(Early stopping)&lt;/p&gt;

&lt;p&gt;過学習するまえに学習を止めるテクニック。
訓練データとDevデータについてコストをプロットして、Devデータのものが上がる前に止める。&lt;/p&gt;

&lt;p&gt;これは、直交化(Orthogonalization)の原則、つまり一度に一つのことを考慮すべきという原則に反していて、コストを最小化するという問題と、過学習を避けるという問題に同時に対処することになるので微妙。&lt;/p&gt;

&lt;p&gt;普通は代わりにL2正則化使えばいいけど、λを最適化する手間を省きたいときには選択肢になりうる、というか実現場ではちょくちょく選択肢になる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;訓練データの正規化(Normalization)&lt;/p&gt;

&lt;p&gt;訓練データの各特徴量について平均を0にして分散を1にすると学習が速くなる。&lt;/p&gt;

&lt;p&gt;訓練データを正規化したらテストデータも正規化する。
その際、正規化パラメータは訓練データのものを使う。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;勾配消失(Vanishing gradient)、勾配爆発(Exploding gradient)&lt;/p&gt;

&lt;p&gt;ニューラルネットワークの層が深くなると、層の出力や勾配が指数関数的に大きくなったり小さくなったりして、学習が難しくなる問題。
長年ディープニューラルネットワークの発展を妨げてきた問題。&lt;/p&gt;

&lt;p&gt;パラメータのランダム初期化をすると防げる。
ガウス分布で作ったパラメータに特定の値を掛けてを分散が1/n(ReLUの時は2/n)になるように調整して、活性化関数に入力する値を約1に抑える。
掛ける値は活性化関数ごとにだいたい決まっていて((e.g. Xavier Initialization)、その値をハイパーパラメータとして調整するのはそれほど優先度は高くない。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gradient checking&lt;/p&gt;

&lt;p&gt;順伝播・逆伝播が正確に実装できているかを、数値計算手法で概算した勾配と逆伝播で出した勾配を比べてチェックするテクニック。
計算コストが高くなるので、デバッグ時にのみ使う。&lt;/p&gt;

&lt;p&gt;Dropoutしてるときには使えない。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;初期化&lt;/p&gt;

&lt;p&gt;ゼロ初期化、大きい値でのランダム初期化、He初期化(Xavier初期化っぽいやつ)を実装して性能を比べる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;正則化&lt;/p&gt;

&lt;p&gt;正則化無し、L2正則化、Dropoutの実装と比較。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gradient checking&lt;/p&gt;

&lt;p&gt;Gradient checkingの実装と、その結果を利用した逆伝播のデバッグ。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Yoshua Bengioへのインタビュー&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2週目&lt;/p&gt;

&lt;p&gt;学習を速くするテクニックについて。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ミニバッチ勾配降下法(Mini-batch gradient descent)&lt;/p&gt;

&lt;p&gt;普通の勾配降下法(i.e. バッチ勾配降下法)よりかなり速いので、大規模データでよく使われるテクニック。
学習回数に対するコストのプロットはノイジーになる。&lt;/p&gt;

&lt;p&gt;ミニバッチサイズというハイパーパラメータが増える。
ミニバッチサイズがmならバッチ勾配降下法、1なら確率的勾配降下法(Stochastic gradient descent)になる。&lt;/p&gt;

&lt;p&gt;ミニバッチ勾配降下法と確率的勾配降下法は収束しない。&lt;/p&gt;

&lt;p&gt;バッチ勾配降下法は遅すぎる。
確率的勾配降下法はベクトル化の恩恵がなくなるという欠点がある。
ので、適当なミニバッチサイズにするのがよく、それが一番速い。&lt;/p&gt;

&lt;p&gt;2000個くらいのデータならバッチ勾配降下法。
それより多ければ、64～512位のミニバッチサイズがいい。
メモリ効率を考えると2の累乗数がいいし、CPU/GPUメモリサイズに乗るサイズにすべし。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;指数加重移動平均 (EWMA: Exponentially Weighted Moving Average)&lt;/p&gt;

&lt;p&gt;ノイズのあるデータから、よりスムーズなプロットを書く手法。
過去数日の平均をもとにプロットする。&lt;/p&gt;

&lt;p&gt;この手法だと、最初の方のデータが不当に小さくなってしまう。
これが問題になるなら、バイアス補正(Bias correction)をかける。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;モーメンタム(Momentum)付き勾配降下法&lt;/p&gt;

&lt;p&gt;パラメータを更新するときに、勾配そのままではなく、勾配の指数加重移動平均を使う手法。
勾配降下を滑らかに速くできる。
慣性(勢い)をつけて走り抜ける感じ。&lt;/p&gt;

&lt;p&gt;指数加重移動平均を計算するときのβが新たなハイパーパラメータになる。
普通0.9。この場合バイアス補正はそんなに効果ないので普通かけない。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;RMSprop&lt;/p&gt;

&lt;p&gt;パラメータを更新するときに、勾配そのままではなく、勾配の二乗平均平方根(RMS: Root Mean Square)を使う手法。
学習率を上げつつ、勾配降下を滑らかに速くできる。&lt;/p&gt;

&lt;p&gt;二乗平均平方根を計算するときのβと、ゼロ除算を防ぐためのεが新たなハイパーパラメータになる。
提唱者は、&lt;code&gt;β=0.999&lt;/code&gt;、&lt;code&gt;ε=10^-8&lt;/code&gt;を推奨しているし、これらをチューニングすることはあまりない。&lt;/p&gt;

&lt;p&gt;Couseraが広めたことで、よく使われるようになった。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Adam(Adaptive Moment Estimation)&lt;/p&gt;

&lt;p&gt;モーメンタムとRMSpropとバイアス補正を組み合わせた最適化アルゴリズム。
これをミニバッチ勾配降下法と一緒に使えばだいたい上手くいく。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;学習率減衰(Learning rate decay)&lt;/p&gt;

&lt;p&gt;ミニバッチ勾配降下を収束させるために、学習率を徐々に小さくする手法。
エポックごとに学習率を下げる。&lt;/p&gt;

&lt;p&gt;学習率αが、α0と 減衰率(Decay rate)とエポック番号から計算されるようになるので、α0と減衰率がハイパーパラメータ。&lt;/p&gt;

&lt;p&gt;学習率の計算方法にはいくつかある。
指数関数的に下げたり、階段状に下げたり。&lt;/p&gt;

&lt;p&gt;Andrew先生はあまり使わない手法。
学習率をよくチューニングすれば十分。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;局所最適解(Local optima)&lt;/p&gt;

&lt;p&gt;かつて、勾配が0になる点は、コストの谷、つまり局所最適解だと考えられていて、そこに嵌ることが問題だった。&lt;/p&gt;

&lt;p&gt;けど、ディープニューラルネットワークでは多くは尾根的なもの。
鞍の上みたいな部分なので鞍点(Saddle point)と呼ばれる。
特徴量が沢山あるので、ちょっと動かすとどれかの勾配は負になる。&lt;/p&gt;

&lt;p&gt;よって局所最適解はあまり恐れなくていい。
代わりに、鞍点の台地みたいな部分では勾配が小さいので学習効率が悪くなる。
ここを勢いよく抜けたいので、モーメンタムやRMSpropやAdamが有効になる。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ミニバッチ勾配降下法実装&lt;/p&gt;

&lt;p&gt;訓練データをシャッフルして、ミニバッチサイズに分割して(余りは余りでミニバッチにする)、forループで回す。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;モーメンタムとAdam実装&lt;/p&gt;

&lt;p&gt;単なるミニバッチ勾配降下法とモーメンタム付きとAdamを比較。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Yuanqing Linへのインタビュー&lt;/p&gt;

&lt;p&gt;中国の国立深層学習研究所のトップ。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2週目&lt;/p&gt;

&lt;p&gt;ハイパーパラメータのチューニング方法、バッチ正規化、ソフトマックス回帰、TensorFlow。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;動画&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ハイパーパラメータのチューニング&lt;/p&gt;

&lt;p&gt;一番重要なのは学習率。
次はモーメンタムのβとかミニバッチサイズとか隠れ層のノード数。
その次がレイヤ数とか学習率減衰率。&lt;/p&gt;

&lt;p&gt;チューニングの際は、かつてはグリッドサーチ(Grid search)がよく使われたけど、これはハイパーパラメータが多くなるとつらい。
ランダムサーチ(Randomized search)がより効率的。&lt;/p&gt;

&lt;p&gt;グリッドサーチだとあるパラメータを固定して別のパラメータを変化させるけど、変化させたパラメータがどうでもいいものだった場合、その試行がほとんど無駄になるので。&lt;/p&gt;

&lt;p&gt;粗くランダムサーチして当たりをつけ、範囲を絞って細かいランダムサーチする。&lt;/p&gt;

&lt;p&gt;ランダムといってもいろいろあって、ユニット数なんかは一様にランダムでいいけど、学習率なんかはlogスケールの上でランダムにしたほうがいい。&lt;/p&gt;

&lt;p&gt;実運用では、計算リソースが少ない場合に採るパンダアプローチと、潤沢なリソースで複数のモデルを同時に訓練するキャビアアプローチがある。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;バッチ正規化(Batch normalization)&lt;/p&gt;

&lt;p&gt;深層学習の実用化において最も重要なアルゴリズムの一つ。
ハイパーパラメータの選定を簡単にして、ディープニューラルネットワークの訓練を簡単にする。&lt;/p&gt;

&lt;p&gt;バッチ正規化では、各層の入力を正規化する。
ミニバッチごとに平均を0、分散を1に正規化した後、βとγというパラメータでそれぞれを調整する。
aよりz(i.e. 活性化関数適用前)を正規化するのが普通。&lt;/p&gt;

&lt;p&gt;(ハイパーではない)パラメータとしてβとγが層ごとに増える。
これらもWとともに学習する。
βがbの役割をするので、bはいらなくなる。&lt;/p&gt;

&lt;p&gt;バッチ正規化は共変量シフト(Covariate shift)という問題に対応するもの。
共変量シフトは、訓練した後で入力の分散が変わると、また訓練しなおさないといけないという問題。
ニューラルネットワークの内部では、前のほうの層のWが学習を進めるたびに変わり、その層の出力が変わる。
つまり後のほうの層への入力が変わるので、後のほうの層の学習が進みにくい。
バッチ正規化は、この後のほうの層への入力の分散を一定範囲に抑えることで、後のほうの層の学習を効率化する。&lt;/p&gt;

&lt;p&gt;Dropoutと同様な論理(ノードへの依存が分散される)で正則化の効果もややある。&lt;/p&gt;

&lt;p&gt;訓練が終わったら、最後のミニバッチの平均μと分散σ^2を保存しておいて、予測時に使う。
μとσ^2は訓練データ全体から再計算してもよさそうだけど、普通はやらない。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ソフトマックス回帰(Softmax regression)&lt;/p&gt;

&lt;p&gt;ニューラルネットワークで多値分類(Multi-class classification)するアルゴリズム。
出力層(ソフトマックス層)のノード数をクラス数にして、活性化関数にソフトマックス関数を使う。
出力層の各ノードは、サンプルが各クラスに属する確率を出力する。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;TensorFlow&lt;/p&gt;

&lt;p&gt;ディープラーニングフレームワークはいろいろある: Caffe/Caffe2、CNTK、DL2J、Keras、Lasagne、mxnet、PaddlePaddle、TensorFlow、Theano、Torch。
プログラミングしやすいこと、訓練性能がいいこと、オープンであることが重要。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;TensorFlowの基本&lt;/p&gt;

&lt;p&gt;TensorFlowでのプログラムはだいたい以下のような手順で書く。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;テンソル(tensor)をつくる。これはまだ評価されない。&lt;/li&gt;
&lt;li&gt;テンソル間の計算式(計算グラフ)を書く。&lt;/li&gt;
&lt;li&gt;テンソルを初期化する。&lt;/li&gt;
&lt;li&gt;セッションを作る。&lt;/li&gt;
&lt;li&gt;セッションを実行する。ここで計算が実行される。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;後で(セッション実行時に)値を入れたい場合はプレースホルダ(placeholder)を使う。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;TensorFlowでのニューラルネットワーク実装&lt;/p&gt;

&lt;p&gt;画像を読み込んで多クラス分類するNNを作る。
以下、今回使った関数の一部。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;シグモイド関数: &lt;code&gt;tf.sigmoid(x)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;エントロピーコスト: &lt;code&gt;tf.nn.sigmoid_cross_entropy_with_logits(logits, labels)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;One-hotエンコーディング: &lt;code&gt;tf.one_hot(labels, depth, axis)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;最急降下法: &lt;code&gt;tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    
      
        <item>
          <title>CourseraのDeep Learning SpecializationのNeural Networks and Deep Learningコースを修了した</title>
          <link>https://www.kaitoy.xyz/2018/01/05/coursera-deep-learning-neural-networks-and-deep-learning/</link>
          <pubDate>Fri, 05 Jan 2018 15:20:23 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2018/01/05/coursera-deep-learning-neural-networks-and-deep-learning/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;https://www.kaitoy.xyz/2017/12/22/coursera-machine-learning/&#34;&gt;CourseraのMachine Learningコース&lt;/a&gt;に続いて、同じくAndrew先生による&lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;&gt;Deep Learning Specialization&lt;/a&gt;を受講中。&lt;/p&gt;

&lt;p&gt;これは深層学習の基本を学べるもので、以下の5つのコースからなる。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Neural Networks and Deep Learning&lt;/li&gt;
&lt;li&gt;Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization&lt;/li&gt;
&lt;li&gt;Structuring Machine Learning Projects&lt;/li&gt;
&lt;li&gt;Convolutional Neural Networks&lt;/li&gt;
&lt;li&gt;Sequence Models&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;この内、最初のNeural Networks and Deep Learningを修了したので、記念にブログしておく。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h1 id=&#34;deep-learning-specializationとは&#34;&gt;Deep Learning Specializationとは&lt;/h1&gt;

&lt;p&gt;Deep Learning Specializationは&lt;a href=&#34;https://learner.coursera.help/hc/en-us/articles/208280296&#34;&gt;Coursera Specialization&lt;/a&gt;のひとつ。
Coursera Specializationはサブスクリプションモデルで、つまりあるSpecializationのサブスクリプションを購入すると、受講完了するまで毎月定額の料金を支払うことになる。&lt;/p&gt;

&lt;p&gt;Deep Learning Specializationは月$49で、5コース合わせて16週分の内容。
最初の7日間はトライアルで無料なので、この間に全部終わらせられればタダ。
無理だけど。&lt;/p&gt;

&lt;p&gt;Deep Learning Specializationでは、PythonとTensorFlowでディープニューラルネットワーク、CNN、RNN、LSTM、Adam、Dropout、バッチ正規化、Xavier/He initializationなどを学べる。
Machine Learningコースと同じく、5分～15分くらいの動画による講義と、小テストと、プログラミング課題から構成されている。&lt;/p&gt;

&lt;p&gt;プログラミング課題は、coursera hubという、ホステッドJupyter Notebookで解いて提出できるので楽。&lt;/p&gt;

&lt;h1 id=&#34;neural-networks-and-deep-learningコースとは&#34;&gt;Neural Networks and Deep Learningコースとは&lt;/h1&gt;

&lt;p&gt;ディープニューラルネットワークの仕組みを学んで実装する4週間のコース。
また、深層学習の偉い人へのインタビューを見れる。
Machine Learningコースと被っている内容が少なくなく、かなり楽だったが、結構ペースが速いので、Machine Learningコースをやっていなかったら辛かったと思う。&lt;/p&gt;

&lt;p&gt;動画は大抵日本語字幕が付いている。
日本語字幕が付いていない奴は、英語字幕が機械生成したっぽいもので見辛い。&lt;/p&gt;

&lt;p&gt;2018/1/1に始めて、1/5に完了。
5日間かかった。
修了したら&lt;a href=&#34;https://www.coursera.org/account/accomplishments/certificate/G77XMU9TNEKX&#34;&gt;Certifacate&lt;/a&gt;もらえた。&lt;/p&gt;

&lt;p&gt;以下、4週分の内容をキーワードレベルで書いておく。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1週目&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;深層学習(Deep Learning)概要&lt;/p&gt;

&lt;p&gt;AIは次世代の電気。産業革命を起こす。
AIで今一番熱い分野が深層学習。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ニューラルネットワーク(Neural Network)。&lt;/li&gt;
&lt;li&gt;畳み込みニューラルネットワーク(CNN: Convolutional Neural Network)。&lt;/li&gt;
&lt;li&gt;再帰型ニューラルネットワーク(RNN: Recurrent Neural Network)。&lt;/li&gt;
&lt;li&gt;深層学習の適用分野・例。&lt;/li&gt;
&lt;li&gt;深層学習が実用化した背景。&lt;/li&gt;
&lt;li&gt;シグモイド関数(Sigmoid function) vs ReLU(Rectified Linear Unit)関数。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;深層学習のゴッドファーザー、Geoffrey Hintonへのインタビュー。&lt;/p&gt;

&lt;p&gt;ニューラルネットワークの黎明期を支え、ReLU関数の有効性を証明したりボルツマンマシンを発明したりした人。
自身が歩んできた深層学習の歴史や今取り組んでいる・注目している理論について、
高尚な話をしていたようだったが、高尚すぎてよくわからなかった。&lt;/p&gt;

&lt;p&gt;今日成果を出しているのは教師あり学習だけど、教師無し学習のほうが重要と考えているとのこと。&lt;/p&gt;

&lt;p&gt;これから機械学習に取り組む人へ、以下のアドバイスをしていた。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;文献を読みすぎるな。&lt;/li&gt;
&lt;li&gt;文献を読んで、間違っていると感じるところをみつけて、それに取り組め。&lt;/li&gt;
&lt;li&gt;人から何を言われても気にせず、自分の信念に従って研究しろ。&lt;/li&gt;
&lt;li&gt;誰かに無意味なことをしていると指摘されたら、むしろ価値のあることをしていると思え。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2週目&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ニューラルネットワークプログラミングの基礎&lt;/p&gt;

&lt;p&gt;ロジスティック回帰は小さい(1層1ノード)ニューラルネットワーク。
ロジスティック回帰の微分を逆伝播で計算する。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;二値分類(Binary classification)、ロジスティック回帰(Logistic regression)。&lt;/li&gt;
&lt;li&gt;損失関数(Loss function)、交差エントロピー(Cross entropy)、目的関数(Cost function)。&lt;/li&gt;
&lt;li&gt;最急降下法(Gradient descent)。&lt;/li&gt;
&lt;li&gt;微分(Derivatives)。&lt;/li&gt;
&lt;li&gt;逆伝播(Backpropagation)、計算グラフ(Computation graph)、連鎖律(Chain rule)。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Pythonとベクトル化(Vectorization)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;forループ vs ベクトル化。&lt;/li&gt;
&lt;li&gt;Jupyter Notebook、NumPy、ブロードキャスト(Broadcasting)。&lt;/li&gt;
&lt;li&gt;ロジスティック回帰のベクトル化。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ロジスティック回帰で猫の画像の判別。&lt;/li&gt;
&lt;li&gt;NumPy、h5py、Matplotlib、PIL、SciPy。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;深層学習とロボットの研究者、Pieter Abbeelへのインタビュー&lt;/p&gt;

&lt;p&gt;深層強化学習の有名な研究者。DQN。&lt;/p&gt;

&lt;p&gt;かつて、機械学習で成果を出すためには、取り組んでいる課題特有の分野の知識が必要だったが、2012年にGeoffreyが発表したAlexNetがそれを覆した。
Pieterはそのアイデアを深層強化学習に適用し発展させた。&lt;/p&gt;

&lt;p&gt;強化学習は、どこからデータ収集するのか、報酬の分配はどうするかといったところに課題がある。
また、安全性にも課題。学習の過程で失敗を繰り返すので、自動運転などをどう学習させるか、またどう学び続けさせるか。
ネガティブデータを集めるのがむずい。&lt;/p&gt;

&lt;p&gt;短時間の実験でうまくいっても、長時間の稼働でうまくいくとも限らない。&lt;/p&gt;

&lt;p&gt;強化学習は複雑すぎるので、アルゴリズム自体を学習できるようにしたい。
プログラムを自動で変更しながら学習するなど。&lt;/p&gt;

&lt;p&gt;強化学習は、アルゴリズムを変更しなくても様々なことを学べる。
けど、ゼロから学ぶと時間がかかるので、以前学んだことを活かして次の課題に取り組めるようにするのが最先端の研究。&lt;/p&gt;

&lt;p&gt;まずは教師あり学習で人の代わりができるようになり、その後目的を与えて、強化学習で改善していく、っていう感じのことができるとうれしい。&lt;/p&gt;

&lt;p&gt;これから機械学習に取り組む人へ、以下のアドバイスをしていた。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;需要が高まっているので、AIを始めるにはよい時期。&lt;/li&gt;
&lt;li&gt;オンライン講座がたくさんあるので、学びやすい。自分で試したり再現したりしてみることが重要。&lt;/li&gt;
&lt;li&gt;TensorFlow、Chainer、Theano、PyTorchなど、手軽に試せるツールが色々ある。&lt;/li&gt;
&lt;li&gt;専門的な教育を受けなくても、自己学習でトップクラスになれる。&lt;/li&gt;
&lt;li&gt;機械学習を学ぶのに、大学で研究すべきか大企業で仕事を得るべきかについては、どれくらいの指導を受けれるかによる。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;3週目&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;浅いニューラルネットワーク&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ロジスティック回帰を多重にして2層のニューラルネットワーク化。&lt;/li&gt;
&lt;li&gt;ニューラルネットワークアルゴリズムのベクトル化。&lt;/li&gt;
&lt;li&gt;活性化関数(Activation function)の選択: シグモイド vs tanh vs ReLU vs Leaky ReLU vs 線形活性化関数(Linear activation function)。&lt;/li&gt;
&lt;li&gt;順伝播(Forward propagation)、逆伝播(Backpropagation)。&lt;/li&gt;
&lt;li&gt;ランダム初期化(Random initialization)。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;二値分類するニューラルネットワークを実装。&lt;/li&gt;
&lt;li&gt;scikit-learn。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;GAN(Generative Adversarial Network)の発明者、Ian Goodfellowへのインタビュー&lt;/p&gt;

&lt;p&gt;GANは生成モデル(学習したデータに似たデータを生成するモデル)。
バーで飲んでいるときに思いつき、一晩で実装した。&lt;/p&gt;

&lt;p&gt;GANは今は繊細過ぎるのが課題で、安定性の向上に今取り組んでいる。&lt;/p&gt;

&lt;p&gt;機械学習のセキュリティにも興味がある。モデルをだまして想定外の動作をさせるような攻撃への対策など。&lt;/p&gt;

&lt;p&gt;これから機械学習に取り組む人へ、以下のアドバイスをしていた。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;機械学習のアルゴリズムよりも、線形代数や確率といった数学の基礎を習得することが重要。&lt;/li&gt;
&lt;li&gt;AIの道を進むのに、近年では博士号は必ずしも要らない。コードを書いてGitHubに上げろ。自身も実際に、オープンソース活動をしているひとの貢献を見て興味をもって採用したことがある。&lt;/li&gt;
&lt;li&gt;論文を公開するとよりいいけど、コードのほうが楽。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;4週目&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;深いニューラルネットワーク&lt;/p&gt;

&lt;p&gt;3層以上のニューラルネットワーク。その実装方法と有効性について。
ハイパーパラメータ(Hyperparameters): 学習率(Learning rate)、学習回数、レイヤ数、ノード数、活性化関数、等。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;プログラミング課題&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ディープニューラルネットワークの実装。&lt;/li&gt;
&lt;li&gt;再度猫画像の判別。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    

  </channel>
</rss>

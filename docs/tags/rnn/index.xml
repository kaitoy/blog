<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>rnn on To Be Decided</title>
    <link>https://www.kaitoy.xyz/tags/rnn/</link>
    <description>Recent content in rnn on To Be Decided</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© 2015 Kaito Yamada</copyright>
    <lastBuildDate>Tue, 27 Feb 2018 00:49:05 +0900</lastBuildDate>
    
	<atom:link href="https://www.kaitoy.xyz/tags/rnn/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>CourseraのDeep Learning SpecializationのSequence Modelsコースを修了した</title>
      <link>https://www.kaitoy.xyz/2018/02/27/coursera-deep-learning-sequence-models/</link>
      <pubDate>Tue, 27 Feb 2018 00:49:05 +0900</pubDate>
      
      <guid>https://www.kaitoy.xyz/2018/02/27/coursera-deep-learning-sequence-models/</guid>
      <description>CourseraのDeep Learning SpecializationのConvolutional Neural Networksコースを修了したのに続き、Sequence Modelsコースを修了した。
   (adsbygoogle = window.adsbygoogle || []).push({});  このコースは、RNNの原理、代表的なアーキテクチャ、自然言語処理などについて学べる3週間のコース。 生成モデルが色々出てきて面白い。 動画は今のところ全部英語。
2018/2/6に始めて、2/27に完了。 22日間かかった。 修了したらまたCertifacateもらえた。
また、これでDeep Learning Specializationのすべてのコースを修了したので、全部まとめたCertifacateももらえた。 結局2ヶ月ほどかかり、1万円以上課金してしまった…
以下、3週分の内容をメモ程度に書いておく。
 1週目
連続データを扱うシーケンス(Sequence)モデルについて学ぶ。 RNN、LSTM、GRU、BRNN。
 動画
 再帰型ニューラルネットワーク(Recurrent Neural Network)
シーケンスモデルにはRNNなどがあって、音声認識(Speech recognition)や自然言語処理(Natural language processing)に使われる。 音楽生成(Music generation)、感情分類(Sentiment classification)、DNA解析(DNA sequence analysis)、動画行動認識(Video Activity Recognition)、固有表現抽出(Named entity recognition)なんてのも。
入力だけが連続データだったり、出力だけが連続データだったり、両方だったり。
自然言語処理では、ボキャブラリ(Vocabulary)を使って単語をone hotベクトルにして扱う。 ボキャブラリは普通5万次元くらいのベクトル。 ボキャブラリにない単語はそれ用(unknown)の次元に割り当てる。
入力や出力の次元がサンプルごとに違うので、普通のNNは使えない。 また、普通のNNだと、文のある個所から学んだ特徴を他の箇所と共有しない。 また、普通のNNだと、入力サイズが大きすぎて、パラメータが多くなりすぎる。 RNNはこうした問題を持たない。
RNNは、最初の単語xを受け取り、層で計算し、最初の出力yとアクティベーションaを出し、そのaと次のxを同じ層で受け取り、次のyとaをだす、ということを繰り返す。 xにかける重みをWax、aにかける重みをWaa、yにかける重みをWyaと呼ぶ。 あとaとyを計算するときに足すバイアスがあって、それぞれba、by。 あるxの計算をするときに、その前のxも使うので、連続データ処理に向いてる。 けど、後のxを考慮しないところが欠点。 この欠点に対処したのがBRNN(Bidirectional RNN)。
RNNのaの活性化関数にはtanhがよく使われる。 ReLUもあり。 yには二値分類だったらシグモイドだし、そうでなければソフトマックスとか。
損失関数は普通に交差エントロピーでいいけど、yがベクトルなので、その各要素について交差エントロピーを計算して、足し合わせたものが損失になる。 ここから逆伝播するんだけど、その際に連続データを過去にさかのぼるので、時をかける逆伝播(BPTT: Backpropagation through time)と呼ばれる。</description>
    </item>
    
  </channel>
</rss>
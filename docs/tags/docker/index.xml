<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>To Be Decided </title>
    <link>https://www.kaitoy.xyz/tags/docker/</link>
    <language>en-us</language>
    <author>Kaito Yamada</author>
    <rights>(C) 2018</rights>
    <updated>2018-05-05 21:54:30 &#43;0900 JST</updated>

    
      
        <item>
          <title>Kubernetes 1.10のkubeletの起動オプションをKubelet ConfigファイルとPodSecurityPolicyで置き換える</title>
          <link>https://www.kaitoy.xyz/2018/05/05/kubernetes-kubelet-config-and-pod-sec-policy/</link>
          <pubDate>Sat, 05 May 2018 21:54:30 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2018/05/05/kubernetes-kubelet-config-and-pod-sec-policy/</guid>
          <description>

&lt;p&gt;「&lt;a href=&#34;https://www.kaitoy.xyz/2018/04/17/kubernetes110-from-scratch/&#34;&gt;Kubernetes 1.10をスクラッチから全手動で構築&lt;/a&gt;」、「&lt;a href=&#34;https://www.kaitoy.xyz/2018/05/04/kubernetes-with-weave-net/&#34;&gt;Kubernetes 1.10のクラスタにWeave Netをデプロイする&lt;/a&gt;」の続き。&lt;/p&gt;

&lt;p&gt;kubeletの起動オプションの代わりに、Kubelet ConfigファイルとPodSecurityPolicyを使うように変更した話。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h1 id=&#34;kubelet-configファイル&#34;&gt;Kubelet Configファイル&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;journalctl -u kubelet&lt;/code&gt;すると、以下の警告が出ている。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Apr 28 15:31:39 k8s-master kubelet[1370]: Flag --address has been deprecated, This parameter should be set via the config file specified by the Kubelet&#39;s -
-config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 28 15:31:40 k8s-master kubelet[1370]: Flag --pod-manifest-path has been deprecated, This parameter should be set via the config file specified by the K
ubelet&#39;s --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 28 15:31:40 k8s-master kubelet[1370]: Flag --cluster-dns has been deprecated, This parameter should be set via the config file specified by the Kubelet
&#39;s --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 28 15:31:40 k8s-master kubelet[1370]: Flag --cluster-domain has been deprecated, This parameter should be set via the config file specified by the Kube
let&#39;s --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 28 15:31:40 k8s-master kubelet[1370]: Flag --authorization-mode has been deprecated, This parameter should be set via the config file specified by the
Kubelet&#39;s --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 28 15:31:40 k8s-master kubelet[1370]: Flag --client-ca-file has been deprecated, This parameter should be set via the config file specified by the Kube
let&#39;s --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 28 15:31:40 k8s-master kubelet[1370]: Flag --cgroup-driver has been deprecated, This parameter should be set via the config file specified by the Kubel
et&#39;s --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 28 15:31:40 k8s-master kubelet[1370]: Flag --tls-min-version has been deprecated, This parameter should be set via the config file specified by the Kubelet&#39;s --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Apr 28 15:31:40 k8s-master kubelet[1370]: Flag --allow-privileged has been deprecated, will be removed in a future version
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kubeletのいくつかのオプションは、&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/&#34;&gt;https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/&lt;/a&gt; を参照してKubelet Configファイルのほうに書けとある。&lt;/p&gt;

&lt;p&gt;参照先のマニュアルには現時点でほぼ何も書いてないし、ググっても情報が無いので、&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/kubelet/apis/kubeletconfig/v1beta1/types.go&#34;&gt;ソース&lt;/a&gt;を見てそれっぽく書いてみた。&lt;/p&gt;

&lt;p&gt;将来的に調整しそうなパラメータは、Kubelet Configファイルにデフォルト値とともにコメントとして書き出している。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# DNS_SERVER_IP=10.0.0.10
# DNS_DOMAIN=&amp;quot;cluster.local&amp;quot;
# mkdir -p /etc/kubernetes/manifests
# cat &amp;gt; /etc/kubernetes/kubelet.conf &amp;lt;&amp;lt; EOF
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
featureGates:
  RotateKubeletServerCertificate: true
address: &amp;quot;0.0.0.0&amp;quot;
staticPodPath: &amp;quot;/etc/kubernetes/manifests&amp;quot;
clusterDNS: [&amp;quot;${DNS_SERVER_IP}&amp;quot;]
clusterDomain: &amp;quot;${DNS_DOMAIN}&amp;quot;
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: &amp;quot;5m0s&amp;quot;
    cacheUnauthorizedTTL: &amp;quot;30s&amp;quot;
authentication:
  x509:
    clientCAFile: &amp;quot;/etc/kubernetes/pki/ca.crt&amp;quot;
  webhook:
    enabled: false
    cacheTTL: &amp;quot;0s&amp;quot;
  anonymous:
    enabled: false
cgroupDriver: &amp;quot;cgroupfs&amp;quot;
tlsMinVersion: &amp;quot;VersionTLS12&amp;quot;
tlsCipherSuites:
- &amp;quot;TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256&amp;quot;
- &amp;quot;TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384&amp;quot;
- &amp;quot;TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256&amp;quot;
- &amp;quot;TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384&amp;quot;
readOnlyPort: 0
# port: 10250
# containerLogMaxSize: &amp;quot;10Mi&amp;quot;
# containerLogMaxFiles: 5
# evictionHard:
#   imagefs.available: &amp;quot;15%&amp;quot;
#   memory.available: &amp;quot;100Mi&amp;quot;
#   nodefs.available: &amp;quot;10%&amp;quot;
#   nodefs.inodesFree: &amp;quot;5%&amp;quot;
# evictionMaxPodGracePeriod: 0
# evictionPressureTransitionPeriod: &amp;quot;5m0s&amp;quot;
# fileCheckFrequency: &amp;quot;20s&amp;quot;
# imageGCHighThresholdPercent: 85
# imageGCLowThresholdPercent: 80
# maxOpenFiles: 1000000
# maxPods: 110
# imageMinimumGCAge: &amp;quot;2m0s&amp;quot;
# nodeStatusUpdateFrequency: &amp;quot;10s&amp;quot;
# runtimeRequestTimeout: &amp;quot;2m0s&amp;quot;
# streamingConnectionIdleTimeout: &amp;quot;4h0m0s&amp;quot;
# syncFrequency: &amp;quot;1m0s&amp;quot;
# volumeStatsAggPeriod: &amp;quot;1m0s&amp;quot;
EOF
# PAUSE_IMAGE=k8s.gcr.io/pause-amd64:3.1
# cat &amp;gt; /etc/systemd/system/kubelet.service &amp;lt;&amp;lt; EOF
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
ExecStart=/usr/bin/kubelet \\
  --allow-privileged=true \\
  --config=/etc/kubernetes/kubelet.conf \\
  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\
  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\
  --network-plugin=cni \\
  --cni-conf-dir=/etc/cni/net.d \\
  --cni-bin-dir=/opt/cni/bin \\
  --cert-dir=/etc/kubernetes/pki \\
  --rotate-certificates=true \\
  --v=2 \\
  --pod-infra-container-image=${PAUSE_IMAGE}
Restart=always
RestartSec=10s

[Install]
WantedBy=multi-user.target
EOF
# systemctl daemon-reload
# systemctl restart kubelet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--allow-privileged&lt;/code&gt;だけは、警告が出てるけどKubelet Configファイルで設定できない。&lt;/p&gt;

&lt;h2 id=&#34;podsecuritypolicy&#34;&gt;PodSecurityPolicy&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;--allow-privileged&lt;/code&gt;は非推奨。
どうも代わりに&lt;a href=&#34;https://kubernetes.io/docs/concepts/policy/pod-security-policy/&#34;&gt;PodSecurityPolicy&lt;/a&gt;で制御しろということのようだ。&lt;/p&gt;

&lt;p&gt;PodSecurityPolicyを使うにはまず、kube-apiserverの起動オプションの&lt;code&gt;--admission-control&lt;/code&gt;に&lt;code&gt;PodSecurityPolicy&lt;/code&gt;を追加する必要がある。&lt;/p&gt;

&lt;p&gt;で、privilegedななんでもできるPodSecurityPolicyと、それを使うロールを作成する。
因みにPodSecurityPolicyは名前空間に属さない。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# export KUBECONFIG=/etc/kubernetes/admin.kubeconfig
# kubectl create -f- &amp;lt;&amp;lt;EOF
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: privileged
spec:
  privileged: true
  hostIPC: true
  hostPID: true
  hostNetwork: true
  hostPorts:
  - min: 0
    max: 65535
  volumes:
  - &amp;quot;*&amp;quot;
  fsGroup:
    rule: &amp;quot;RunAsAny&amp;quot;
  runAsUser:
    rule: &amp;quot;RunAsAny&amp;quot;
  supplementalGroups:
    rule: &amp;quot;RunAsAny&amp;quot;
  allowPrivilegeEscalation: true
  allowedCapabilities:
  - &amp;quot;*&amp;quot;
  seLinux:
    rule: &amp;quot;RunAsAny&amp;quot;
EOF
# kubectl -n kube-system create role psp:privileged --verb=use --resource=podsecuritypolicy --resource-name=privileged
# kubectl -n weave create role psp:privileged --verb=use --resource=podsecuritypolicy --resource-name=privileged
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;今のところ、privilegedなPodSecurityPolicyが必要なService AccountはWeave Netのkube-system:weave-netと、Weave Scopeのweave:weave-scopeとweave:default。
こいつらに上記ロールをバインドする。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# kubectl -n kube-system create rolebinding weave-net:psp:privileged --role=psp:privileged --serviceaccount=kube-system:weave-net
# kubectl -n weave create rolebinding weave-scope:psp:privileged --role=psp:privileged --serviceaccount=weave:weave-scope
# kubectl -n weave create rolebinding weave-default:psp:privileged --role=psp:privileged --serviceaccount=weave:default
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;あと、CoreDNS用のPodSecurityPolicyとロールを作ってバインドする。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# export KUBECONFIG=/etc/kubernetes/admin.kubeconfig
# kubectl apply -f- &amp;lt;&amp;lt;EOF
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: coredns
spec:
  privileged: false
  hostIPC: false
  hostPID: false
  hostNetwork: false
  hostPorts:
  - min: 0
    max: 65535
  volumes:
  - &amp;quot;configMap&amp;quot;
  - &amp;quot;secret&amp;quot;
  fsGroup:
    rule: &amp;quot;RunAsAny&amp;quot;
  runAsUser:
    rule: &amp;quot;RunAsAny&amp;quot;
  supplementalGroups:
    rule: &amp;quot;RunAsAny&amp;quot;
  allowPrivilegeEscalation: true
  seLinux:
    rule: &amp;quot;RunAsAny&amp;quot;
EOF
# kubectl -n kube-system create role psp:coredns --verb=use --resource=podsecuritypolicy --resource-name=coredns
# kubectl -n kube-system create rolebinding coredns:psp:coredns --role=psp:coredns --serviceaccount=kube-system:coredns
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;これで晴れてkubeletから&lt;code&gt;--allow-privileged&lt;/code&gt;を外せる、と思ったら、外したら動かなかった。
どうも現時点では&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/58010&#34;&gt;kubeletとPodSecurityPolicyが連携できていない&lt;/a&gt;らしく、&lt;code&gt;--allow-privileged&lt;/code&gt;は付けとかないといけないようだ。
付けといても、PodSecurityPolicyでprivilegedをtrueにしないとprivilegedが許可されないので、動きとしては問題ない。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--allow-privileged&lt;/code&gt;はKubernetes 1.12で廃止される予定なので、それまでにはなんとかなるだろう。&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Kubernetes 1.10のクラスタにWeave Netをデプロイする</title>
          <link>https://www.kaitoy.xyz/2018/05/04/kubernetes-with-weave-net/</link>
          <pubDate>Fri, 04 May 2018 11:14:33 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2018/05/04/kubernetes-with-weave-net/</guid>
          <description>

&lt;p&gt;「&lt;a href=&#34;https://www.kaitoy.xyz/2018/04/17/kubernetes110-from-scratch/&#34;&gt;Kubernetes 1.10をスクラッチから全手動で構築&lt;/a&gt;」で、Kubernetes 1.10のクラスタに、ネットワークプロバイダとして&lt;a href=&#34;https://github.com/coreos/flannel&#34;&gt;flannel&lt;/a&gt;をデプロイしたけど、flannelは&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/network-policies/&#34;&gt;Network Policy&lt;/a&gt;をサポートしていないので、代わりに&lt;a href=&#34;https://www.weave.works/oss/net/&#34;&gt;Weave Net&lt;/a&gt;をデプロイしてみた話。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h1 id=&#34;weave-netにした理由&#34;&gt;Weave Netにした理由&lt;/h1&gt;

&lt;p&gt;Network Policyをサポートしているネットワークプロバイダには現時点で以下のものがある。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.projectcalico.org/&#34;&gt;Calico&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cilium/cilium&#34;&gt;Cilium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kube-router.io/&#34;&gt;Kube-router&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/romana/romana&#34;&gt;Romana&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Weave Net&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;このなかで、よく名前を聞くのがCalicoとWeave Net。
GitHubのスター数が圧倒的に多いのがWeave Net。
&lt;a href=&#34;https://engineering.skybettingandgaming.com/2017/02/03/overlay-network-performance-testing/&#34;&gt;性能が比較的いい&lt;/a&gt;のがWeave Net。&lt;/p&gt;

&lt;p&gt;ということでWeave Netにした。&lt;/p&gt;

&lt;h1 id=&#34;weave-netデプロイ&#34;&gt;Weave Netデプロイ&lt;/h1&gt;

&lt;p&gt;以下を参考に設定してデプロイする。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.weave.works/docs/net/latest/kubernetes/kube-addon/&#34;&gt;https://www.weave.works/docs/net/latest/kubernetes/kube-addon/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.weave.works/docs/net/latest/install/installing-weave/&#34;&gt;https://www.weave.works/docs/net/latest/install/installing-weave/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/weaveworks/weave/blob/master/prog/weave-kube/launch.sh&#34;&gt;https://github.com/weaveworks/weave/blob/master/prog/weave-kube/launch.sh&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;kubernetesマニフェスト&#34;&gt;Kubernetesマニフェスト&lt;/h2&gt;

&lt;p&gt;Weave NetをKubernetesクラスタにデプロイするためのマニフェストは、&lt;a href=&#34;https://github.com/weaveworks/weave/releases&#34;&gt;GitHub Releases&lt;/a&gt;か&lt;code&gt;https://cloud.weave.works&lt;/code&gt;からダウンロードできる。
今回は後者にする。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;https://cloud.weave.works&lt;/code&gt;を使う場合、Kubernetesのバージョンなどのパラメータは&lt;a href=&#34;https://www.weave.works/docs/net/latest/kubernetes/kube-addon/#-changing-configuration-options&#34;&gt;クエリストリングで指定できる&lt;/a&gt;。
主なパラメータは以下。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;k8s-version: Kubernetesのバージョン。指定しないとlatest。&lt;/li&gt;
&lt;li&gt;password-secret: ノード間の&lt;a href=&#34;https://www.weave.works/docs/net/latest/concepts/encryption/&#34;&gt;Weave Net通信の暗号化&lt;/a&gt;に使うパスワードを保持するSecret名。指定しないと平文。(参考: &lt;a href=&#34;https://www.weave.works/docs/net/latest/tasks/manage/security-untrusted-networks/&#34;&gt;https://www.weave.works/docs/net/latest/tasks/manage/security-untrusted-networks/&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;IPALLOC_RANGE: Podに割り当てるIPアドレスの範囲。指定しないと10.32.0.0/12。&lt;/li&gt;
&lt;li&gt;CHECKPOINT_DISABLE: Weave Netのアップデートを&lt;a href=&#34;https://www.weave.works/docs/net/latest/install/installing-weave/#checkpoint&#34;&gt;定期的にチェック&lt;/a&gt;する機能の無効化オプション。&lt;/li&gt;
&lt;li&gt;WEAVE_MTU: MTUを指定するオプション。&lt;a href=&#34;https://www.weave.works/docs/net/latest/tasks/manage/fastdp/#packet-size-mtu&#34;&gt;デフォルトで1376バイト&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;WEAVE_MTUはとりあえずデフォルトにしておいて、IPALLOC_RANGEもデフォルトにして、通信暗号化して、CHECKPOINT_DISABLEをtrueにするとすると、マニフェストは以下のようにダウンロードできる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# curl -fsSLo weave-daemonset.yaml &amp;quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d &#39;\n&#39;)&amp;amp;env.CHECKPOINT_DISABLE=1&amp;amp;password-secret=weave-passwd&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(通信暗号化は単一ノードなら不要だと思うけどとりあえず設定しておく。)&lt;/p&gt;

&lt;h2 id=&#34;kubernetesコンポーネントの起動オプション&#34;&gt;Kubernetesコンポーネントの起動オプション&lt;/h2&gt;

&lt;p&gt;kube-controller-managerの起動オプションの&lt;code&gt;--cluster-cidr&lt;/code&gt;はIPALLOC_RANGEと同じにする必要がある。
今回は10.32.0.0/12を指定する。&lt;/p&gt;

&lt;p&gt;また、kube-proxyの起動オプションの&lt;a href=&#34;https://www.weave.works/docs/net/latest/kubernetes/kube-addon/#-things-to-watch-out-for&#34;&gt;要件&lt;/a&gt;は以下。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--masquerade-all&lt;/code&gt;を指定してはいけない。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--cluster-cidr&lt;/code&gt;を指定する場合、IPALLOC_RANGEと同じにする必要がある。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;また、kube-apiserverとkube-controller-managerの起動オプションに&lt;code&gt;--allow-privileged&lt;/code&gt;を付ける必要があるはず。&lt;/p&gt;

&lt;h2 id=&#34;secret作成&#34;&gt;Secret作成&lt;/h2&gt;

&lt;p&gt;password-secretに渡すSecretは以下のように作成できる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# WEAVE_PASSWORD=$(echo -n &#39;your_secure_password&#39; | base64)
# cat &amp;lt;&amp;lt;EOF | kubectl create -f -
apiVersion: v1
kind: Secret
metadata:
  namespace: kube-system
  name: weave-passwd
type: Opaque
data:
  weave-passwd: ${WEAVE_PASSWORD}
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;これで準備完了。&lt;/p&gt;

&lt;h2 id=&#34;マニフェスト適用&#34;&gt;マニフェスト適用&lt;/h2&gt;

&lt;p&gt;以下のコマンドでマニフェストを適用し、Weave Netをデプロイできる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# kubectl apply -f weave-daemonset.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;weaveworks/weave-kube:2.3.0&lt;/code&gt;と&lt;code&gt;weaveworks/weave-npc:2.3.0&lt;/code&gt;がpullされる。
前者が本体で、後者がNetwork Policy Controller。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;マスタノード上で以下のコマンドを実行すると、Weave NetのAPIを叩いて状態を確認できる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# curl http://localhost:6784/status
        Version: 2.3.0 (version check update disabled)

        Service: router
       Protocol: weave 1..2
           Name: 92:44:35:3d:f8:d8(k8s-master)
     Encryption: enabled
  PeerDiscovery: enabled
        Targets: 1
    Connections: 1 (1 failed)
          Peers: 1
 TrustedSubnets: none

        Service: ipam
         Status: ready
          Range: 10.32.0.0/12
  DefaultSubnet: 10.32.0.0/12
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Kubernetes 1.10をスクラッチから全手動で構築</title>
          <link>https://www.kaitoy.xyz/2018/04/17/kubernetes110-from-scratch/</link>
          <pubDate>Tue, 17 Apr 2018 00:31:48 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2018/04/17/kubernetes110-from-scratch/</guid>
          <description>

&lt;p&gt;Oracle Linux 7.4.0のVMでKubernetes1.10.0のクラスタをスクラッチから全手動で作った。
参考にしたのは主に以下。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://nixaid.com/deploying-kubernetes-cluster-from-scratch/&#34;&gt;https://nixaid.com/deploying-kubernetes-cluster-from-scratch/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/scratch/&#34;&gt;https://kubernetes.io/docs/getting-started-guides/scratch/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ulam.io/blog/kubernetes-scratch/&#34;&gt;https://ulam.io/blog/kubernetes-scratch/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/creating-a-linux-master&#34;&gt;https://docs.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/creating-a-linux-master&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h2 id=&#34;構成&#34;&gt;構成&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;マシン: Windows 10 Homeのラップトップの上のVMware PlayerのVM

&lt;ul&gt;
&lt;li&gt;CPU: 2コア&lt;/li&gt;
&lt;li&gt;メモリ: 4GB&lt;/li&gt;
&lt;li&gt;NIF: NATのを一つ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;OS: Oracle Linux 7.4.0

&lt;ul&gt;
&lt;li&gt;Minimalインストール&lt;/li&gt;
&lt;li&gt;IPアドレス: 192.168.171.200、静的割り当て&lt;/li&gt;
&lt;li&gt;ホスト名: k8s-master (hostsで解決)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Docker: Oracle Container Runtime for Docker (docker-engine) 17.06.2&lt;/li&gt;
&lt;li&gt;Kubernetes: バージョン1.10.0

&lt;ul&gt;
&lt;li&gt;単一ノード&lt;/li&gt;
&lt;li&gt;全コンポーネント(kubelet、kube-proxy、kube-apiserver、kube-controller-manager、kube-scheduler、etcd)をsystemdで起動 (i.e. 非コンテナ)&lt;/li&gt;
&lt;li&gt;コンポーネント間通信とkubectlの通信をTLSで暗号化&lt;/li&gt;
&lt;li&gt;コンポーネント間通信とkubectlの通信の認証は&lt;a href=&#34;https://kubernetes.io/docs/admin/authentication/#x509-client-certs&#34;&gt;x509クライアント証明書&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TLS BootstrappingにはBootstrap token使用。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/tls/certificate-rotation/&#34;&gt;Certificate Rotation&lt;/a&gt;有効&lt;/li&gt;
&lt;li&gt;etcd 3.1.12&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/coreos/flannel&#34;&gt;flannel&lt;/a&gt; 0.10.0&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/coredns/coredns&#34;&gt;CoreDNS&lt;/a&gt; 1.1.1&lt;/li&gt;
&lt;li&gt;SERVICE_CLUSTER_IP_RANGE (Serviceに割り当てるIPの範囲) は10.0.0.0/16

&lt;ul&gt;
&lt;li&gt;kube-apiserverのIPはこの範囲の最初のIP(i.e. 10.0.0.1)になる。&lt;/li&gt;
&lt;li&gt;ホストネットワークや、CLUSTER_CIDRと範囲が被らないようにする必要がある。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;CLUSTER_CIDR (Podに割り当てるIPの範囲) は10.244.0.0/16

&lt;ul&gt;
&lt;li&gt;flannelの要件に合わせている。&lt;/li&gt;
&lt;li&gt;ホストネットワークや、SERVICE_CLUSTER_IP_RANGEと範囲が被らないようにする必要がある。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;kubeletの動作条件にあるので、swapをoffにする。
Oracle Linuxにログインして、&lt;code&gt;/etc/fstab&lt;/code&gt;のswapの行を削除して、以下のコマンドを実行。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# swapoff -a
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;SELinuxはちゃんと設定すればKubernetes動かせるはずだけど、面倒なのでとりあえず無効にする。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;/etc/selinux/config&lt;/code&gt;を編集して、&lt;code&gt;SELINUX&lt;/code&gt;を&lt;code&gt;permissive&lt;/code&gt;にして、以下のコマンドを実行。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# setenforce 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;ファイアウォールもちゃんと設定すればいいんだけど面倒なのでとりあえず無効にする。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# systemctl stop firewalld
# systemctl disable firewalld
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;これで準備完了。&lt;/p&gt;

&lt;h2 id=&#34;クラスタ構築手順&#34;&gt;クラスタ構築手順&lt;/h2&gt;

&lt;p&gt;おおむね、k8sコンポーネント間の通信の暗号化に使う鍵と証明書の生成、各コンポーネント用kubeconfigの生成、etcdのデプロイ、k8sコンポーネントのデプロイ、fannelデプロイ、CoreDNSデプロイ、という流れ。
ついでに最後に&lt;a href=&#34;https://github.com/weaveworks/scope&#34;&gt;Weave Scope&lt;/a&gt;をデプロイしてみる。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Bridge netfilterとIP forwardingを設定&lt;/p&gt;

&lt;p&gt;まず、Bridge netfilterモジュールをロードする。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# modprobe br_netfilter
# cat &amp;gt; /etc/sysconfig/modules/br_netfilter.modules &amp;lt;&amp;lt; EOF
#!/bin/sh
/sbin/modprobe br_netfilter &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
EOF
# chmod 755 /etc/sysconfig/modules/br_netfilter.modules
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Bridge netfilterとIP forwardingを有効化する。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cat &amp;gt; /etc/sysctl.d/kubernetes.conf &amp;lt;&amp;lt; EOF
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
EOF
# sysctl -p /etc/sysctl.d/kubernetes.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;設定確認。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# lsmod |grep br_netfilter
# sysctl -a | grep -E &amp;quot;net.bridge.bridge-nf-call-|net.ipv4.ip_forward&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;x509証明書生成&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;opensslの設定作成&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# mkdir -p /etc/kubernetes/pki
# cd /etc/kubernetes/pki
# K8S_SERVICE_IP=10.0.0.1
# MASTER_IP=192.168.171.200
# cat &amp;gt; openssl.cnf &amp;lt;&amp;lt; EOF
[ req ]
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_ca ]
basicConstraints = critical, CA:TRUE
keyUsage = critical, digitalSignature, keyEncipherment, keyCertSign
[ v3_req_server ]
basicConstraints = CA:FALSE
keyUsage = critical, digitalSignature, keyEncipherment
extendedKeyUsage = serverAuth
[ v3_req_client ]
basicConstraints = CA:FALSE
keyUsage = critical, digitalSignature, keyEncipherment
extendedKeyUsage = clientAuth
[ v3_req_apiserver ]
basicConstraints = CA:FALSE
keyUsage = critical, digitalSignature, keyEncipherment
extendedKeyUsage = serverAuth
subjectAltName = @alt_names_cluster
[ v3_req_etcd ]
basicConstraints = CA:FALSE
keyUsage = critical, digitalSignature, keyEncipherment
extendedKeyUsage = serverAuth, clientAuth
subjectAltName = @alt_names_etcd
[ alt_names_cluster ]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster.local
DNS.5 = k8s-controller
IP.1 = ${MASTER_IP}
IP.2 = ${K8S_SERVICE_IP}
[ alt_names_etcd ]
DNS.1 = k8s-controller
IP.1 = ${MASTER_IP}
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Kubernetes CA証明書生成&lt;/p&gt;

&lt;p&gt;以降で生成する証明書に署名するための証明書。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cd /etc/kubernetes/pki
# CA_DAYS=5475
# openssl ecparam -name secp521r1 -genkey -noout -out ca.key
# chmod 0600 ca.key
# openssl req -x509 -new -sha256 -nodes -key ca.key -days $CA_DAYS -out ca.crt -subj &amp;quot;/CN=kubernetes-ca&amp;quot;  -extensions v3_ca -config ./openssl.cnf
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kube-apiserver証明書生成&lt;/p&gt;

&lt;p&gt;kube-apiserverのサーバ証明書。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cd /etc/kubernetes/pki
# APISERVER_DAYS=5475
# openssl ecparam -name secp521r1 -genkey -noout -out kube-apiserver.key
# chmod 0600 kube-apiserver.key
# openssl req -new -sha256 -key kube-apiserver.key -subj &amp;quot;/CN=kube-apiserver&amp;quot; | openssl x509 -req -sha256 -CA ca.crt -CAkey ca.key -CAcreateserial -out kube-apiserver.crt -days $APISERVER_DAYS -extensions v3_req_apiserver -extfile ./openssl.cnf
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kube-apiserver-kubelet証明書生成&lt;/p&gt;

&lt;p&gt;kube-apiserverがkubeletのAPIにアクセスするときのクライアント証明書。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cd /etc/kubernetes/pki
# APISERVER_KUBELET_CLIENT_DAYS=5475
# openssl ecparam -name secp521r1 -genkey -noout -out apiserver-kubelet-client.key
# chmod 0600 apiserver-kubelet-client.key
# openssl req -new -key apiserver-kubelet-client.key -subj &amp;quot;/CN=kube-apiserver-kubelet-client/O=system:masters&amp;quot; | openssl x509 -req -sha256 -CA ca.crt -CAkey ca.key -CAcreateserial -out apiserver-kubelet-client.crt -days $APISERVER_KUBELET_CLIENT_DAYS -extensions v3_req_client -extfile ./openssl.cnf
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;adminクライアント証明書生成&lt;/p&gt;

&lt;p&gt;kubectlがkube-apiserverのAPIにアクセスするときのクライアント証明書。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cd /etc/kubernetes/pki
# ADMIN_DAYS=5475
# openssl ecparam -name secp521r1 -genkey -noout -out admin.key
# chmod 0600 admin.key
# openssl req -new -key admin.key -subj &amp;quot;/CN=kubernetes-admin/O=system:masters&amp;quot; | openssl x509 -req -sha256 -CA ca.crt -CAkey ca.key -CAcreateserial -out admin.crt -days $ADMIN_DAYS -extensions v3_req_client -extfile ./openssl.cnf
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Service Accountキーとkube-controller-managerのクライアント証明書生成&lt;/p&gt;

&lt;p&gt;Service Accountトークンにkube-controller-managerが署名するときに使うキーを生成する。
そのキーを使ってkube-controller-managerのクライアント証明書も生成する。&lt;/p&gt;

&lt;p&gt;kube-apiserverがトークンの署名を確認するとき(?)に使う公開鍵も生成する。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cd /etc/kubernetes/pki
# CONTROLLER_MANAGER_DAYS=5475
# openssl ecparam -name secp521r1 -genkey -noout -out sa.key
# openssl ec -in sa.key -outform PEM -pubout -out sa.pub
# chmod 0600 sa.key
# openssl req -new -sha256 -key sa.key -subj &amp;quot;/CN=system:kube-controller-manager&amp;quot; | openssl x509 -req -sha256 -CA ca.crt -CAkey ca.key -CAcreateserial -out sa.crt -days $CONTROLLER_MANAGER_DAYS -extensions v3_req_client -extfile ./openssl.cnf
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kube-schedulerクライアント証明書生成&lt;/p&gt;

&lt;p&gt;kube-schedulerがkube-apiserverにリクエストするときに使うクライアント証明書。
多分。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cd /etc/kubernetes/pki
# SCHEDULER_DAYS=5475
# openssl ecparam -name secp521r1 -genkey -noout -out kube-scheduler.key
# chmod 0600 kube-scheduler.key
# openssl req -new -sha256 -key kube-scheduler.key -subj &amp;quot;/CN=system:kube-scheduler&amp;quot; | openssl x509 -req -sha256 -CA ca.crt -CAkey ca.key -CAcreateserial -out kube-scheduler.crt -days $SCHEDULER_DAYS -extensions v3_req_client -extfile ./openssl.cnf
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;front proxy CA証明書生成&lt;/p&gt;

&lt;p&gt;front proxyのクライアント証明書に署名するのにつかう証明書。
front proxyってなんだ?
多分、kube-apiserverの前にいて、クラスタの外からの通信を受けるやつ(&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/proxies/&#34;&gt;apiserver proxy&lt;/a&gt;)か。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cd /etc/kubernetes/pki
# FRONT_PROXY_CA_DAYS=5475
# openssl ecparam -name secp521r1 -genkey -noout -out front-proxy-ca.key
# chmod 0600 front-proxy-ca.key
# openssl req -x509 -new -sha256 -nodes -key front-proxy-ca.key -days $FRONT_PROXY_CA_DAYS -out front-proxy-ca.crt -subj &amp;quot;/CN=front-proxy-ca&amp;quot; -extensions v3_ca -config ./openssl.cnf
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;front proxyクライアント証明書&lt;/p&gt;

&lt;p&gt;クラスタの外からくるHTTPリクエストの、&lt;code&gt;--requestheader-username-headers&lt;/code&gt;で指定されたヘッダに書いてあるユーザ名を認証するときに見るクライアント証明書。
kubectlの証明書(adminクライアント証明書)とは違うんだろうか。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cd /etc/kubernetes/pki
# FRONT_PROXY_CLIENT_DAYS=5475
# openssl ecparam -name secp521r1 -genkey -noout -out front-proxy-client.key
# chmod 0600 front-proxy-client.key
# openssl req -new -sha256 -key front-proxy-client.key -subj &amp;quot;/CN=front-proxy-client&amp;quot; | openssl x509 -req -sha256 -CA front-proxy-ca.crt -CAkey front-proxy-ca.key -CAcreateserial -out front-proxy-client.crt -days $FRONT_PROXY_CLIENT_DAYS -extensions v3_req_client -extfile ./openssl.cnf
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;etcd CA証明書&lt;/p&gt;

&lt;p&gt;以降で生成するetcdの証明書に署名するための証明書。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cd /etc/kubernetes/pki
# ETCD_CA_DAYS=5475
# openssl ecparam -name secp521r1 -genkey -noout -out etcd-ca.key
# chmod 0600 etcd-ca.key
# openssl req -x509 -new -sha256 -nodes -key etcd-ca.key -days $ETCD_CA_DAYS -out etcd-ca.crt -subj &amp;quot;/CN=etcd-ca&amp;quot; -extensions v3_ca -config ./openssl.cnf
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;etcd証明書&lt;/p&gt;

&lt;p&gt;etcdサーバのサーバ証明書。
多分。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cd /etc/kubernetes/pki
# ETCD_DAYS=5475
# openssl ecparam -name secp521r1 -genkey -noout -out etcd.key
# chmod 0600 etcd.key
# openssl req -new -sha256 -key etcd.key -subj &amp;quot;/CN=etcd&amp;quot; | openssl x509 -req -sha256 -CA etcd-ca.crt -CAkey etcd-ca.key -CAcreateserial -out etcd.crt -days $ETCD_DAYS -extensions v3_req_etcd -extfile ./openssl.cnf
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;etcd peer証明書&lt;/p&gt;

&lt;p&gt;etcdサーバが冗長構成のとき、サーバ間の通信の暗号化に使う証明書。
マスタが一つなら要らない?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cd /etc/kubernetes/pki
# ETCD_PEER_DAYS=5475
# openssl ecparam -name secp521r1 -genkey -noout -out etcd-peer.key
# chmod 0600 etcd-peer.key
# openssl req -new -sha256 -key etcd-peer.key -subj &amp;quot;/CN=etcd-peer&amp;quot; | openssl x509 -req -sha256 -CA etcd-ca.crt -CAkey etcd-ca.key -CAcreateserial -out etcd-peer.crt -days $ETCD_PEER_DAYS -extensions v3_req_etcd -extfile ./openssl.cnf
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;確認&lt;/p&gt;

&lt;p&gt;以上で生成した証明書の内容を確認する。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cd /etc/kubernetes/pki
# for i in *crt; do
  echo $i:;
  openssl x509 -subject -issuer -noout -in $i;
  echo;
done
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Kubernetesバイナリインストール&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/scratch/#selecting-images&#34;&gt;公式ドキュメント&lt;/a&gt;によると、Docker、kubelet、kube-proxyはコンテナ外で動かして、etcd、kube-apiserver、kube-controller-manager、kube-schedulerはコンテナで動かすのが推奨されている。
けど、とりあえずは簡単に全部コンテナ外でやる。&lt;/p&gt;

&lt;p&gt;(Oracle Linux用には、各コンポのコンテナイメージ詰め合わせがOracle Container Services for use with Kubernetesという名前で配布されているけど、現時点で1.9までしかないので使わない。)&lt;/p&gt;

&lt;p&gt;バイナリは以下URLからダウンロードできる。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;全部入り: &lt;a href=&#34;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/kubernetes-server-linux-amd64.tar.gz&#34;&gt;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/kubernetes-server-linux-amd64.tar.gz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;kube-apiserver

&lt;ul&gt;
&lt;li&gt;バイナリ: &lt;a href=&#34;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kube-apiserver&#34;&gt;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kube-apiserver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;コンテナ: &lt;a href=&#34;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kube-apiserver.tar&#34;&gt;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kube-apiserver.tar&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;kube-controller-manager

&lt;ul&gt;
&lt;li&gt;バイナリ: &lt;a href=&#34;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kube-controller-manager&#34;&gt;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kube-controller-manager&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;コンテナ: &lt;a href=&#34;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kube-controller-manager.tar&#34;&gt;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kube-controller-manager.tar&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;kube-scheduler

&lt;ul&gt;
&lt;li&gt;バイナリ: &lt;a href=&#34;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kube-scheduler&#34;&gt;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kube-scheduler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;コンテナ: &lt;a href=&#34;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kube-scheduler.tar&#34;&gt;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kube-scheduler.tar&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;kube-proxy

&lt;ul&gt;
&lt;li&gt;バイナリ: &lt;a href=&#34;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kube-proxy&#34;&gt;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kube-proxy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;コンテナ: &lt;a href=&#34;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kube-proxy.tar&#34;&gt;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kube-proxy.tar&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;kubelet: &lt;a href=&#34;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kubelet&#34;&gt;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kubelet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;kubectl: &lt;a href=&#34;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kubectl&#34;&gt;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kubectl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;kubeadm: &lt;a href=&#34;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kubeadm&#34;&gt;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kubeadm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;hyperkube: &lt;a href=&#34;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/hyperkube&#34;&gt;https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/hyperkube&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最後のhyperkubeは、各種Kubernetesバイナリのごった煮。
ファイル名によって動作が変わる。
簡単のためこれを使うけど、個別のバイナリ使ったほうがメモリ使用量などで有利そう。&lt;/p&gt;

&lt;p&gt;hyperkubeとkubeadmのバイナリを&lt;code&gt;/usr/bin/&lt;/code&gt;において、以下のコマンドを実行。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# ln -s /usr/bin/hyperkube /usr/bin/kube-apiserver
# ln -s /usr/bin/hyperkube /usr/bin/kube-controller-manager
# ln -s /usr/bin/hyperkube /usr/bin/kube-scheduler
# ln -s /usr/bin/hyperkube /usr/bin/kube-proxy
# ln -s /usr/bin/hyperkube /usr/bin/kubelet
# ln -s /usr/bin/hyperkube /usr/bin/kubectl
# chmod +x /usr/bin/kube*
# mkdir -p /var/lib/{kubelet,kube-proxy}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kubeconfigファイル生成&lt;/p&gt;

&lt;p&gt;kubectlとマスタコンポーネントがkube-apiserverと話すときに使うkubeconfigファイルを生成する。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;kube-controller-managerのkubeconfig&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# MASTER_IP=192.168.171.200
# KUBERNETES_PUBLIC_ADDRESS=$MASTER_IP
# CLUSTER_NAME=&amp;quot;default&amp;quot;
# KCONFIG=controller-manager.kubeconfig
# KUSER=&amp;quot;system:kube-controller-manager&amp;quot;
# cd /etc/kubernetes/
# kubectl config set-cluster ${CLUSTER_NAME} --certificate-authority=pki/ca.crt --embed-certs=true --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 --kubeconfig=${KCONFIG}
# kubectl config set-credentials ${KUSER} --client-certificate=pki/sa.crt --client-key=pki/sa.key --embed-certs=true --kubeconfig=${KCONFIG}
# kubectl config set-context ${KUSER}@${CLUSTER_NAME} --cluster=${CLUSTER_NAME} --user=${KUSER} --kubeconfig=${KCONFIG}
# kubectl config use-context ${KUSER}@${CLUSTER_NAME} --kubeconfig=${KCONFIG}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;設定確認。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# kubectl config view --kubeconfig=${KCONFIG}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kube-schedulerのkubeconfig&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# MASTER_IP=192.168.171.200
# KUBERNETES_PUBLIC_ADDRESS=$MASTER_IP
# CLUSTER_NAME=&amp;quot;default&amp;quot;
# KCONFIG=scheduler.kubeconfig
# KUSER=&amp;quot;system:kube-scheduler&amp;quot;
# cd /etc/kubernetes/
# kubectl config set-cluster ${CLUSTER_NAME} --certificate-authority=pki/ca.crt --embed-certs=true --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 --kubeconfig=${KCONFIG}
# kubectl config set-credentials ${KUSER} --client-certificate=pki/kube-scheduler.crt --client-key=pki/kube-scheduler.key --embed-certs=true --kubeconfig=${KCONFIG}
# kubectl config set-context ${KUSER}@${CLUSTER_NAME} --cluster=${CLUSTER_NAME} --user=${KUSER} --kubeconfig=${KCONFIG}
# kubectl config use-context ${KUSER}@${CLUSTER_NAME} --kubeconfig=${KCONFIG}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;設定確認。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# kubectl config view --kubeconfig=${KCONFIG}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;adminのkubeconfig&lt;/p&gt;

&lt;p&gt;kubectl用。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# MASTER_IP=192.168.171.200
# KUBERNETES_PUBLIC_ADDRESS=$MASTER_IP
# CLUSTER_NAME=&amp;quot;default&amp;quot;
# KCONFIG=admin.kubeconfig
# KUSER=&amp;quot;kubernetes-admin&amp;quot;
# cd /etc/kubernetes/
# kubectl config set-cluster ${CLUSTER_NAME} --certificate-authority=pki/ca.crt --embed-certs=true --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 --kubeconfig=${KCONFIG}
# kubectl config set-credentials ${KUSER} --client-certificate=pki/admin.crt --client-key=pki/admin.key --embed-certs=true --kubeconfig=${KCONFIG}
# kubectl config set-context ${KUSER}@${CLUSTER_NAME} --cluster=${CLUSTER_NAME} --user=${KUSER} --kubeconfig=${KCONFIG}
# kubectl config use-context ${KUSER}@${CLUSTER_NAME} --kubeconfig=${KCONFIG}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;設定確認。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# kubectl config view --kubeconfig=${KCONFIG}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;etcdデプロイ&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/coreos/etcd/releases/download/v3.1.12/etcd-v3.1.12-linux-amd64.tar.gz&#34;&gt;https://github.com/coreos/etcd/releases/download/v3.1.12/etcd-v3.1.12-linux-amd64.tar.gz&lt;/a&gt;
からアーカイブをダウンロードして、中のetcdとetcdctlを&lt;code&gt;/usr/bin/&lt;/code&gt;にいれて、以下のコマンドを実行。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# chown root:root /usr/bin/etcd*
# chmod 0755 /usr/bin/etcd*
# mkdir -p /var/lib/etcd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で、systemdのユニットファイルを書いてサービス化。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# MASTER_IP=192.168.171.200
# CLUSTER_NAME=&amp;quot;default&amp;quot;
# ETCD_TOKEN=$(openssl rand -hex 5)
# ETCD_CLUSTER_TOKEN=$CLUSTER_NAME-$ETCD_TOKEN
# cat &amp;gt; /etc/systemd/system/etcd.service &amp;lt;&amp;lt; EOF
[Unit]
Description=etcd
Documentation=https://coreos.com/etcd/docs/latest/
After=network.target


[Service]
ExecStart=/usr/bin/etcd \\
  --name default \\
  --listen-client-urls https://${MASTER_IP}:2379,http://127.0.0.1:2379 \\
  --advertise-client-urls https://${MASTER_IP}:2379 \\
  --data-dir=/var/lib/etcd \\
  --cert-file=/etc/kubernetes/pki/etcd.crt \\
  --key-file=/etc/kubernetes/pki/etcd.key \\
  --peer-cert-file=/etc/kubernetes/pki/etcd-peer.crt \\
  --peer-key-file=/etc/kubernetes/pki/etcd-peer.key \\
  --trusted-ca-file=/etc/kubernetes/pki/etcd-ca.crt \\
  --peer-trusted-ca-file=/etc/kubernetes/pki/etcd-ca.crt \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --initial-advertise-peer-urls https://${MASTER_IP}:2380 \\
  --listen-peer-urls https://${MASTER_IP}:2380 \\
  --initial-cluster-token ${ETCD_CLUSTER_TOKEN} \\
  --initial-cluster default=https://${MASTER_IP}:2380 \\
  --initial-cluster-state new
Restart=always
RestartSec=10s


[Install]
WantedBy=multi-user.target
EOF
# systemctl daemon-reload
# systemctl enable etcd
# systemctl start etcd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;確認。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# systemctl status etcd -l
# etcdctl --ca-file=/etc/kubernetes/pki/etcd-ca.crt --cert-file=/etc/kubernetes/pki/etcd.crt --key-file=/etc/kubernetes/pki/etcd.key cluster-health
# etcdctl --ca-file=/etc/kubernetes/pki/etcd-ca.crt --cert-file=/etc/kubernetes/pki/etcd.crt --key-file=/etc/kubernetes/pki/etcd.key member list
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;マスタコンポーネントデプロイ。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;kube-apiserver&lt;/p&gt;

&lt;p&gt;systemdのユニットファイルを書いてサービス化。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# MASTER_IP=192.168.171.200
# SERVICE_CLUSTER_IP_RANGE=&amp;quot;10.0.0.0/16&amp;quot;
# cat &amp;gt; /etc/systemd/system/kube-apiserver.service &amp;lt;&amp;lt; EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target


[Service]
ExecStart=/usr/bin/kube-apiserver \\
  --feature-gates=RotateKubeletServerCertificate=true \\
  --apiserver-count=1 \\
  --allow-privileged=true \\
  --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,DenyEscalatingExec \\
  --authorization-mode=Node,RBAC \\
  --secure-port=6443 \\
  --bind-address=0.0.0.0 \\
  --advertise-address=${MASTER_IP} \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/kube-audit.log \\
  --client-ca-file=/etc/kubernetes/pki/ca.crt \\
  --etcd-cafile=/etc/kubernetes/pki/etcd-ca.crt \\
  --etcd-certfile=/etc/kubernetes/pki/etcd.crt \\
  --etcd-keyfile=/etc/kubernetes/pki/etcd.key \\
  --etcd-servers=https://${MASTER_IP}:2379 \\
  --service-account-key-file=/etc/kubernetes/pki/sa.pub \\
  --service-cluster-ip-range=${SERVICE_CLUSTER_IP_RANGE} \\
  --service-node-port-range=30000-32767 \\
  --tls-cert-file=/etc/kubernetes/pki/kube-apiserver.crt \\
  --tls-private-key-file=/etc/kubernetes/pki/kube-apiserver.key \\
  --enable-bootstrap-token-auth=true \\
  --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt \\
  --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key \\
  --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\
  --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt \\
  --requestheader-username-headers=X-Remote-User \\
  --requestheader-group-headers=X-Remote-Group \\
  --requestheader-allowed-names=front-proxy-client \\
  --requestheader-extra-headers-prefix=X-Remote-Extra- \\
  --v=2 \\
  --tls-min-version=VersionTLS12 \\
  --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
Restart=always
RestartSec=10s


[Install]
WantedBy=multi-user.target
EOF
# systemctl daemon-reload
# systemctl enable kube-apiserver
# systemctl start kube-apiserver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;--allow-privileged&lt;/code&gt;はflannelなどに必要。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--admission-control&lt;/code&gt;には&lt;a href=&#34;https://kubernetes.io/docs/admin/admission-controllers/#is-there-a-recommended-set-of-admission-controllers-to-use&#34;&gt;公式推奨のプラグイン&lt;/a&gt;に加えて、後述のTLS BootstrappingのためのNodeRestrictionを指定。
また、&lt;code&gt;--allow-privileged&lt;/code&gt;の効果を軽減するため、DenyEscalatingExecも追加で指定。
因みに、プラグインを指定する順番はKubernetes 1.10からは気にしなくてよくなった。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--authorization-mode&lt;/code&gt;にはRBACを指定するのが標準。
後述のTLS Bootstrappingをするなら、Nodeも要る。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--tls-min-version&lt;/code&gt;と&lt;code&gt;--tls-cipher-suites&lt;/code&gt;は&lt;a href=&#34;https://www.lambdanote.com/blogs/news/openssl-cookbook&#34;&gt;OpenSSLクックブック&lt;/a&gt;と&lt;a href=&#34;https://golang.org/pkg/crypto/tls/#pkg-constants&#34;&gt;Goのtlsパッケージドキュメント&lt;/a&gt;を参考に設定。
RSA鍵交換はNG、RC4と3DESもNG、AESの鍵長は128ビット以上、SHA1はNG。&lt;/p&gt;

&lt;p&gt;また、(&amp;ndash;tls-min-versionをVersionTLS12にする場合?)TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256が必須で、CBCモードがNG。(参照: &lt;a href=&#34;https://github.com/golang/go/blob/release-branch.go1.9/src/net/http/h2_bundle.go&#34;&gt;https://github.com/golang/go/blob/release-branch.go1.9/src/net/http/h2_bundle.go&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--feature-gates&lt;/code&gt;でRotateKubeletServerCertificateを有効にして、kubeletのサーバ証明書を自動更新するようにしている。
因みに、クライアント証明書を自動更新するRotateKubeletClientCertificateはデフォルトで有効。
これらがCertificate Rotationと呼ばれる機能。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--feature-gates&lt;/code&gt;は全Kubernetesコンポーネントで同じ値を指定するのがよさそう。&lt;/p&gt;

&lt;p&gt;確認。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# systemctl status kube-apiserver -l
# journalctl -u kube-apiserver
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kube-controller-manager&lt;/p&gt;

&lt;p&gt;systemdのユニットファイルを書いてサービス化。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# CLUSTER_CIDR=&amp;quot;10.244.0.0/16&amp;quot;
# SERVICE_CLUSTER_IP_RANGE=&amp;quot;10.0.0.0/16&amp;quot;
# CLUSTER_NAME=&amp;quot;default&amp;quot;
# cat &amp;gt; /etc/systemd/system/kube-controller-manager.service &amp;lt;&amp;lt; EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes
After=network.target


[Service]
ExecStart=/usr/bin/kube-controller-manager \\
  --feature-gates=RotateKubeletServerCertificate=true \\
  --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \\
  --bind-address=0.0.0.0 \\
  --controllers=*,bootstrapsigner,tokencleaner \\
  --service-account-private-key-file=/etc/kubernetes/pki/sa.key \\
  --allocate-node-cidrs=true \\
  --cluster-cidr=${CLUSTER_CIDR} \\
  --cluster-name=${CLUSTER_NAME} \\
  --service-cluster-ip-range=${SERVICE_CLUSTER_IP_RANGE} \\
  --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt \\
  --cluster-signing-key-file=/etc/kubernetes/pki/ca.key \\
  --root-ca-file=/etc/kubernetes/pki/ca.crt \\
  --use-service-account-credentials=true \\
  --tls-min-version=VersionTLS12 \\
  --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 \\
  --v=2 \\
  --experimental-cluster-signing-duration=8760h0m0s
Restart=always
RestartSec=10s


[Install]
WantedBy=multi-user.target
EOF
# systemctl daemon-reload
# systemctl enable kube-controller-manager
# systemctl start kube-controller-manager
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;期限の切れたBootstrap token(後述)を消すためにtokencleanerを有効にしている。&lt;/p&gt;

&lt;p&gt;bootstrapsignerは後述のcluster-infoにBootstrap tokenで署名するためのコントローラ。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#approval-controller&#34;&gt;csrapproving&lt;/a&gt;というコントローラがデフォルトで有効になっていて、後述のTLS BootstrapppingやCertificate Rotationの時に作られるCSRを自動で承認する。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--experimental-cluster-signing-duration&lt;/code&gt;は、Certificate Rotationのための設定で、自動発行する証明書の期限を1年に指定している。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--use-service-account-credentials&lt;/code&gt;をつけると、&lt;a href=&#34;https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles&#34;&gt;各コントローラが別々のService Accountで動く&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;確認。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# systemctl status kube-controller-manager -l
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kube-scheduler&lt;/p&gt;

&lt;p&gt;systemdのユニットファイルを書いてサービス化。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cat &amp;gt; /etc/systemd/system/kube-scheduler.service &amp;lt;&amp;lt; EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes
After=network.target


[Service]
ExecStart=/usr/bin/kube-scheduler \\
  --feature-gates=RotateKubeletServerCertificate=true \\
  --kubeconfig=/etc/kubernetes/scheduler.kubeconfig \\
  --address=0.0.0.0 \\
  --v=2
Restart=always
RestartSec=10s


[Install]
WantedBy=multi-user.target
EOF
# systemctl daemon-reload
# systemctl enable kube-scheduler
# systemctl start kube-scheduler
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;確認。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# systemctl status kube-scheduler -l
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;マスタコンポーネント状態確認&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# export KUBECONFIG=/etc/kubernetes/admin.kubeconfig
# kubectl config use-context kubernetes-admin@default
# kubectl version
# kubectl get componentstatuses
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/&#34;&gt;TLS Bootstrapping&lt;/a&gt;の設定&lt;/p&gt;

&lt;p&gt;TLS Bootstrappingは、Kubernetesクラスタのコンポーネント間の通信がTLSで暗号化されている環境で、ノードが新たにクラスタに参加するとき、自動的にセキュアに&lt;a href=&#34;https://ja.wikipedia.org/wiki/%E8%A8%BC%E6%98%8E%E6%9B%B8%E7%BD%B2%E5%90%8D%E8%A6%81%E6%B1%82&#34;&gt;CSR&lt;/a&gt;を処理する仕組み。&lt;/p&gt;

&lt;p&gt;TLS Bootstrappingでは、kubeletは起動時にBootstrap kubeconfigを読んで、kubeletとノード用のCSRを生成し、それらがkube-controller-managerに承認されると、kubelet用のクライアント証明書と秘密鍵を生成する。
その証明書と鍵を使ってkubeconfigを生成し、以降のクラスタへの接続に使う。&lt;/p&gt;

&lt;p&gt;Bootstrap時の認証には&lt;a href=&#34;https://kubernetes.io/docs/admin/bootstrap-tokens/&#34;&gt;Bootstrap Tokens&lt;/a&gt;か&lt;a href=&#34;https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#token-authentication-file&#34;&gt;Token authentication file&lt;/a&gt;を使うことが推奨されていて、今回は前者を使う。&lt;/p&gt;

&lt;p&gt;(後者については&lt;a href=&#34;https://medium.com/@toddrosner/kubernetes-tls-bootstrapping-cf203776abc7&#34;&gt;この記事&lt;/a&gt;に詳しい。)&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Bootstrap TokenのSecret生成&lt;/p&gt;

&lt;p&gt;以下のように生成できる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# TOKEN_PUB=$(openssl rand -hex 3)
# TOKEN_SECRET=$(openssl rand -hex 8)
# BOOTSTRAP_TOKEN=&amp;quot;${TOKEN_PUB}.${TOKEN_SECRET}&amp;quot;
# kubectl -n kube-system create secret generic bootstrap-token-${TOKEN_PUB} --type &#39;bootstrap.kubernetes.io/token&#39; --from-literal description=&amp;quot;cluster bootstrap token&amp;quot; --from-literal token-id=${TOKEN_PUB} --from-literal token-secret=${TOKEN_SECRET} --from-literal usage-bootstrap-authentication=true --from-literal usage-bootstrap-signing=true --from-literal auth-extra-groups=system:bootstrappers:worker,system:bootstrappers:ingress
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;けど、&lt;a href=&#34;https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-token/#cmd-token-generate&#34;&gt;kubeadm&lt;/a&gt;でも生成出来てこっちのほうが楽なので、それで。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# BOOTSTRAP_TOKEN=$(kubeadm token create --kubeconfig /etc/kubernetes/admin.kubeconfig)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;BOOTSTRAP_TOKENの値はあとで使う。&lt;/p&gt;

&lt;p&gt;expirationは指定できなくて、1日で期限切れになっちゃうけど、クラスタにノードを追加するときに有効であればいいのでまあいい。&lt;/p&gt;

&lt;p&gt;確認。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# TOKEN_PUB=$(echo $BOOTSTRAP_TOKEN | sed -e s/\\..*//)
# kubectl -n kube-system get secret/bootstrap-token-${TOKEN_PUB} -o yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Bootstrap kubeconfig作成&lt;/p&gt;

&lt;p&gt;Bootstrap時は&lt;code&gt;kubelet-bootstrap&lt;/code&gt;というユーザでkube-apiserverに接続する。
&lt;code&gt;kubelet-bootstrap&lt;/code&gt;は&lt;code&gt;system:node-bootstrapper&lt;/code&gt;ロールを持って&lt;code&gt;system:bootstrappers&lt;/code&gt;に属しているユーザとして認証される必要がある。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# MASTER_IP=192.168.171.200
# KUBERNETES_PUBLIC_ADDRESS=$MASTER_IP
# CLUSTER_NAME=&amp;quot;default&amp;quot;
# KCONFIG=&amp;quot;bootstrap.kubeconfig&amp;quot;
# KUSER=&amp;quot;kubelet-bootstrap&amp;quot;
# cd /etc/kubernetes
# kubectl config set-cluster ${CLUSTER_NAME} --certificate-authority=pki/ca.crt --embed-certs=true --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 --kubeconfig=${KCONFIG}
# kubectl config set-context ${KUSER}@${CLUSTER_NAME} --cluster=${CLUSTER_NAME} --user=${KUSER} --kubeconfig=${KCONFIG}
# kubectl config use-context ${KUSER}@${CLUSTER_NAME} --kubeconfig=${KCONFIG}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;確認。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# kubectl config view --kubeconfig=${KCONFIG}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CA証明書とbootstrap kubeconfigをConfigMap(cluster-info)で公開&lt;/p&gt;

&lt;p&gt;kubeletはこのConfigMapを見てクラスタに参加する。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# kubectl -n kube-public create configmap cluster-info --from-file /etc/kubernetes/pki/ca.crt --from-file /etc/kubernetes/bootstrap.kubeconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;anonymousユーザにcluster-infoへのアクセスを許可する。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# kubectl -n kube-public create role system:bootstrap-signer-clusterinfo --verb get --resource configmaps
# kubectl -n kube-public create rolebinding kubeadm:bootstrap-signer-clusterinfo --role system:bootstrap-signer-clusterinfo --user system:anonymous
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kubelet-bootstrapユーザにロールとグループを紐づける。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# kubectl create clusterrolebinding kubeadm:kubelet-bootstrap --clusterrole system:node-bootstrapper --group system:bootstrappers
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;bootstrap.kubeconfigにトークンを追記&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# kubectl config set-credentials kubelet-bootstrap --token=${BOOTSTRAP_TOKEN} --kubeconfig=/etc/kubernetes/bootstrap.kubeconfig
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Docker、CNI、kubeletインストール&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Docker&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/install/linux/docker-ce/centos/#set-up-the-repository&#34;&gt;https://docs.docker.com/install/linux/docker-ce/centos/#set-up-the-repository&lt;/a&gt;
に従ってDocker CEをインストール。
ストレージドライバにはoverlay2をつかうので、device-mapper-persistent-dataとlvm2は入れない。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# yum install -y yum-utils
# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
# yum install -y docker-ce
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;18.03.0-ceが入った。&lt;/p&gt;

&lt;p&gt;が、よくみたらDocker CEはOracle Linuxをサポートしていないので、Docker CEはアンインストールして、代わりに&lt;a href=&#34;https://docs.oracle.com/cd/E77565_01/E87205/html/section_install_upgrade_yum_docker.html&#34;&gt;Oracle Container Runtime for Docker&lt;/a&gt; (aka docker-engine)を入れる。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;/etc/yum.repos.d/public-yum-ol7.repo&lt;/code&gt;の&lt;code&gt;ol7_addons&lt;/code&gt;の&lt;code&gt;enabled&lt;/code&gt;を1にして、以下のコマンドでdocker-engineをインストール。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# yum install -y docker-engine
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;docker-engine 17.06.2が入った。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;/etc/sysconfig/docker&lt;/code&gt;に以下を追記して、 Dockerがオープンできる最大ファイル数を増やす。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DOCKER_NOFILE=1000000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kubernetes環境ではiptablesはkube-proxyが操作するので、Dockerには操作させないようにするため、&lt;code&gt;/etc/sysconfig/docker&lt;/code&gt;の&lt;code&gt;OPTIONS&lt;/code&gt;に&lt;code&gt;--iptables=false&lt;/code&gt;を追加。
(これをすると、&lt;code&gt;--icc=false&lt;/code&gt;は設定できなくなる(不要になる)。)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# systemctl daemon-reload
# systemctl enable docker
# systemctl start docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;確認。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cat /proc/$(pidof dockerd)/environ
# systemctl status docker -l
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CNI&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# mkdir -p /etc/cni/net.d /opt/cni/bin/
# cd /tmp
# curl -OL https://github.com/containernetworking/cni/releases/download/v0.6.0/cni-amd64-v0.6.0.tgz
# curl -OL https://github.com/containernetworking/plugins/releases/download/v0.7.1/cni-plugins-amd64-v0.7.1.tgz
# cd /opt/cni/bin
# tar zxf /tmp/cni-amd64-v0.6.0.tgz
# tar zxf /tmp/cni-plugins-amd64-v0.7.1.tgz
# chmod +x /opt/cni/bin/*
# cat &amp;gt;/etc/cni/net.d/99-loopback.conf &amp;lt;&amp;lt;EOF
{
  &amp;quot;type&amp;quot;: &amp;quot;loopback&amp;quot;
}
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kubelet&lt;/p&gt;

&lt;p&gt;前提コマンド(conntrack)インストール。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# yum -y install conntrack-tools
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;systemdのユニットファイルを書いてサービス化。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# DNS_SERVER_IP=10.0.0.10
# PAUSE_IMAGE=k8s.gcr.io/pause-amd64:3.1
# DNS_DOMAIN=&amp;quot;cluster.local&amp;quot;
# mkdir -p /etc/kubernetes/manifests
# cat &amp;gt; /etc/systemd/system/kubelet.service &amp;lt;&amp;lt; EOF
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service


[Service]
ExecStart=/usr/bin/kubelet \\
  --feature-gates=RotateKubeletServerCertificate=true \\
  --address=0.0.0.0 \\
  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\
  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\
  --pod-manifest-path=/etc/kubernetes/manifests \\
  --network-plugin=cni \\
  --cni-conf-dir=/etc/cni/net.d \\
  --cni-bin-dir=/opt/cni/bin \\
  --cluster-dns=${DNS_SERVER_IP} \\
  --cluster-domain=${DNS_DOMAIN} \\
  --authorization-mode=Webhook \\
  --client-ca-file=/etc/kubernetes/pki/ca.crt \\
  --cert-dir=/etc/kubernetes/pki \\
  --rotate-certificates=true \\
  --v=2 \\
  --cgroup-driver=cgroupfs \\
  --pod-infra-container-image=${PAUSE_IMAGE} \\
  --tls-min-version=VersionTLS12 \\
  --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 \\
  --allow-privileged=true
Restart=always
RestartSec=10s


[Install]
WantedBy=multi-user.target
EOF
# systemctl daemon-reload
# systemctl enable kubelet
# systemctl start kubelet
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(実際は、systemctl start kubeletするまえに、後述のNode CSR自動承認設定をすべし。)&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--allow-privileged&lt;/code&gt;はflannelなどに必要。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--pod-infra-container-image&lt;/code&gt;では&lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/build/pause&#34;&gt;pause&lt;/a&gt;コンテナイメージを指定する。
このコンテナはPod毎に起動され、Podネットワークの名前空間を保持するために使われるらしい。
今回使った&lt;code&gt;k8s.gcr.io/pause-amd64:3.1&lt;/code&gt;はKubernetesチームが配布しているものだけど、Oracleが配布しているものもあり、そちらを使うには、Oracle Linux 7.4のダウンロード媒体リストに含まれるOracle Container Services for use with Kubernetes 1.1.9.1に入っているpause-amd64.tarを&lt;code&gt;docker load&lt;/code&gt;しておいて、そのイメージ名を&lt;code&gt;--pod-infra-container-image&lt;/code&gt;に渡せばいい。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--bootstrap-kubeconfig&lt;/code&gt;で指定したkubeconfigでTLS Bootstrapして、&lt;code&gt;--cert-dir&lt;/code&gt;で指定したディレクトリに証明書と鍵を生成して、&lt;code&gt;--kubeconfig&lt;/code&gt;で指定したパスに以降使うkubeconfigを生成する。
この証明書を自動更新(i.e. Certificate Rotation)するオプションが&lt;code&gt;--rotate-certificates&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;--pod-manifest-path&lt;/code&gt;で指定したディレクトリはkubeletに定期的にスキャンされ、そこに置いたKubernetesマニフェスト(ドットで始まるもの以外)が読まれる。
(参照: &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/static-pod/&#34;&gt;Static Pods&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;本当は&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/&#34;&gt;Kubelet Configファイル&lt;/a&gt;を使ったほうがいいみたいなので、いずれそれに対応する。
(対応した: 「&lt;a href=&#34;https://www.kaitoy.xyz/2018/05/05/kubernetes-kubelet-config-and-pod-sec-policy/&#34;&gt;Kubernetes 1.10のkubeletの起動オプションをKubelet ConfigファイルとPodSecurityPolicyで置き換える&lt;/a&gt;」)&lt;/p&gt;

&lt;p&gt;起動確認。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# systemctl status kubelet -l
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Node CSR手動承認&lt;/p&gt;

&lt;p&gt;TLS Bootstrappingで生成されたCSRを手動で承認する。&lt;/p&gt;

&lt;p&gt;CSRは以下のコマンドで見れる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# kubectl get csr
NAME                                                   AGE       REQUESTOR                 CONDITION
csr-cf9hm                                              24m       system:node:k8s-master  Pending
node-csr-Vcw_4HioW1CI96eDH29RMKPrOchEN133053wm6DCXUk   24m       system:bootstrap:itacbw   Pending
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;node-csr-…&lt;/code&gt;がクライアント証明書のためのCSRで、&lt;code&gt;csr-…&lt;/code&gt;がサーバ証明書の。
これらを承認する。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# kubectl certificate approve node-csr-Vcw_4HioW1CI96eDH29RMKPrOchEN133053wm6DCXUk
# kubectl certificate approve csr-cf9hm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(因みに否認するときは&lt;code&gt;kubectl certificate deny&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;これでクラスタにノードが追加されたはず。
確認。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# kubectl get node
NAME         STATUS    ROLES     AGE       VERSION
k8s-master   Ready     &amp;lt;none&amp;gt;    36s       v1.10.0
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Node CSR自動承認設定&lt;/p&gt;

&lt;p&gt;前節でやった手動承認はcsrapprovingが自動でやってくれる。&lt;/p&gt;

&lt;p&gt;新規ノード参加時のCSRを承認するClusterRoleとして&lt;code&gt;system:certificates.k8s.io:certificatesigningrequests:nodeclient&lt;/code&gt;が自動生成されているので、これを&lt;code&gt;system:bootstrappers&lt;/code&gt;グループにバインドしてやると、自動承認が有効になる。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;s&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cat &amp;lt;&amp;lt;EOF | kubectl create -f -
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: auto-approve-csrs-for-group
subjects:
- kind: Group
  name: system:bootstrappers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
  apiGroup: rbac.authorization.k8s.io
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;また、kubeletのクライアント証明書を自動更新(i.e. RotateKubeletClientCertificate)するときのCSRを承認するClusterRoleとして&lt;code&gt;system:certificates.k8s.io:certificatesigningrequests:selfnodeclient&lt;/code&gt;が自動生成されていて、これをノード毎のユーザにバインドしてやると、自動承認が有効になる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# NODE_USER_NAME=k8s-master
# cat &amp;lt;&amp;lt;EOF | kubectl create -f -
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: ${NODE_USER_NAME}-node-client-cert-renewal
subjects:
- kind: User
  name: system:node:${NODE_USER_NAME}
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
  apiGroup: rbac.authorization.k8s.io
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kubeletのサーバ証明書を自動更新(i.e. RotateKubeletServerCertificate)するときのCSRを承認するClusterRoleは現時点で自動生成されないので、自分で作ってノード毎のユーザにバインドして、自動承認を有効にする。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cat &amp;lt;&amp;lt;EOF | kubectl create -f -
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: approve-node-server-renewal-csr
rules:
- apiGroups: [&amp;quot;certificates.k8s.io&amp;quot;]
  resources: [&amp;quot;certificatesigningrequests/selfnodeserver&amp;quot;]
  verbs: [&amp;quot;create&amp;quot;]
EOF
# NODE_USER_NAME=k8s-master
# cat &amp;lt;&amp;lt;EOF | kubectl create -f -
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: ${NODE_USER_NAME}-server-client-cert-renewal
subjects:
- kind: User
  name: system:node:${NODE_USER_NAME}
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: approve-node-server-renewal-csr
  apiGroup: rbac.authorization.k8s.io
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kube-proxy、オーバレイネットワーク、DNSのデプロイ&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;kube-proxy&lt;/p&gt;

&lt;p&gt;Service Account作成。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# export KUBECONFIG=/etc/kubernetes/admin.kubeconfig
# kubectl -n kube-system create serviceaccount kube-proxy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kube-proxyのkubeconfig作成&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# MASTER_IP=192.168.171.200
# KUBERNETES_PUBLIC_ADDRESS=$MASTER_IP
# export KUBECONFIG=/etc/kubernetes/admin.kubeconfig
# SECRET=$(kubectl -n kube-system get sa/kube-proxy --output=jsonpath=&#39;{.secrets[0].name}&#39;)
# JWT_TOKEN=$(kubectl -n kube-system get secret/$SECRET --output=jsonpath=&#39;{.data.token}&#39; | base64 -d)
# CLUSTER_NAME=&amp;quot;default&amp;quot;
# KCONFIG=&amp;quot;kube-proxy.kubeconfig&amp;quot;
# cd /etc/kubernetes
# kubectl config set-cluster ${CLUSTER_NAME} --certificate-authority=pki/ca.crt --embed-certs=true --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 --kubeconfig=${KCONFIG}
# kubectl config set-context ${CLUSTER_NAME} --cluster=${CLUSTER_NAME} --user=default --namespace=default --kubeconfig=${KCONFIG}
# kubectl config set-credentials ${CLUSTER_NAME} --token=${JWT_TOKEN} --kubeconfig=${KCONFIG}
# kubectl config use-context ${CLUSTER_NAME} --kubeconfig=${KCONFIG}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;確認。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# kubectl config view --kubeconfig=${KCONFIG}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Service Accountのkube-proxyに&lt;code&gt;system:node-proxier&lt;/code&gt;というClusterRoleを付ける。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# kubectl create clusterrolebinding kubeadm:node-proxier --clusterrole system:node-proxier --serviceaccount kube-system:kube-proxy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;systemdのユニットファイルを書いてサービス化。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cat &amp;gt; /etc/systemd/system/kube-proxy.service &amp;lt;&amp;lt; EOF
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes
After=network.target


[Service]
ExecStart=/usr/bin/kube-proxy \\
  --feature-gates=RotateKubeletServerCertificate=true \\
  --bind-address 0.0.0.0 \\
  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\
  --v=2
Restart=always
RestartSec=10s


[Install]
WantedBy=multi-user.target
EOF
# systemctl daemon-reload
# systemctl enable kube-proxy
# systemctl start kube-proxy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;確認。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# systemctl status kube-proxy -l
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;オーバレイネットワーク (flannel)&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/coreos/flannel/blob/master/Documentation/kubernetes.md&#34;&gt;flannelのドキュメント&lt;/a&gt;を参考に。&lt;/p&gt;

&lt;p&gt;flannelをデプロイするには、kube-apiserverとkube-controller-managerの起動オプションに&lt;code&gt;--allow-privileged&lt;/code&gt;を付ける必要がある。&lt;/p&gt;

&lt;p&gt;また、公式が配布しているKubernetesマニフェストを使う場合、kube-controller-managerの起動オプションの&lt;code&gt;--cluster-cidr&lt;/code&gt;で&lt;code&gt;10.244.0.0/16&lt;/code&gt;を指定する必要がある。&lt;/p&gt;

&lt;p&gt;デプロイ自体は以下のコマンドを実行するだけ。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# export KUBECONFIG=/etc/kubernetes/admin.kubeconfig
# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このKubernetesマニフェストでは、quay.ioから&lt;code&gt;quay.io/coreos/flannel:v0.10.0-amd64&lt;/code&gt;というコンテナイメージがpullされる。&lt;/p&gt;

&lt;p&gt;Oracleもflannelのコンテナイメージを配布していて、そちらを使うには、Oracle Linux 7.4のダウンロード媒体リストに含まれるOracle Container Services for use with Kubernetes 1.1.9.1に入っているflannel.tarを&lt;code&gt;docker load&lt;/code&gt;しておいて、そのイメージを使うようにマニフェストを書きかえればいい。&lt;/p&gt;

&lt;p&gt;起動確認。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# kubectl -n kube-system get po
NAME                    READY     STATUS    RESTARTS   AGE
kube-flannel-ds-gkcqd   1/1       Running   0          1m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;flannelは&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/network-policies/&#34;&gt;Network Policy&lt;/a&gt;をサポートしていないので、&lt;a href=&#34;https://www.projectcalico.org/&#34;&gt;Calico&lt;/a&gt;か&lt;a href=&#34;https://www.weave.works/oss/net/&#34;&gt;Weave Net&lt;/a&gt;あたりにすればよかったかも。
(やった: 「&lt;a href=&#34;https://www.kaitoy.xyz/2018/05/04/kubernetes-with-weave-net/&#34;&gt;Kubernetes 1.10のクラスタにWeave Netをデプロイする&lt;/a&gt;」)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CoreDNS&lt;/p&gt;

&lt;p&gt;Kubernetes 1.10からは、サービスディスカバリに(kube-dnsの代わりに)CoreDNSを使うのが標準になった。&lt;/p&gt;

&lt;p&gt;以下を参考にCoreDNSをデプロイする:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/coredns/&#34;&gt;https://kubernetes.io/docs/tasks/administer-cluster/coredns/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://coredns.io/2018/01/29/deploying-kubernetes-with-coredns-using-kubeadm/&#34;&gt;https://coredns.io/2018/01/29/deploying-kubernetes-with-coredns-using-kubeadm/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/coredns/deployment/tree/master/kubernetes&#34;&gt;https://github.com/coredns/deployment/tree/master/kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cd /tmp
# curl -LO https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed
# curl -LO https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/deploy.sh
# chmod +x deploy.sh
# DNS_SERVER_IP=&amp;quot;10.0.0.10&amp;quot;
# SERVICE_CLUSTER_IP_RANGE=&amp;quot;10.0.0.0/16&amp;quot;
# DNS_DOMAIN=&amp;quot;cluster.local&amp;quot;
# ./deploy.sh -r $SERVICE_CLUSTER_IP_RANGE -i $DNS_SERVER_IP -d $DNS_DOMAIN &amp;gt; coredns.yaml
# export KUBECONFIG=/etc/kubernetes/admin.kubeconfig
# kubectl apply -f coredns.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このKubernetesマニフェストではDocker Hubから&lt;code&gt;coredns/coredns:1.1.1&lt;/code&gt;というイメージがpullされる。&lt;/p&gt;

&lt;p&gt;起動確認。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# kubectl -n kube-system get pods -o wide | grep coredns
coredns-8459d9f654-b585f   1/1       Running   0          48s       10.244.0.3        k8s-master
coredns-8459d9f654-x7drc   1/1       Running   0          48s       10.244.0.2        k8s-master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;起動確認時にCoreDNSのIPアドレスを確認して、動作確認。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# dig @10.244.0.3 kubernetes.default.svc.cluster.local +noall +answer


; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG 9.9.4-RedHat-9.9.4-61.el7 &amp;lt;&amp;lt;&amp;gt;&amp;gt; @10.244.0.3 kubernetes.default.svc.cluster.local +noall +answer
; (1 server found)
;; global options: +cmd
kubernetes.default.svc.cluster.local. 5 IN A    10.0.0.1
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Kubernetesアプリデプロイ&lt;/p&gt;

&lt;p&gt;前節まででKubernetesクラスタの構築は完了。
試しにKubernetesアプリをひとつデプロイしてみる。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/weaveworks/scope&#34;&gt;Weave Scope&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.weave.works/docs/scope/latest/installing/#k8s&#34;&gt;ドキュメント&lt;/a&gt;を参考に。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# cd /tmp
# export KUBECONFIG=/etc/kubernetes/admin.kubeconfig
# curl -sSL -o scope.yaml https://cloud.weave.works/k8s/scope.yaml?k8s-service-type=NodePort
# kubectl apply -f scope.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このKubernetesマニフェストではDocker Hubから&lt;code&gt;weaveworks/scope:1.8.0&lt;/code&gt;というイメージがpullされる。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kubectl -n weave get svc/weave-scope-app&lt;/code&gt;でポート調べて、&lt;code&gt;http://k8s-master:&amp;lt;ポート&amp;gt;/&lt;/code&gt;をブラウザ開くとWeave ScopeのGUIが見れる。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Skaffoldを触ってみた</title>
          <link>https://www.kaitoy.xyz/2018/04/01/hello-skaffold/</link>
          <pubDate>Sun, 01 Apr 2018 09:59:43 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2018/04/01/hello-skaffold/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;https://github.com/GoogleCloudPlatform/skaffold#run-a-deployment-pipeline-once&#34;&gt;Skaffold&lt;/a&gt;を試してみた話。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h2 id=&#34;skaffoldとは&#34;&gt;Skaffoldとは&lt;/h2&gt;

&lt;p&gt;Googleが開発している、Kubernetesアプリケーションを快適に開発するためのツール。
アプリケーションのソースを監視し、変更が入ると、自動的にコンテナイメージをビルドしてKubernetesクラスタにデプロイしてくれる。&lt;/p&gt;

&lt;p&gt;2018/3/16に&lt;a href=&#34;https://cloudplatform.googleblog.com/2018/03/introducing-Skaffold-Easy-and-repeatable-Kubernetes-development.html&#34;&gt;発表&lt;/a&gt;された新しいツールで、触った感じではまだこれからといった感じだった。&lt;/p&gt;

&lt;p&gt;Goで書かれていて、Linux、OS X、Windows用のバイナリが提供されている。&lt;/p&gt;

&lt;p&gt;似たツールにはMicrosoftの&lt;a href=&#34;https://draft.sh/&#34;&gt;Draft&lt;/a&gt;がある。&lt;/p&gt;

&lt;p&gt;また、Gitのコミットを自動デプロイしてくれるものに、&lt;a href=&#34;https://gitkube.sh/&#34;&gt;Gitkube&lt;/a&gt;、&lt;a href=&#34;http://jenkins-x.io/&#34;&gt;Jenkins X (エックス)&lt;/a&gt;がある。&lt;/p&gt;

&lt;h2 id=&#34;windows版を試す&#34;&gt;Windows版を試す&lt;/h2&gt;

&lt;p&gt;自PCがWindowsなのでWindows版を試す。
会社で使ってるのもWindowsだし。&lt;/p&gt;

&lt;p&gt;Skaffoldを使うには、Skaffoldの実行バイナリ、Kubernetesクラスタ、そのクラスタをコンテクストに設定したkubectl、Dockerが必要。&lt;/p&gt;

&lt;p&gt;まずWindows版Skaffoldをインストールする。
&lt;a href=&#34;https://github.com/GoogleCloudPlatform/skaffold/releases&#34;&gt;GitHubのリリースページ&lt;/a&gt;からWindowsバイナリをダウンロードして、skaffold.exeにリネームしてPATHの通ったところに置くだけ。
Skaffoldのバージョンは0.3.0。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Kubernetesクラスタは、Windows 10 Home上にminikube 0.22.2で作ったKubernetes 1.7.0のクラスタ。
minikubeは&lt;a href=&#34;https://www.kaitoy.xyz/2017/10/10/goslings-on-kubernetes/&#34;&gt;以前&lt;/a&gt;インストールしたものを使う。&lt;/p&gt;

&lt;p&gt;minikubeを起動。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;&amp;gt; minikube start --kubernetes-version v1.7.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kubectlもminikubeと一緒にインストール済み。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Dockerについては、デーモンはminikube上のを使えばいいとして、クライアント(Docker Client)はskaffoldコマンドから実行するのでWindows上にないとだめはなず。&lt;/p&gt;

&lt;p&gt;WindowsでDockerと言えば今なら&lt;a href=&#34;https://www.docker.com/docker-windows&#34;&gt;Docker for Windows&lt;/a&gt;だけど、これはWindows 10 Proじゃないと使えなかったはずなので、&lt;a href=&#34;https://docs.docker.com/toolbox/&#34;&gt;Docker Toolbox&lt;/a&gt;でクライアントをいれた。&lt;/p&gt;

&lt;p&gt;このクライアントはデフォルトではローカルのデーモンを見てるので、minikubeのデーモンを見させる。
そのための設定はminikubeのコマンドで分かるようになっている。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;&amp;gt; minikube docker-env
SET DOCKER_TLS_VERIFY=1
SET DOCKER_HOST=tcp://192.168.99.100:2376
SET DOCKER_CERT_PATH=C:\Users\kaitoy\.minikube\certs
SET DOCKER_API_VERSION=1.23
REM Run this command to configure your shell:
REM @FOR /f &amp;quot;tokens=*&amp;quot; %i IN (&#39;minikube docker-env&#39;) DO @%i
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これに従って以下のコマンドを実行するとクライアントの設定完了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;&amp;gt; @FOR /f &amp;quot;tokens=*&amp;quot; %i IN (&#39;minikube docker-env&#39;) DO @%i
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;これで準備完了。&lt;/p&gt;

&lt;p&gt;Skaffoldの&lt;a href=&#34;https://github.com/GoogleCloudPlatform/skaffold/tree/10d56cf0fd3c253b0716a084419b5833e53d9870#getting-started-with-local-tooling&#34;&gt;Getting Started&lt;/a&gt;をやってみる。&lt;/p&gt;

&lt;p&gt;Skaffoldのリポジトリをcloneして、コマンドプロンプト開いて、&lt;code&gt;examples/getting-started&lt;/code&gt;にcdして、以下のコマンドを実行。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;&amp;gt; skaffold dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;エラーで終わった。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[31mERRO[0m[0047] run: running skaffold steps: starting watch on file C:\Users\kaitoy\Desktop\skaffold\examples\getting-started\Dockerfile: adding watch for C:\Users\kaitoy\Desktop\skaffold\examples\getting-started\Dockerfile: The parameter is incorrect.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;MinGW(Git Bash)でやっても同じ結果。
&lt;a href=&#34;https://github.com/GoogleCloudPlatform/skaffold/issues/287&#34;&gt;Issuesに登録されているやつ&lt;/a&gt;と同じ問題っぽい。&lt;/p&gt;

&lt;p&gt;対応を待つしかない。&lt;/p&gt;

&lt;h2 id=&#34;linux版を試す&#34;&gt;Linux版を試す&lt;/h2&gt;

&lt;p&gt;Linux版も試してみる。
minikubeのVMがLinux(Boot2Docker)なので、そこで動かす。
&lt;a href=&#34;https://ja.wikipedia.org/wiki/Windows_Subsystem_for_Linux&#34;&gt;WSL&lt;/a&gt;は試さない。
会社のPCがWindows 7で使えないので。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;SkaffoldのLinux版バイナリをダウンロードしてskaffoldにリネームして、minikubeのBoot2DockerにSSHでログインして、PATHの通ったところに置く。
因みにminikubeのBoot2Dockerは、ユーザdockerパスワードtcuserでログインできる。&lt;/p&gt;

&lt;p&gt;kubectlのLinux版バイナリもダウンロードしてPATHに入れたら準備完了。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;examples/getting-started&lt;/code&gt;にcdして&lt;code&gt;skaffold dev&lt;/code&gt;したらエラー。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ERRO[0001] run: getting skaffold config: getting k8s client: Error creating kubeConfig: invalid configuration: no configuration has been provided
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ちょっと調べたら、kubectlのコンテクストが設定されていないのがだめっぽい。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# kubectl config current-context
error: current-context is not set
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Windows上のkubectlに設定されたコンテクストを参考に、以下の内容を&lt;code&gt;~/.kube/config&lt;/code&gt;に。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1
clusters:
- cluster:
    certificate-authority: /c/Users/kaitoy/.minikube/ca.crt
    server: https://localhost:8443
  name: minikube
contexts:
- context:
    cluster: minikube
    user: minikube
  name: minikube
current-context: minikube
kind: Config
preferences: {}
users:
- name: minikube
  user:
    client-certificate: /c/Users/kaitoy/.minikube/client.crt
    client-key: /c/Users/kaitoy/.minikube/client.key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;再度&lt;code&gt;skaffold dev&lt;/code&gt;したら違うエラー。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WARN[0002] run: build: build step: running build: read auth configs: docker config: opening docker config: open /home/docker/.docker/config.json: no such file or directory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;.docker/config.json&lt;/code&gt;は&lt;code&gt;docker login&lt;/code&gt;すると生成されるものらしい。
SkaffoldのREADME.mdにはminikube使うならDocker image registry要らないって書いてあるんだけど…&lt;/p&gt;

&lt;p&gt;色々あって、ファイルがあればいいだけっぽいので、以下で良し。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# echo {} &amp;gt; ~/.docker/config.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;再度&lt;code&gt;skaffold dev&lt;/code&gt;したら動いた。
Dockerビルドが走り、minikubeにPodがデプロイされた。&lt;/p&gt;

&lt;p&gt;Getting Startedのサンプルは、一秒ごとに「[getting-started] Hello world!」というメッセージをコンソールに表示する。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;examples/getting-started/main.go&lt;/code&gt;の&lt;code&gt;fmt.Println(&amp;quot;Hello world!&amp;quot;)&lt;/code&gt;のとこをいじってメッセージを変えたら、自動で再Dockerビルドしてデプロイされて、新しいメッセージを表示し始めた。
便利。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;examples/getting-started/skaffold.yaml&lt;/code&gt;がSkaffoldの設定ファイルで、ここに定義されたKubernetesマニフェストをデプロイしてくれるっぽい。
watchするファイルはどう決めているんだろうか。
Dockerfileとmain.goはwatchしてるけど、新しいファイルを作ってもDockerビルド走らなかった。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;ひとつ問題は、Linuxファイルシステム上で編集しないと変更を検知してくれない。&lt;/p&gt;

&lt;p&gt;minikubeのVMには&lt;code&gt;C:\Users&lt;/code&gt;がマウントされてるので、最初はWindows上にcloneしたサンプルをSkaffoldで実行しつつ、Windows上でmain.goを編集してみたんだけど、それだとダメだった。&lt;/p&gt;

&lt;p&gt;やはりWindows版Skaffoldの修正が待たれる。&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Kubernetes 1.8のアクセス制御について。あとDashboard。</title>
          <link>https://www.kaitoy.xyz/2017/10/31/retry-dashboard-on-k8s-cluster-by-kubeadm/</link>
          <pubDate>Tue, 31 Oct 2017 16:57:04 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2017/10/31/retry-dashboard-on-k8s-cluster-by-kubeadm/</guid>
          <description>

&lt;p&gt;「&lt;a href=&#34;https://www.kaitoy.xyz/2017/10/21/build-kubernetes-cluster-by-kubeadm/&#34;&gt;Kubernetes1.8のクラスタを構築する。kubeadmで。&lt;/a&gt;」で、Dashboardがうまく動かない問題が発生したんだけど、それを解決した話。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h2 id=&#34;問題の現象&#34;&gt;問題の現象&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/admin/kubeadm/&#34;&gt;kubeadm&lt;/a&gt;でKubernetesクラスタを組んで、自前のアプリ(&lt;a href=&#34;https://www.kaitoy.xyz/2016/12/11/goslings-development-memo0-intro-design/&#34;&gt;Goslings&lt;/a&gt;)のデプロイまではうまくできたんだけど、&lt;a href=&#34;https://github.com/kubernetes/dashboard&#34;&gt;Dashboard&lt;/a&gt;をデプロイしたら動かず、Web UIに&lt;code&gt;kubectl proxy&lt;/code&gt;経由でつないでもタイムアウトしてしまった。&lt;/p&gt;

&lt;h2 id=&#34;対策&#34;&gt;対策&lt;/h2&gt;

&lt;p&gt;なんとなく、クラスタ内部での名前解決には&lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns&#34;&gt;kube-dns&lt;/a&gt;によるDNSサービスが使われているっぽいので、&lt;code&gt;/etc/hosts&lt;/code&gt;に余計な事書いたのがいけなかったと思った。&lt;/p&gt;

&lt;p&gt;ので、&lt;code&gt;/etc/hosts&lt;/code&gt;からk8s-masterとk8s-nodeのエントリを削除してから、&lt;code&gt;kubeadm init&lt;/code&gt;からやり直してみた。&lt;/p&gt;

&lt;h2 id=&#34;結果&#34;&gt;結果&lt;/h2&gt;

&lt;p&gt;したらちゃんと動いた。&lt;/p&gt;

&lt;p&gt;VMのホストで&lt;code&gt;kubectl proxy&lt;/code&gt;して、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy\Desktop&amp;gt;kubectl proxy
Starting to serve on 127.0.0.1:8001
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/&lt;/code&gt;にブラウザでつないだらサインイン画面が表示された。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/retry-dashboard-on-k8s-cluster-by-kubeadm/dashboard.png&#34; alt=&#34;dashboard&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Dashboardのサインイン処理はKubernetes(というかkube-apiserver)のそれに移譲している。
Dashboardはそこで認証されたユーザでクラスタのリソースにアクセスし、情報を取得して表示する。多分。&lt;/p&gt;

&lt;p&gt;Dashboardへのサインイン方法は&lt;a href=&#34;https://github.com/kubernetes/dashboard/wiki/Access-control&#34;&gt;いくつかある&lt;/a&gt;が、それらを理解するにはKubernetesのアクセス制御について学ぶことを推奨とあったのでちょっと&lt;a href=&#34;https://kubernetes.io/docs/admin/accessing-the-api/&#34;&gt;Kubernetesのドキュメント&lt;/a&gt;を読んだ。&lt;/p&gt;

&lt;h2 id=&#34;kubernetesのアクセス制御&#34;&gt;Kubernetesのアクセス制御&lt;/h2&gt;

&lt;p&gt;Kubernetesクラスタのエンドポイントはkube-apiserverであり、クラスタのリソースへのアクセス制御もkube-apiserverがやる。
クライアントとkube-apiserverとのTLSセッションが確立した後、HTTP層のデータを見てアクセス制御をするんだけど、その処理は&lt;a href=&#34;https://kubernetes.io/docs/admin/authentication/&#34;&gt;Authentication&lt;/a&gt;(認証)、&lt;a href=&#34;https://kubernetes.io/docs/admin/authorization/&#34;&gt;Authorization&lt;/a&gt;(認可)、&lt;a href=&#34;https://kubernetes.io/docs/admin/admission-controllers/&#34;&gt;Admission&lt;/a&gt;(許可)の三段階からなる。&lt;/p&gt;

&lt;h3 id=&#34;authentication&#34;&gt;Authentication&lt;/h3&gt;

&lt;p&gt;第一段階がAuthentication。
ここでは、kube-apiserverに仕込まれたAuthenticatorモジュールがユーザ認証をする。&lt;/p&gt;

&lt;p&gt;Kubernetesが認証するユーザには、Kubernetesが管理するService Accountと、クラスタ外部で管理される通常ユーザの二通りがある。
Service AccountはPodがkube-apiserverと話すためのユーザで、通常ユーザは主に人がkubectlとかでkube-apiserverと話すためのユーザ。(匿名で話すこともできる。)
前者はServiceAccountオブジェクトで定義されるけど、後者用のオブジェクトはない。&lt;/p&gt;

&lt;p&gt;ServiceAccountはNamespaceと関連付き(つまりnamespace毎にユニーク)、Secretに紐づく。
Secretオブジェクトはクレデンシャルのセットを定義し、Podにマウントされる。
ServiceAccountとSecretは、ふつうは自動で作られ、Podに割り当てられる。&lt;/p&gt;

&lt;p&gt;kube-apiserverには一つ以上のAuthenticatorモジュールを設定できて、どれかで認証できれば次の段階に進める。
認証失敗するとHTTPステータスコード401が返る。&lt;/p&gt;

&lt;p&gt;Authenticatorモジュールには以下のようなものがある。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/admin/authentication/#x509-client-certs&#34;&gt;クライアント証明書&lt;/a&gt;: X.509のディジタル証明書を使うモジュール。kube-apiserver起動時に&lt;code&gt;--client-ca-file&lt;/code&gt;オプションで証明書ファイルを渡してやると有効になる。証明書のCommon Nameがユーザ名になり、Organizationがグループになる。クライアント側は、その証明書と対応する秘密鍵をクレデンシャルとして指定する。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/admin/authentication/#putting-a-bearer-token-in-a-request&#34;&gt;Bearer Token&lt;/a&gt;: 無記名トークンを使うモジュール。kube-apiserver起動時に&lt;code&gt;--token-auth-file&lt;/code&gt;オプションでトークン情報を渡してやると有効になる。トークン情報はCSVで、「&lt;code&gt;token,user,uid,&amp;quot;group1,group2,group3&amp;quot;&lt;/code&gt;」という形式で書く。クライアント側は、トークン文字列をクレデンシャルとして指定する。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/admin/authentication/#static-password-file&#34;&gt;ベーシック認証&lt;/a&gt;: ユーザ名とパスワードで認証するモジュール。kube-apiserver起動時に&lt;code&gt;--basic-auth-file&lt;/code&gt;オプションでユーザ名とパスワードのリストを渡してやると有効になる。このリストはCSVで、「&lt;code&gt;password,user,uid,&amp;quot;group1,group2,group3&amp;quot;&lt;/code&gt;」という形式で書く。クライアント側は、ユーザ名とパスワードをクレデンシャルとして指定する。HTTPクライアントの時はAuthorizationヘッダが使える。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/admin/authentication/#service-account-tokens&#34;&gt;Service Account Token&lt;/a&gt;: Service Accountを署名付きBearer Tokenで認証するモジュール。デフォルトで有効になる。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;このあたり、Qiitaの「&lt;a href=&#34;https://qiita.com/hiyosi/items/43465d4fc501c2044d01#x509-client-certs&#34;&gt;kubernetesがサポートする認証方法の全パターンを動かす&lt;/a&gt;」という記事をみると理解が深まる。&lt;/p&gt;

&lt;h3 id=&#34;authorization&#34;&gt;Authorization&lt;/h3&gt;

&lt;p&gt;Authenticationをパスすると、クライアントのユーザ(とグループ)が認証され、第二段階のAuthorizationモジュールの処理に移る。
ここでは、リクエストの内容(操作対象、操作種別(メソッド)等)を見て、それがユーザに許されたものなら認可する。
何を許すかは事前にクラスタにポリシーを定義しておく。&lt;/p&gt;

&lt;p&gt;kube-apiserver起動時に&lt;code&gt;--authorization-mode&lt;/code&gt;オプションで一つ以上のAuthenticatorモジュールを指定できて、どれかで認可されれば次の段階に進める。
さもなくばHTTPステータスコード403が返る。&lt;/p&gt;

&lt;p&gt;Authorizationモジュールには以下のようなものがある。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/admin/authorization/node/&#34;&gt;Node&lt;/a&gt;: kubeletからのリクエストを認可する。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/admin/authorization/abac/&#34;&gt;ABAC Mode&lt;/a&gt;: Attribute-based Access Control。リクエストに含まれる属性とPolicyオブジェクトを比較して、マッチするものがあれば認可。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/admin/authorization/rbac/&#34;&gt;RBAC Mode&lt;/a&gt;: Role-Based Access Control。RoleオブジェクトやClusterRoleオブジェクトでロールを作成し、アクセスできるリソースや許可する操作を定義して、RoleBindingオブジェクトやClusterRoleBindingオブジェクトでユーザ名やグループと紐づける。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/admin/authorization/webhook/&#34;&gt;Webhook Mode&lt;/a&gt;: リクエストの内容を示すSubjectAccessReviewオブジェクトをシリアライズしたJSONデータをHTTPでPOSTして、そのレスポンスによって認可可否を決める。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;admission-control&#34;&gt;Admission Control&lt;/h3&gt;

&lt;p&gt;Authorizationをパスすると、第三段階のAdmission Controlモジュールの処理に移る。
ここでは、オブジェクトの作成、削除、更新などのリクエストをインターセプトして、オブジェクトの永続化前にそのオブジェクトを確認して、永続化を許可するかを決める。
リクエストされたオブジェクトやそれに関連するオブジェクトを永続化前にいじって、デフォルト値を設定したりもできる。
読み取りリクエストの場合は実行されない。&lt;/p&gt;

&lt;p&gt;kube-apiserver起動時に&lt;code&gt;--admission-control&lt;/code&gt;オプションで複数のAdmission Controlモジュールを指定できて、全てが許可しないとリクエストが却下される。&lt;/p&gt;

&lt;p&gt;Admission Controlモジュールは色々あるんだけど、Kubernetes 1.6以降では&lt;code&gt;--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds&lt;/code&gt;と指定するのが強く推奨されている。
ここで指定している&lt;a href=&#34;https://kubernetes.io/docs/admin/admission-controllers/#serviceaccount&#34;&gt;ServiceAccountモジュール&lt;/a&gt;は、kube-controller-managerに含まれるServiceAccountControllerとTokenControllerと協調し、Service Account周りの処理を&lt;a href=&#34;https://kubernetes.io/docs/admin/service-accounts-admin/#service-account-automation&#34;&gt;自動化&lt;/a&gt;してくれるもの。&lt;/p&gt;

&lt;p&gt;ServiceAccountControllerは、各Namespaceに&lt;code&gt;default&lt;/code&gt;という名前のService Accountを作る。&lt;/p&gt;

&lt;p&gt;ServiceAccountが作成されるとTokenControllerが動き、対応したSecretとトークンを生成して紐づける。&lt;/p&gt;

&lt;p&gt;ServiceAccountモジュールは、Podの作成や更新時に動き、以下の処理をする。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;PodにServiceAccountが設定されていなければ、&lt;code&gt;default&lt;/code&gt;を設定する。&lt;/li&gt;
&lt;li&gt;Podに設定されたServiceAccountが存在していることを確認し、存在していなければリクエストを却下する。&lt;/li&gt;
&lt;li&gt;PodがImagePullSecretsを含んでいなければ、ServiceAccountのImagePullSecretsをPodに追加する。&lt;/li&gt;
&lt;li&gt;トークンを含んだVolumeをPodに追加する。&lt;/li&gt;
&lt;li&gt;Pod内の各コンテナの&lt;code&gt;/var/run/secrets/kubernetes.io/serviceaccount&lt;/code&gt;にそのVolumeをマウントさせる。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;dashboardへbearer-tokenでサインイン&#34;&gt;DashboardへBearer Tokenでサインイン&lt;/h2&gt;

&lt;p&gt;Dashboardの話に戻る。
とりあえず&lt;a href=&#34;https://github.com/kubernetes/dashboard/wiki/Access-control#bearer-token&#34;&gt;Bearer Tokenでのサインイン&lt;/a&gt;を試す。&lt;/p&gt;

&lt;p&gt;クラスタにはデフォルトで色んなService Accountが作られていて、異なる権限を持っている。
そのいずれかのSecretのTokenを使ってDashboardへサインインできるらしい。&lt;/p&gt;

&lt;p&gt;以下のコマンドで&lt;code&gt;kube-system&lt;/code&gt;というNamespaceのSecretを一覧できる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl -n kube-system get secret
NAME                                     TYPE                                  DATA      AGE
attachdetach-controller-token-skzmj      kubernetes.io/service-account-token   3         18m
bootstrap-signer-token-mhqfh             kubernetes.io/service-account-token   3         18m
bootstrap-token-2964e0                   bootstrap.kubernetes.io/token         7         18m
certificate-controller-token-fvrgm       kubernetes.io/service-account-token   3         18m
cronjob-controller-token-hmrdm           kubernetes.io/service-account-token   3         18m
daemon-set-controller-token-vqz85        kubernetes.io/service-account-token   3         18m
default-token-h987g                      kubernetes.io/service-account-token   3         18m
deployment-controller-token-86bp9        kubernetes.io/service-account-token   3         18m
disruption-controller-token-6mskg        kubernetes.io/service-account-token   3         18m
endpoint-controller-token-d4wz6          kubernetes.io/service-account-token   3         18m
generic-garbage-collector-token-smfgq    kubernetes.io/service-account-token   3         18m
horizontal-pod-autoscaler-token-wsbn9    kubernetes.io/service-account-token   3         18m
job-controller-token-fttt2               kubernetes.io/service-account-token   3         18m
kube-dns-token-sn5qq                     kubernetes.io/service-account-token   3         18m
kube-proxy-token-w96xd                   kubernetes.io/service-account-token   3         18m
kubernetes-dashboard-certs               Opaque                                2         7m
kubernetes-dashboard-key-holder          Opaque                                2         6m
kubernetes-dashboard-token-gtppc         kubernetes.io/service-account-token   3         7m
namespace-controller-token-5kksd         kubernetes.io/service-account-token   3         18m
node-controller-token-chpwt              kubernetes.io/service-account-token   3         18m
persistent-volume-binder-token-d5x49     kubernetes.io/service-account-token   3         18m
pod-garbage-collector-token-l8sct        kubernetes.io/service-account-token   3         18m
replicaset-controller-token-njjwr        kubernetes.io/service-account-token   3         18m
replication-controller-token-qrr5h       kubernetes.io/service-account-token   3         18m
resourcequota-controller-token-dznjm     kubernetes.io/service-account-token   3         18m
service-account-controller-token-99nh8   kubernetes.io/service-account-token   3         18m
service-controller-token-9cw7k           kubernetes.io/service-account-token   3         18m
statefulset-controller-token-8z8w9       kubernetes.io/service-account-token   3         18m
token-cleaner-token-cxbkc                kubernetes.io/service-account-token   3         18m
ttl-controller-token-k7gh7               kubernetes.io/service-account-token   3         18m
weave-net-token-lqdgm                    kubernetes.io/service-account-token   3         17m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で、適当にそれっぽいSecret、&lt;code&gt;deployment-controller-token-86bp9&lt;/code&gt;を選んで、&lt;code&gt;kubectl describe&lt;/code&gt;したらTokenが見れた。
(Dataセクションの&lt;code&gt;token&lt;/code&gt;のとこ。)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl -n kube-system describe secret deployment-controller-token-86bp9
Name:         deployment-controller-token-86bp9
Namespace:    kube-system
Labels:       &amp;lt;none&amp;gt;
Annotations:  kubernetes.io/service-account.name=deployment-controller
              kubernetes.io/service-account.uid=17fc5207-b627-11e7-9867-000c2938deae

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZXBsb3ltZW50LWNvbnRyb2xsZXItdG9rZW4tODZicDkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVwbG95bWVudC1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMTdmYzUyMDctYjYyNy0xMWU3LTk4NjctMDAwYzI5MzhkZWFlIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRlcGxveW1lbnQtY29udHJvbGxlciJ9.ZGV9XDd-GQjAwRuLKpdsWL_dTeF0Mr_2gF117OW4BhEuLwPujnsfOuysAQ-DUtNOp1NHKGitlfxjh6fKo4tFsdwLVJWrRK6i4YH1Mm2No7Sheks7IQn1FnwSmr7yCuvjlHD2e4RpZH0wupOFoY7FHntilhOWbXTJzJzi7TozLX02EKbkVGAsvch3LZ6p8jmUH5hr8DdKc4jbmTRp86SOiFS4_-TJ3RtAHCxiioAuKzXm3-rAWdeGLLcKrM2pAFSAGaBNu8MO5BZlAi6h3Xt4x-8-1ZXs4mudtJiECvjB-XIwiwzhpq8wIPZvvQQ-f1khixOyk1RfIXRJhIE5Gqvi8g
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サインイン画面でTokenを選択し、
この、&lt;code&gt;eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZXBsb3ltZW50LWNvbnRyb2xsZXItdG9rZW4tODZicDkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVwbG95bWVudC1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMTdmYzUyMDctYjYyNy0xMWU3LTk4NjctMDAwYzI5MzhkZWFlIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRlcGxveW1lbnQtY29udHJvbGxlciJ9.ZGV9XDd-GQjAwRuLKpdsWL_dTeF0Mr_2gF117OW4BhEuLwPujnsfOuysAQ-DUtNOp1NHKGitlfxjh6fKo4tFsdwLVJWrRK6i4YH1Mm2No7Sheks7IQn1FnwSmr7yCuvjlHD2e4RpZH0wupOFoY7FHntilhOWbXTJzJzi7TozLX02EKbkVGAsvch3LZ6p8jmUH5hr8DdKc4jbmTRp86SOiFS4_-TJ3RtAHCxiioAuKzXm3-rAWdeGLLcKrM2pAFSAGaBNu8MO5BZlAi6h3Xt4x-8-1ZXs4mudtJiECvjB-XIwiwzhpq8wIPZvvQQ-f1khixOyk1RfIXRJhIE5Gqvi8g&lt;/code&gt;を入力したらサインインできて、GoslingsのDeploymentの情報が見れた。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/retry-dashboard-on-k8s-cluster-by-kubeadm/deploy.png&#34; alt=&#34;deploy&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Podも見れる。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/retry-dashboard-on-k8s-cluster-by-kubeadm/pods.png&#34; alt=&#34;pods&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;けどServiceは見れない。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/retry-dashboard-on-k8s-cluster-by-kubeadm/service.png&#34; alt=&#34;service&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;各画面でオレンジ色のワーニングも出ていて、&lt;code&gt;deployment-controller&lt;/code&gt;ユーザで見れる範囲はあまり広くないことが分かる。&lt;/p&gt;

&lt;h2 id=&#34;dashboardへadmin権限でサインイン&#34;&gt;DashboardへAdmin権限でサインイン&lt;/h2&gt;

&lt;p&gt;DashboardのPodのService Accountである&lt;code&gt;kubernetes-dashboard&lt;/code&gt;にAdmin権限を付けてやって、サインイン画面でSKIPを押すとなんでも見れるようになる。セキュリティリスクがあるので本番ではNG設定だけど。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;cluster-admin&lt;/code&gt;というClusterRoleがあって、これを&lt;code&gt;kubernetes-dashboard&lt;/code&gt;にバインドするClusterRoleBindingを作ってやればいい。&lt;/p&gt;

&lt;p&gt;ので、以下のようなYAMLファイルを書いて、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yml&#34;&gt;apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard
  labels:
    k8s-app: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: kubernetes-dashboard
  namespace: kube-system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;kubectl&lt;/code&gt;で投げる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy\Desktop&amp;gt;kubectl create -f dashboard-admin.yml
clusterrolebinding &amp;quot;kubernetes-dashboard&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;したらServiceも見えるようになった。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/retry-dashboard-on-k8s-cluster-by-kubeadm/service-admin.png&#34; alt=&#34;service-admin&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;ついでにHWリソース情報も見れた。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/retry-dashboard-on-k8s-cluster-by-kubeadm/resources.png&#34; alt=&#34;resources&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;満足した。&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Kubernetes1.8のクラスタを構築する。kubeadmで。</title>
          <link>https://www.kaitoy.xyz/2017/10/21/build-kubernetes-cluster-by-kubeadm/</link>
          <pubDate>Sat, 21 Oct 2017 10:42:46 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2017/10/21/build-kubernetes-cluster-by-kubeadm/</guid>
          <description>

&lt;p&gt;「&lt;a href=&#34;https://www.kaitoy.xyz/2017/10/10/goslings-on-kubernetes/&#34;&gt;Kubernetes 1.8が出たので、Minikubeを触ってみる&lt;/a&gt;」でMinikubeをやったんだけど、もう一歩ステップアップすべく、&lt;a href=&#34;https://kubernetes.io/docs/admin/kubeadm/&#34;&gt;kubeadm&lt;/a&gt;でKubernetesクラスタを組んでみた話。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h2 id=&#34;kubeadmとは&#34;&gt;kubeadmとは&lt;/h2&gt;

&lt;p&gt;kubeadm(キューブアダム)はKubernetesに含まれるコマンドで、Kubernetesクラスタを簡単に構築するツール。
Kubernetes 1.4で追加され、Kubernetes 1.8の時点でまだベータで、本番環境には使わないでとなっている。
Qiitaの「&lt;a href=&#34;https://qiita.com/helix_kaz/items/9c4a83532f949d8a94ef&#34;&gt;kubeadmが何をやっているのかみてみた&lt;/a&gt;」という記事が、中でどんな動作をしてるかを解説していて参考になる。&lt;/p&gt;

&lt;p&gt;コマンドの使用感からすると、&lt;a href=&#34;https://docs.docker.com/engine/swarm/&#34;&gt;DockerのSwarmモード&lt;/a&gt;でのクラスタ構築の容易さをKubernetesに取り込むことを目指して開発されている気がした。&lt;/p&gt;

&lt;p&gt;ネットで見かけた評判だと、確かに簡単にクラスタ構築できて素晴らしいけど、TLSの証明書生成など、細かく制御できなくて困るところがあって、やはり本番に使えるレベルではないとのこと。&lt;/p&gt;

&lt;p&gt;まあとにかく試してみる価値はあろう。&lt;/p&gt;

&lt;h2 id=&#34;kubeadmインストール&#34;&gt;kubeadmインストール&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/setup/independent/install-kubeadm/&#34;&gt;Kubernetesのドキュメント&lt;/a&gt;に従ってkubeadmをインストールする。
バージョンは最新版の1.8.1。&lt;/p&gt;

&lt;h3 id=&#34;vm作成&#34;&gt;VM作成&lt;/h3&gt;

&lt;p&gt;kubeadmのサポートOSは、Ubuntu 16.04+、Debian 9、CentOS 7、RHEL 7、Fedora 25/26、HypriotOS v1.0.1+となっている。
慣れているCentOS 7を使うことにする。
(HypriotOSってなんだろう?)&lt;/p&gt;

&lt;p&gt;自前のノートPCのWindows 10 x64 Home Edition上のVMware Player 12のVMにCentOS 7を入れた。
メモリは1GB以上が要件なので、味を付けて1.4GBで。
VM間で通信できることって要件があったけど、インターネット接続も必要なはずなので、NICはNATのやつで。&lt;/p&gt;

&lt;p&gt;このVMはMasterになる。&lt;/p&gt;

&lt;h3 id=&#34;os設定&#34;&gt;OS設定&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports&#34;&gt;Kubernetesが使うポート&lt;/a&gt;をいろいろ開けなければいけないんだけど、めんどいのでfirewalldを無効にする。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@localhost ~]# systemctl stop firewalld
[root@localhost ~]# systemctl disable firewalld
Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.
Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;なんとなくIPアドレスをDHCPから静的割り当てに。(192.168.171.200)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# nmcli c modify ens33 ipv4.method manual
[root@k8s-master ~]# nmcli c modify ens33 ipv4.addresses 192.168.171.200/24
[root@k8s-master ~]# nmcli c modify ens33 ipv4.dns 192.168.171.2
[root@k8s-master ~]# nmcli c modify ens33 ipv4.gateway 192.168.171.2
[root@k8s-master ~]# systemctl restart network
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;ホスト名をlocalhost.localdomainからk8s-masterに変更。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@localhost ~]# hostnamectl set-hostname k8s-master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ログアウトログインで反映。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;/etc/hosts&lt;/code&gt;を編集して、k8s-masterのエントリを追加。
あとで作るもう一つのVM、k8s-nodeのほうもエントリを追加。
(これはだめだったっぽい。詳細は後述。)&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;クラスタを構成するノードは、一意のMACアドレスとproduct_uuidを持っていないといけない。
Kubernetesがそれらでクラスタ内のノードを区別してるので。&lt;/p&gt;

&lt;p&gt;MACアドレスは&lt;code&gt;ip link&lt;/code&gt;コマンドなどで確認できる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# ip link
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: ens33: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether 00:0c:29:38:de:ae brd ff:ff:ff:ff:ff:ff
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;product_uuidは、&lt;a href=&#34;https://ja.wikipedia.org/wiki/SMBIOS&#34;&gt;SMBIOS&lt;/a&gt;という、PC固有のデータを保存・参照するための仕様があって、それに従って保存されたシステムの識別子らしい。
product_uuidは&lt;code&gt;dmidecode&lt;/code&gt;コマンドなどで確認できる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# dmidecode -s system-uuid
58114D56-A744-3610-C3C5-9B15A838DEAE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;kubeletがちゃんと動くためにはswapを無効にする必要がある。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# swapoff -a
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(このコマンドはよくなかった。詳細は後述。)&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;ebtablesとethtoolを入れる必要がある。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# yum install -y ebtables ethtool
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Dockerも入れないと。
v1.12が推奨で、v1.11かv1.13でもいい。
適当に入れたらv1.12.6だった。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# yum install -y docker
[root@k8s-master ~]# systemctl enable docker &amp;amp;&amp;amp; systemctl start docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Podネットワークなどが機能する要件として、コンテナがホストファイルシステムにアクセスできる必要があるが、そのためには現状、SELinuxを無効化する必要がある。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# setenforce 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(このコマンドもよくなかった。詳細は後述。)&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;RHEL系の場合、iptablesがバイパスされてトラフィックが変にルーティングされる問題があるため、&lt;code&gt;net.bridge.bridge-nf-call-iptables&lt;/code&gt;を1にセットしておく必要がある。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# cat &amp;lt;&amp;lt;EOF &amp;gt;  /etc/sysctl.d/k8s.conf
&amp;gt; net.bridge.bridge-nf-call-ip6tables = 1
&amp;gt; net.bridge.bridge-nf-call-iptables = 1
&amp;gt; EOF
[root@k8s-master ~]# sysctl --system
* Applying /usr/lib/sysctl.d/00-system.conf ...
net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0
* Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...
kernel.yama.ptrace_scope = 0
* Applying /usr/lib/sysctl.d/50-default.conf ...
kernel.sysrq = 16
kernel.core_uses_pid = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.conf.default.promote_secondaries = 1
net.ipv4.conf.all.promote_secondaries = 1
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
* Applying /usr/lib/sysctl.d/99-docker.conf ...
fs.may_detach_mounts = 1
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /etc/sysctl.d/k8s.conf ...
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
* Applying /etc/sysctl.conf ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Cgroup Driverを、Dockerとkubeletとの間で一致させておく必要がある。
以下のようにして確認できる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf | grep KUBELET_CGROUP_ARGS
Environment=&amp;quot;KUBELET_CGROUP_ARGS=--cgroup-driver=systemd&amp;quot;
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_CADVISOR_ARGS $KUBELET_CGROUP_ARGS $KUBELET_CERTIFICATE_ARGS $KUBELET_EXTRA_ARGS
[root@k8s-master ~]# docker info |grep -i cgroup
 WARNING: Usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.
Cgroup Driver: systemd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;どっちもsystemdだったので問題なし。
(違ってたら&lt;a href=&#34;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#troubleshooting&#34;&gt;&lt;code&gt;KUBELET_CGROUP_ARGS&lt;/code&gt;を変更する必要がある&lt;/a&gt;。)&lt;/p&gt;

&lt;h3 id=&#34;kubelet-kubeadm-kubectlインストール&#34;&gt;kubelet、kubeadm、kubectlインストール&lt;/h3&gt;

&lt;p&gt;ここでやっとkubeadmのインストール。
kubeletとkubectlも一緒にインストールする。&lt;/p&gt;

&lt;p&gt;まずYUMリポジトリを追加して、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
&amp;gt; [kubernetes]
&amp;gt; name=Kubernetes
&amp;gt; baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
&amp;gt; enabled=1
&amp;gt; gpgcheck=1
&amp;gt; repo_gpgcheck=1
&amp;gt; gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
&amp;gt;         https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
&amp;gt; EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;インストール。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# yum install kubelet kubeadm kubectl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で、kubeletをサービス登録。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# systemctl enable kubelet &amp;amp;&amp;amp; systemctl start kubelet
Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /etc/systemd/system/kubelet.service.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;ここでVMのスナップショットをとっておいて、後でNodeを追加するときに使う。&lt;/p&gt;

&lt;h2 id=&#34;master構築&#34;&gt;Master構築&lt;/h2&gt;

&lt;p&gt;Masterは&lt;code&gt;kubeadm init&lt;/code&gt;で構築できる。
&lt;code&gt;--apiserver-advertise-address&lt;/code&gt;でkube-apiserverがlistenするIPアドレスを指定すべし。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# kubeadm init --apiserver-advertise-address=192.168.171.200
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.8.1
[init] Using Authorization modes: [Node RBAC]
[preflight] Running pre-flight checks
[preflight] WARNING: Running with swap on is not supported. Please disable swap or set kubelet&#39;s --fail-swap-on flag to false.
[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.171.200]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Valid certificates and keys now exist in &amp;quot;/etc/kubernetes/pki&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;admin.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;kubelet.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;controller-manager.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;scheduler.conf&amp;quot;
[controlplane] Wrote Static Pod manifest for component kube-apiserver to &amp;quot;/etc/kubernetes/manifests/kube-apiserver.yaml&amp;quot;
[controlplane] Wrote Static Pod manifest for component kube-controller-manager to &amp;quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&amp;quot;
[controlplane] Wrote Static Pod manifest for component kube-scheduler to &amp;quot;/etc/kubernetes/manifests/kube-scheduler.yaml&amp;quot;
[etcd] Wrote Static Pod manifest for a local etcd instance to &amp;quot;/etc/kubernetes/manifests/etcd.yaml&amp;quot;
[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory &amp;quot;/etc/kubernetes/manifests&amp;quot;
[init] This often takes around a minute; or longer if the control plane images have to be pulled.
[kubelet-check] It seems like the kubelet isn&#39;t running or healthy.
[kubelet-check] The HTTP call equal to &#39;curl -sSL http://localhost:10255/healthz&#39; failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.
[kubelet-check] It seems like the kubelet isn&#39;t running or healthy.
[kubelet-check] The HTTP call equal to &#39;curl -sSL http://localhost:10255/healthz&#39; failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.
[kubelet-check] It seems like the kubelet isn&#39;t running or healthy.
[kubelet-check] The HTTP call equal to &#39;curl -sSL http://localhost:10255/healthz&#39; failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.
[kubelet-check] It seems like the kubelet isn&#39;t running or healthy.
[kubelet-check] The HTTP call equal to &#39;curl -sSL http://localhost:10255/healthz/syncloop&#39; failed with error: Get http://localhost:10255/healthz/syncloop: dial tcp [::1]:10255: getsockopt: connection refused.
[kubelet-check] It seems like the kubelet isn&#39;t running or healthy.
[kubelet-check] The HTTP call equal to &#39;curl -sSL http://localhost:10255/healthz/syncloop&#39; failed with error: Get http://localhost:10255/healthz/syncloop: dial tcp [::1]:10255: getsockopt: connection refused.
[kubelet-check] It seems like the kubelet isn&#39;t running or healthy.
[kubelet-check] The HTTP call equal to &#39;curl -sSL http://localhost:10255/healthz/syncloop&#39; failed with error: Get http://localhost:10255/healthz/syncloop: dial tcp [::1]:10255: getsockopt: connection refused.
[kubelet-check] It seems like the kubelet isn&#39;t running or healthy.
[kubelet-check] The HTTP call equal to &#39;curl -sSL http://localhost:10255/healthz&#39; failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.
[kubelet-check] It seems like the kubelet isn&#39;t running or healthy.
[kubelet-check] The HTTP call equal to &#39;curl -sSL http://localhost:10255/healthz/syncloop&#39; failed with error: Get http://localhost:10255/healthz/syncloop: dial tcp [::1]:10255: getsockopt: connection refused.
[kubelet-check] It seems like the kubelet isn&#39;t running or healthy.
[kubelet-check] The HTTP call equal to &#39;curl -sSL http://localhost:10255/healthz&#39; failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: getsockopt: connection refused.

Unfortunately, an error has occurred:
        timed out waiting for the condition

This error is likely caused by that:
        - The kubelet is not running
        - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)
        - There is no internet connection; so the kubelet can&#39;t pull the following control plane images:
                - gcr.io/google_containers/kube-apiserver-amd64:v1.8.1
                - gcr.io/google_containers/kube-controller-manager-amd64:v1.8.1
                - gcr.io/google_containers/kube-scheduler-amd64:v1.8.1

You can troubleshoot this for example with the following commands if you&#39;re on a systemd-powered system:
        - &#39;systemctl status kubelet&#39;
        - &#39;journalctl -xeu kubelet&#39;
couldn&#39;t initialize a Kubernetes cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;なんか失敗した。
&lt;code&gt;getsockopt: connection refused.&lt;/code&gt;ってのがたくさん出てる。
ググると、swapがあやしい。
確認してみたら、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# swapon -s
Filename                                Type            Size    Used    Priority
/dev/dm-1                               partition       2097148 0       -1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;無効になってない。
&lt;code&gt;swapoff -a&lt;/code&gt;でswap無効にしても、OS再起動したらもとに戻ってしまうのか。&lt;/p&gt;

&lt;p&gt;永続的に無効にするため、&lt;code&gt;/etc/fstab&lt;/code&gt;を編集して、以下の行を削除した。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/dev/mapper/centos-swap swap                    swap    defaults        0 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で、OSリブート。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;kubeadm initをやり直す前に、いったん&lt;code&gt;kubeadm reset&lt;/code&gt;して初期化する。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# kubeadm reset
[preflight] Running pre-flight checks
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in &amp;quot;/var/lib/kubelet&amp;quot;
[reset] Removing kubernetes-managed containers
[reset] Deleting contents of stateful directories: [/var/lib/kubelet /etc/cni/net.d /var/lib/dockershim /var/run/kubernetes /var/lib/etcd]
[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;2回目の&lt;code&gt;kubeadm init&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# kubeadm init --apiserver-advertise-address=192.168.171.200
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.8.1
[init] Using Authorization modes: [Node RBAC]
[preflight] Running pre-flight checks
[preflight] Starting the kubelet service
[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.171.200]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Valid certificates and keys now exist in &amp;quot;/etc/kubernetes/pki&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;admin.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;kubelet.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;controller-manager.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;scheduler.conf&amp;quot;
[controlplane] Wrote Static Pod manifest for component kube-apiserver to &amp;quot;/etc/kubernetes/manifests/kube-apiserver.yaml&amp;quot;
[controlplane] Wrote Static Pod manifest for component kube-controller-manager to &amp;quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&amp;quot;
[controlplane] Wrote Static Pod manifest for component kube-scheduler to &amp;quot;/etc/kubernetes/manifests/kube-scheduler.yaml&amp;quot;
[etcd] Wrote Static Pod manifest for a local etcd instance to &amp;quot;/etc/kubernetes/manifests/etcd.yaml&amp;quot;
[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory &amp;quot;/etc/kubernetes/manifests&amp;quot;
[init] This often takes around a minute; or longer if the control plane images have to be pulled.

Unfortunately, an error has occurred:
        timed out waiting for the condition

This error is likely caused by that:
        - The kubelet is not running
        - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)
        - There is no internet connection; so the kubelet can&#39;t pull the following control plane images:
                - gcr.io/google_containers/kube-apiserver-amd64:v1.8.1
                - gcr.io/google_containers/kube-controller-manager-amd64:v1.8.1
                - gcr.io/google_containers/kube-scheduler-amd64:v1.8.1

You can troubleshoot this for example with the following commands if you&#39;re on a systemd-powered system:
        - &#39;systemctl status kubelet&#39;
        - &#39;journalctl -xeu kubelet&#39;
couldn&#39;t initialize a Kubernetes cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;また違う感じのエラー。
エラーメッセージに従って、&lt;code&gt;journalctl -xeu kubelet&lt;/code&gt;でログを見てみたら、以下のようなエラーが。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Post https://192.168.171.200:6443/api/v1/nodes: dial tcp 192.168.171.200:6443: getsockopt: connection refused
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/kubeadm/issues/228&#34;&gt;kubeadmにIssue&lt;/a&gt;にこのエラーが載っている。
原因はいろいろあるっぽいけど、そのひとつにSELinuxがあったので確認してみたら、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# getenforce
Enforcing
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SELinuxが有効になっていた。
&lt;code&gt;setenforce 0&lt;/code&gt;もOS再起動で元に戻ってしまった模様。&lt;/p&gt;

&lt;p&gt;永続的にSELinuxを無効にするため、&lt;code&gt;/etc/selinux/config&lt;/code&gt;を編集して、&lt;code&gt;SELINUX&lt;/code&gt;を&lt;code&gt;disabled&lt;/code&gt;にして、OS再起動した。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;で、&lt;code&gt;kubeadm reset&lt;/code&gt;したら3回目の&lt;code&gt;kubeadm init&lt;/code&gt;する。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# kubeadm init --apiserver-advertise-address=192.168.171.200
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[init] Using Kubernetes version: v1.8.1
[init] Using Authorization modes: [Node RBAC]
[preflight] Running pre-flight checks
[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.171.200]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Valid certificates and keys now exist in &amp;quot;/etc/kubernetes/pki&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;admin.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;kubelet.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;controller-manager.conf&amp;quot;
[kubeconfig] Wrote KubeConfig file to disk: &amp;quot;scheduler.conf&amp;quot;
[controlplane] Wrote Static Pod manifest for component kube-apiserver to &amp;quot;/etc/kubernetes/manifests/kube-apiserver.yaml&amp;quot;
[controlplane] Wrote Static Pod manifest for component kube-controller-manager to &amp;quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&amp;quot;
[controlplane] Wrote Static Pod manifest for component kube-scheduler to &amp;quot;/etc/kubernetes/manifests/kube-scheduler.yaml&amp;quot;
[etcd] Wrote Static Pod manifest for a local etcd instance to &amp;quot;/etc/kubernetes/manifests/etcd.yaml&amp;quot;
[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory &amp;quot;/etc/kubernetes/manifests&amp;quot;
[init] This often takes around a minute; or longer if the control plane images have to be pulled.
[apiclient] All control plane components are healthy after 99.510003 seconds
[uploadconfig]?Storing the configuration used in ConfigMap &amp;quot;kubeadm-config&amp;quot; in the &amp;quot;kube-system&amp;quot; Namespace
[markmaster] Will mark node k8s-master as master by adding a label and a taint
[markmaster] Master k8s-master tainted and labelled with key/value: node-role.kubernetes.io/master=&amp;quot;&amp;quot;
[bootstraptoken] Using token: 957b7b.eaaf0cb656edba7b
[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] Creating the &amp;quot;cluster-info&amp;quot; ConfigMap in the &amp;quot;kube-public&amp;quot; namespace
[addons] Applied essential addon: kube-dns
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run (as a regular user):

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &amp;quot;kubectl apply -f [podnetwork].yaml&amp;quot; with one of the options listed at:
  http://kubernetes.io/docs/admin/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join --token 957b7b.eaaf0cb656edba7b 192.168.171.200:6443 --discovery-token-ca-cert-hash sha256:7d16ade2b651ebac573368b1b4db5c0f1236979584e61833efe90a96ff34ae2e
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;できた。
このメッセージの最後に書かれたコマンドを、後でNodeを追加するときに使う。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;kubectlがこのVM上のkube-apiserverと話せるように、コンテキストを設定する。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# mkdir -p $HOME/.kube
[root@k8s-master ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[root@k8s-master ~]# chown $(id -u):$(id -g) $HOME/.kube/config
[root@k8s-master ~]# kubectl get nodes
NAME         STATUS     ROLES     AGE       VERSION
k8s-master   NotReady   master    16m       v1.8.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;podネットワークアドオンインストール&#34;&gt;Podネットワークアドオンインストール&lt;/h3&gt;

&lt;p&gt;Podネットワークはアプリのデプロイの前にセットアップしておく必要がある。&lt;/p&gt;

&lt;p&gt;多くの選択肢があるなか、有名な&lt;a href=&#34;https://github.com/coreos/flannel&#34;&gt;Flannel&lt;/a&gt;にしようと思ったけど、Flannelを使うには
&lt;code&gt;kubeadm init&lt;/code&gt;時に&lt;code&gt;--pod-network-cidr=10.244.0.0/16&lt;/code&gt;を渡さないといけなかった。
やり直すのは面倒なので代わりに&lt;a href=&#34;https://www.weave.works/docs/net/latest/kube-addon/&#34;&gt;Weave Net&lt;/a&gt;にする。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# export kubever=$(kubectl version | base64 | tr -d &#39;\n&#39;)
[root@k8s-master ~]# kubectl apply -f &amp;quot;https://cloud.weave.works/k8s/net?k8s-version=$kubever&amp;quot;
serviceaccount &amp;quot;weave-net&amp;quot; created
clusterrole &amp;quot;weave-net&amp;quot; created
clusterrolebinding &amp;quot;weave-net&amp;quot; created
daemonset &amp;quot;weave-net&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これでPodネットワークアドオンインストール完了。
しばらくして、&lt;code&gt;kube-dns&lt;/code&gt;のPodが起動していれば(i.e. STATUSがRunningになってれば)OK。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# kubectl get pods --all-namespaces
NAMESPACE     NAME                                 READY     STATUS    RESTARTS   AGE
kube-system   etcd-k8s-master                      1/1       Running   0          1m
kube-system   kube-apiserver-k8s-master            1/1       Running   0          1m
kube-system   kube-controller-manager-k8s-master   1/1       Running   0          1m
kube-system   kube-dns-545bc4bfd4-xtlnh            3/3       Running   0          6m
kube-system   kube-proxy-922wk                     1/1       Running   0          6m
kube-system   kube-scheduler-k8s-master            1/1       Running   0          1m
kube-system   weave-net-s2kkw                      2/2       Running   0          2m
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;masterにpodをデプロイさせる設定&#34;&gt;MasterにPodをデプロイさせる設定&lt;/h3&gt;

&lt;p&gt;デフォルトでは、セキュリティの都合でMasterコンポーネントが動くNodeにはPodがデプロイされない。
けど、VM2個でPodを分散デプロイしてみたいので、この縛りを外しておく。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# kubectl taint nodes --all node-role.kubernetes.io/master-
node &amp;quot;k8s-master&amp;quot; untainted
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;以上でMasterのセットアップは完了。&lt;/p&gt;

&lt;h2 id=&#34;node追加&#34;&gt;Node追加&lt;/h2&gt;

&lt;p&gt;次にNodeをひとつ追加する。&lt;/p&gt;

&lt;p&gt;k8s-masterで&lt;code&gt;kubeadm init&lt;/code&gt;するまえに撮ったスナップショットをクローンして、ホスト名とIPアドレスを変更し、これを追加するNodeのマシン(k8s-node)にする。
クローンしたらMACアドレスもproduct_uuidも変わったので、問題なく使えそう。&lt;/p&gt;

&lt;p&gt;k8s-nodeをクラスタに追加するには、このVM上で、&lt;code&gt;kubeadm init&lt;/code&gt;成功時のメッセージの最後に表示されたコマンド(i.e. &lt;code&gt;kubeadm join&lt;/code&gt;)を実行するだけ。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-node ~]# kubeadm join --token 957b7b.eaaf0cb656edba7b 192.168.171.200:6443 --discovery-token-ca-cert-hash sha256:7d16ade2b651ebac573368b1b4db5c0f1236979584e61833efe90a96ff34ae2e
[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.
[preflight] Running pre-flight checks
[discovery] Trying to connect to API Server &amp;quot;192.168.171.200:6443&amp;quot;
[discovery] Created cluster-info discovery client, requesting info from &amp;quot;https://192.168.171.200:6443&amp;quot;
[discovery] Requesting info from &amp;quot;https://192.168.171.200:6443&amp;quot; again to validate TLS against the pinned public key
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &amp;quot;192.168.171.200:6443&amp;quot;
[discovery] Successfully established connection with API Server &amp;quot;192.168.171.200:6443&amp;quot;
[bootstrap] Detected server version: v1.8.1
[bootstrap] The server supports the Certificates API (certificates.k8s.io/v1beta1)

Node join complete:
* Certificate signing request sent to master and response
  received.
* Kubelet informed of new secure connection details.

Run &#39;kubectl get nodes&#39; on the master to see this machine join.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;できた。
k8s-masterでNodeの状態を確認する。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@k8s-master ~]# kubectl get nodes
NAME         STATUS    ROLES     AGE       VERSION
k8s-master   Ready     master    42m       v1.8.1
k8s-node     Ready     &amp;lt;none&amp;gt;    45s       v1.8.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;k8s-masterもk8s-nodeもReady。&lt;/p&gt;

&lt;h2 id=&#34;vmホストのkubectlの設定&#34;&gt;VMホストのkubectlの設定&lt;/h2&gt;

&lt;p&gt;kubectlはkube-apiserverのWeb APIを呼ぶコマンドなので、接続先さえちゃんと設定すればMasterのマシン上でなくても使える。
VMのホスト(i.e. Windows 10 PC)で使えるようにしたい。&lt;/p&gt;

&lt;p&gt;kubectlの接続先情報は、&lt;code&gt;kubeadm init&lt;/code&gt;時に生成された&lt;code&gt;/etc/kubernetes/admin.conf&lt;/code&gt;に書かれているので、これをホストに持ってきてkubectlに渡してやればいい。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy\Desktop&amp;gt;kubectl --kubeconfig admin.conf get nodes
NAME         STATUS    ROLES     AGE       VERSION
k8s-master   Ready     master    51m       v1.8.1
k8s-node     Ready     &amp;lt;none&amp;gt;    10m       v1.8.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;できた。&lt;/p&gt;

&lt;p&gt;admin.confを&lt;code&gt;%UserProfile%\.kube\&lt;/code&gt;の下に&lt;code&gt;config&lt;/code&gt;という名前で置いてやると、&lt;code&gt;--kubeconfig&lt;/code&gt;で指定しなくても読んでくれる。&lt;/p&gt;

&lt;h2 id=&#34;goslingsデプロイ&#34;&gt;Goslingsデプロイ&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.kaitoy.xyz/2017/10/11/goslings-on-kubernetes-cont/#%E7%95%AA%E5%A4%96%E7%B7%A82-%E5%91%BD%E4%BB%A4%E7%9A%84%E3%82%AA%E3%83%96%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88%E8%A8%AD%E5%AE%9A&#34;&gt;「Kubernetesのチュートリアルをやる」の番外編&lt;/a&gt;で作ったオブジェクト定義ファイルを使って、今回作ったクラスタに&lt;a href=&#34;https://www.kaitoy.xyz/2016/12/11/goslings-development-memo0-intro-design/&#34;&gt;Goslings&lt;/a&gt;をデプロイしてみる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy\Desktop&amp;gt;kubectl --kubeconfig admin.conf create -f deploy_goslings.yml
deployment &amp;quot;goslings-sample&amp;quot; created

C:\Users\kaitoy\Desktop&amp;gt;kubectl --kubeconfig admin.conf create -f service_goslings.yml
service &amp;quot;goslings-sample&amp;quot; created

C:\Users\kaitoy\Desktop&amp;gt;kubectl --kubeconfig admin.conf get po -o wide
NAME                              READY     STATUS    RESTARTS   AGE       IP           NODE
goslings-sample-dfd84c69c-4mgh9   1/1       Running   0          12m       10.244.1.3   k8s-node
goslings-sample-dfd84c69c-cd5mm   1/1       Running   0          12m       10.244.0.3   k8s-master
goslings-sample-dfd84c69c-nwwh7   1/1       Running   0          12m       10.244.1.2   k8s-node

C:\Users\kaitoy\Desktop&amp;gt;kubectl --kubeconfig admin.conf get svc
NAME              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
goslings-sample   NodePort    10.109.174.204   &amp;lt;none&amp;gt;        8080:30004/TCP   7m
kubernetes        ClusterIP   10.96.0.1        &amp;lt;none&amp;gt;        443/TCP          1h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;普通にデプロイできた。
レプリカ3つがちゃんと2つのNodeに分散されてる。&lt;/p&gt;

&lt;p&gt;k8s-masterのIPアドレス( &lt;a href=&#34;http://192.168.171.200:30004/&#34;&gt;http://192.168.171.200:30004/&lt;/a&gt; )でもk8s-nodeのIPアドレス( &lt;a href=&#34;http://192.168.171.201:30004/&#34;&gt;http://192.168.171.201:30004/&lt;/a&gt; )でもGoslingsにつなげた。
普通はMasterのIPアドレスを使うらしい。
そりゃそうか。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/build-kubernetes-cluster-by-kubeadm/goslings.png&#34; alt=&#34;goslings&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;試しにk8s-nodeのVMを落としてみる。
k8s-node上のPodがk8s-masterに移動してくれることを期待してたけど、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy\Desktop&amp;gt;kubectl --kubeconfig admin.conf get po -o wide
NAME                              READY     STATUS    RESTARTS   AGE       IP           NODE
goslings-sample-dfd84c69c-4mgh9   1/1       Running   0          55m       10.244.1.3   k8s-node
goslings-sample-dfd84c69c-cd5mm   1/1       Running   0          55m       10.244.0.3   k8s-master
goslings-sample-dfd84c69c-nwwh7   1/1       Running   0          55m       10.244.1.2   k8s-node
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;なんかk8s-nodeで動き続けていることになってる。&lt;/p&gt;

&lt;p&gt;クラスタうまく動いていないんだろうか…&lt;/p&gt;

&lt;h2 id=&#34;ダッシュボードデプロイ&#34;&gt;ダッシュボードデプロイ&lt;/h2&gt;

&lt;p&gt;Kubernetesクラスタの状態をWeb UIで確認できる、&lt;a href=&#34;https://github.com/kubernetes/dashboard&#34;&gt;Dashboard&lt;/a&gt;をデプロイしてみる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy\Desktop&amp;gt;kubectl --kubeconfig admin.conf apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
secret &amp;quot;kubernetes-dashboard-certs&amp;quot; created
serviceaccount &amp;quot;kubernetes-dashboard&amp;quot; created
role &amp;quot;kubernetes-dashboard-minimal&amp;quot; created
rolebinding &amp;quot;kubernetes-dashboard-minimal&amp;quot; created
deployment &amp;quot;kubernetes-dashboard&amp;quot; created
service &amp;quot;kubernetes-dashboard&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;できた。&lt;/p&gt;

&lt;p&gt;Dashboardが起動するまでしばらくまってから、&lt;code&gt;kubectl proxy&lt;/code&gt;して、
&lt;code&gt;http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/&lt;/code&gt;にブラウザでつなげばGUIが開くはずなんだけど、タイムアウトしてつながらなかった。&lt;/p&gt;

&lt;p&gt;クラスタうまく動いていないんだろうか…&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;NICがNATなのがだめだったかもと思い、ブリッジにしてみたけど同じ結果だった。
PodのフェールオーバーもしないしDashboardも開けない。&lt;/p&gt;

&lt;p&gt;ちゃんと一つ一つ自分で構築しないとよく分からないな。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;あとでふと思い立って、&lt;code&gt;/etd/hosts&lt;/code&gt;をいじったらDashboardは動いた。
それについてはまた&lt;a href=&#34;https://www.kaitoy.xyz/2017/10/31/retry-dashboard-on-k8s-cluster-by-kubeadm/&#34;&gt;別の記事&lt;/a&gt;で。&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Kubernetesのチュートリアルをやる</title>
          <link>https://www.kaitoy.xyz/2017/10/11/goslings-on-kubernetes-cont/</link>
          <pubDate>Wed, 11 Oct 2017 23:48:40 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2017/10/11/goslings-on-kubernetes-cont/</guid>
          <description>

&lt;p&gt;「&lt;a href=&#34;https://www.kaitoy.xyz/2017/10/10/goslings-on-kubernetes/&#34;&gt;Kubernetes 1.8が出たので、Minikubeを触ってみる&lt;/a&gt;」の続き。
Minikubeのセットアップまではできたので、&lt;a href=&#34;https://kubernetes.io/docs/tutorials/kubernetes-basics/&#34;&gt;Kubernetes Basics&lt;/a&gt;というチュートリアルをやりながら、&lt;a href=&#34;https://kaitoy.github.io/goslings/&#34;&gt;Goslings&lt;/a&gt;をデプロイする。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-basics-概要&#34;&gt;Kubernetes Basics - 概要&lt;/h2&gt;

&lt;p&gt;Kubernetes Basicsは、公式のチュートリアルで、Kubernetesクラスタのオーケストレーションの基本を学ぶことができるもの。
以下の6つのモジュールからなる。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Kubernetesクラスタを作る&lt;/li&gt;
&lt;li&gt;アプリをデプロイする&lt;/li&gt;
&lt;li&gt;アプリを調査する&lt;/li&gt;
&lt;li&gt;アプリを公開する&lt;/li&gt;
&lt;li&gt;アプリをスケールする&lt;/li&gt;
&lt;li&gt;アプリをアップデートする&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;チュートリアルで使うのは&lt;a href=&#34;https://github.com/kubernetes/minikube&#34;&gt;Minikube&lt;/a&gt;だけど、自分でセットアップする必要はない。
&lt;a href=&#34;https://www.katacoda.com/&#34;&gt;Katacoda&lt;/a&gt;という、ブラウザ上でIT技術を学べるプラットフォームがあり、Kubernetes Basicsはそれを利用して、ブラウザ上のターミナルからホステッドMinikubeを操作できるようにしている。&lt;/p&gt;

&lt;p&gt;が、&lt;a href=&#34;https://www.kaitoy.xyz/2017/10/10/goslings-on-kubernetes/&#34;&gt;前回の記事&lt;/a&gt;で自PC上にMinikubeをセットアップしたので、そちらを使うことにする。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-basics-モジュール-1-kubernetesクラスタを作る&#34;&gt;Kubernetes Basics - モジュール 1: Kubernetesクラスタを作る&lt;/h2&gt;

&lt;p&gt;Minikubeを起動してkubectlでクラスタの状態をみるだけのモジュール。&lt;/p&gt;

&lt;p&gt;これは&lt;a href=&#34;https://www.kaitoy.xyz/2017/10/10/goslings-on-kubernetes/&#34;&gt;前回の記事&lt;/a&gt;でカバーしている。&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-basics-モジュール-2-アプリをデプロイする&#34;&gt;Kubernetes Basics - モジュール 2: アプリをデプロイする&lt;/h2&gt;

&lt;p&gt;アプリ(i.e. コンテナ)をデプロイするにはDeploymentオブジェクトを作る。
MasterはDeploymentのspecに従って各ノードにアプリのインスタンスをスケジューリングする。
Deploymentは、アプリが落ちたら再起動してくれる、つまりself-healingも実現する。&lt;/p&gt;

&lt;p&gt;Deploymentオブジェクトを作るコマンドは&lt;code&gt;kubectl run &amp;lt;オブジェクト名&amp;gt; --image=&amp;lt;Dockerイメージ名&amp;gt;&lt;/code&gt;。
Goslingsをこれでデプロイする。&lt;/p&gt;

&lt;p&gt;Goslingsコンテナは3つの引数を受け取り、指定したポートでWebサーバを起動する。
&lt;code&gt;--port&lt;/code&gt;オプションでそのポートをexposeするようにして、&lt;code&gt;--&lt;/code&gt;の後にコンテナに渡す引数を記述する。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl run goslings --image=kaitoy/goslings:latest --port 8080 -- 8080 /tmp https://github.com/kaitoy/
deployment &amp;quot;goslings&amp;quot; created

C:\Users\kaitoy&amp;gt;kubectl get deployment
NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
goslings   1         1         1            1           27s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デプロイできた。
裏でPodも作られていて、アプリが起動されている。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl get pods
NAME                        READY     STATUS              RESTARTS   AGE
goslings-1210510689-6w5tf   0/1       ContainerCreating   0          1m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(&lt;code&gt;kubectl get&lt;/code&gt;に指定するのは、省略形の&lt;code&gt;deploy&lt;/code&gt;とか&lt;code&gt;po&lt;/code&gt;でもいい。)&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Podは隔離されたネットワークで動くので、そのままではPod同士は通信できるけど、外からはアクセスできない。
kubectlでプロキシを作ってやることで、外からアクセスできるようになる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl proxy
Starting to serve on 127.0.0.1:8001
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これで、kube-apiserverへのプロキシがローカルホストで起動した。
この状態で&lt;code&gt;http://localhost:8001&lt;/code&gt;を開くと、kube-apiserverのAPI一覧が見れる。
例えば、&lt;code&gt;http://localhost:8001/version&lt;/code&gt;にアクセスすると、以下のJSONデータが返ってくる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;major&amp;quot;: &amp;quot;1&amp;quot;,
  &amp;quot;minor&amp;quot;: &amp;quot;7&amp;quot;,
  &amp;quot;gitVersion&amp;quot;: &amp;quot;v1.7.0&amp;quot;,
  &amp;quot;gitCommit&amp;quot;: &amp;quot;d3ada0119e776222f11ec7945e6d860061339aad&amp;quot;,
  &amp;quot;gitTreeState&amp;quot;: &amp;quot;dirty&amp;quot;,
  &amp;quot;buildDate&amp;quot;: &amp;quot;2017-10-04T09:25:40Z&amp;quot;,
  &amp;quot;goVersion&amp;quot;: &amp;quot;go1.8.3&amp;quot;,
  &amp;quot;compiler&amp;quot;: &amp;quot;gc&amp;quot;,
  &amp;quot;platform&amp;quot;: &amp;quot;linux/amd64&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;各Podへも以下のURLでアクセスできる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://localhost:8001/api/v1/proxy/namespaces/default/pods/&amp;lt;Pod名&amp;gt;/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pod名の部分は&lt;code&gt;kubectl get&lt;/code&gt;で確認できる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl get po
NAME                        READY     STATUS    RESTARTS   AGE
goslings-1210510689-6w5tf   1/1       Running   0          24m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;実際に、&lt;code&gt;http://localhost:8001/api/v1/proxy/namespaces/default/pods/goslings-1210510689-6w5tf/&lt;/code&gt;をブラウザで開いたら、GoslingsのGUIが出た。
ヒュー。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/goslings-on-kubernetes-cont/goslings-proxy.png&#34; alt=&#34;goslings-proxy&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-basics-モジュール-3-アプリを調査する&#34;&gt;Kubernetes Basics - モジュール 3: アプリを調査する&lt;/h2&gt;

&lt;p&gt;以下のコマンドで、アプリの状態を調査するモジュール。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kubectl get: リソースをリスト表示する。&lt;/li&gt;
&lt;li&gt;kubectl describe: リソースの詳細情報を表示する。&lt;/li&gt;
&lt;li&gt;kubectl logs: コンテナのログを表示する。&lt;code&gt;docker logs&lt;/code&gt;的な。&lt;/li&gt;
&lt;li&gt;kubectl exec: コンテナ内でコマンドを実行する。&lt;code&gt;docker exec&lt;/code&gt;的な。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kubectl get&lt;/code&gt;はさんざんやったので飛ばして、&lt;code&gt;kubectl describe&lt;/code&gt;してみる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl describe po
Name:           goslings-1210510689-6w5tf
Namespace:      default
Node:           minikube/192.168.99.100
Start Time:     Tue, 10 Oct 2017 21:51:48 +0900
Labels:         pod-template-hash=1210510689
                run=goslings
Annotations:    kubernetes.io/created-by={&amp;quot;kind&amp;quot;:&amp;quot;SerializedReference&amp;quot;,&amp;quot;apiVersion&amp;quot;:&amp;quot;v1&amp;quot;,&amp;quot;reference&amp;quot;:{&amp;quot;kind&amp;quot;:&amp;quot;ReplicaSet&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;default&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;goslings-1210510689&amp;quot;,&amp;quot;uid&amp;quot;:&amp;quot;c74b6518-adb9-11e7-88a0-08002798178d...
Status:         Running
IP:             172.17.0.2
Created By:     ReplicaSet/goslings-1210510689
Controlled By:  ReplicaSet/goslings-1210510689
Containers:
  goslings:
    Container ID:       docker://ce90460886c9555f7748bf59e8d9892f05c05020e7841154ee85713d6d9b0c2d
    Image:              kaitoy/goslings:latest
    Image ID:           docker-pullable://kaitoy/goslings@sha256:a587e3c5f202cdaa6d4d5a9c4f6a01ba6f4782e00277c3a18c77dd034daa0109
    Port:               8080/TCP
    Args:
      8080
      C:/Users/kaitoy/AppData/Local/Temp
    State:              Running
      Started:          Tue, 10 Oct 2017 21:55:54 +0900
    Ready:              True
    Restart Count:      0
    Environment:        &amp;lt;none&amp;gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-cqq59 (ro)
Conditions:
  Type          Status
  Initialized   True
  Ready         True
  PodScheduled  True
Volumes:
  default-token-cqq59:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-cqq59
    Optional:   false
QoS Class:      BestEffort
Node-Selectors: &amp;lt;none&amp;gt;
Tolerations:    &amp;lt;none&amp;gt;
Events:
  FirstSeen     LastSeen        Count   From                    SubObjectPath                   Type            Reason
                Message
  ---------     --------        -----   ----                    -------------                   --------        ------
                -------
  45m           45m             1       default-scheduler                                       Normal          Scheduled               Successfully assigned goslings-1210510689-6w5tf to minikube
  45m           45m             1       kubelet, minikube                                       Normal          SuccessfulMountVolume   MountVolume.SetUp succeeded for volume &amp;quot;default-token-cqq59&amp;quot;
  45m           45m             1       kubelet, minikube       spec.containers{goslings}       Normal          Pulling
                pulling image &amp;quot;kaitoy/goslings:latest&amp;quot;
  41m           41m             1       kubelet, minikube       spec.containers{goslings}       Normal          Pulled
                Successfully pulled image &amp;quot;kaitoy/goslings:latest&amp;quot;
  41m           41m             1       kubelet, minikube       spec.containers{goslings}       Normal          Created
                Created container
  41m           41m             1       kubelet, minikube       spec.containers{goslings}       Normal          Started
                Started container
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Podの詳細な情報が出た。
EventsのとこにKubernetesの頑張りが見えて面白い。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;次は&lt;code&gt;kubectl logs&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl logs goslings-1210510689-6w5tf

  .   ____          _            __ _ _
 /\\ / ___&#39;_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | &#39;_ | &#39;_| | &#39;_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  &#39;  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::        (v1.4.3.RELEASE)

2017-10-10 12:56:02.498  INFO 6 --- [           main] c.g.kaitoy.goslings.server.Application   : Starting Application on goslings-1210510689-6w5tf with PID 6 (/usr/local/src/goslings/goslings-server/build/libs/goslings-server-0.0.1.jar started by root in /usr/local/src/goslings)
(snip)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Goslingsは&lt;a href=&#34;https://projects.spring.io/spring-boot/&#34;&gt;Spring Boot&lt;/a&gt;でできてるので、そのログが出てる。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;次は&lt;code&gt;kubectl exec&lt;/code&gt;を試す。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl exec goslings-1210510689-6w5tf env
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=goslings-1210510689-6w5tf
KUBERNETES_PORT_443_TCP=tcp://10.0.0.1:443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_PORT=443
KUBERNETES_PORT_443_TCP_ADDR=10.0.0.1
KUBERNETES_SERVICE_HOST=10.0.0.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_PORT=tcp://10.0.0.1:443
LANG=C.UTF-8
JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
JAVA_VERSION=8u111
JAVA_DEBIAN_VERSION=8u111-b14-2~bpo8+1
CA_CERTIFICATES_JAVA_VERSION=20140324
HOME=/root
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;env&lt;/code&gt;コマンドを実行し、コンテナ内の環境変数一覧を出せた。
Kubernetes関係の変数が定義されていることが分かる。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;docker exec&lt;/code&gt;と同様に、&lt;code&gt;-it&lt;/code&gt;オプションを付ければ、コンテナ内に「入る」こともできる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl exec -it goslings-1210510689-6w5tf sh
# ls
Dockerfile  _config.yml  build.log     goslings-server  gradle.properties  gradlew.bat
# exit

C:\Users\kaitoy&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;kubernetes-basics-モジュール-4-アプリを公開する&#34;&gt;Kubernetes Basics - モジュール 4: アプリを公開する&lt;/h2&gt;

&lt;p&gt;Serviceオブジェクト扱うモジュール。&lt;/p&gt;

&lt;p&gt;例えば、以下のような状況にあるとする。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;PodがあるNodeで動いていたんだけど、そのNodeが死んだので、Kubernetesが別のNodeにPodを起動しなおしてくれた。&lt;/li&gt;
&lt;li&gt;同じコンテナイメージを3つのPodで動かして、負荷分散させたい。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;こういう場合、KubernetesはPod毎に固有のIPアドレスを割り当てるので、Podにアクセスするユーザはアクセス先が不安定でめんどいことになる。
この問題を解決してくれるのがServiceで、こいつは、Podを抽象化して、安定したIPアドレスを公開してくれる。
しかもそれはクラスタ外からアクセスできる。&lt;/p&gt;

&lt;p&gt;PodとServiceの紐づけには、&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/&#34;&gt;ラベルとセレクタ&lt;/a&gt;というものが使われる。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Serviceの情報はDeploymentとかと同様に&lt;code&gt;kubectl get&lt;/code&gt;で見れる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl get svc
NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   10.0.0.1     &amp;lt;none&amp;gt;        443/TCP   1d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここで出ているkubernetesというのは、Minikubeがデフォルトで作るService。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Serviceオブジェクトは、&lt;code&gt;kubectl expose&lt;/code&gt;で作ることができる。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;goslings&lt;/code&gt;という名のDeploymentに対し、NodePortのServiceを作り、コンテナの8080ポートを公開するコマンドは以下のようになる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl expose deploy/goslings --type=NodePort --port 8080
service &amp;quot;goslings&amp;quot; exposed

C:\Users\kaitoy&amp;gt;kubectl get services
NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)          AGE
goslings     10.0.0.69    &amp;lt;nodes&amp;gt;       8080:32406/TCP   11s
kubernetes   10.0.0.1     &amp;lt;none&amp;gt;        443/TCP          1d

C:\Users\kaitoy&amp;gt;kubectl describe services/goslings
Name:                   goslings
Namespace:              default
Labels:                 run=goslings
Annotations:            &amp;lt;none&amp;gt;
Selector:               run=goslings
Type:                   NodePort
IP:                     10.0.0.69
Port:                   &amp;lt;unset&amp;gt; 8080/TCP
NodePort:               &amp;lt;unset&amp;gt; 32406/TCP
Endpoints:              172.17.0.2:8080
Session Affinity:       None
Events:                 &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;goslingsという名前のServiceができた。
上記&lt;code&gt;kubectl describe&lt;/code&gt;の出力のNodePortのとこに書いてあるのが外部にさらされたポート。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;minikube ip&lt;/code&gt;を実行すると、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;minikube ip
192.168.99.100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;MinikubeのVMのIPアドレスも分かるので、NodePortのポートと合わせて、&lt;code&gt;http://192.168.99.100:32406&lt;/code&gt;にブラウザでアクセスしたら、GoslingsのGUI見れた。
ヒュー。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/goslings-on-kubernetes-cont/goslings-service.png&#34; alt=&#34;goslings-service&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;ところで、上記&lt;code&gt;kubectl describe&lt;/code&gt;の出力を見ると、特に指定はしなかったが、Podに&lt;code&gt;run=goslings&lt;/code&gt;というLabelが付いていることが分かる。
Serviceのdescribeを見ると、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl describe svc goslings
Name:                   goslings
Namespace:              default
Labels:                 run=goslings
Annotations:            &amp;lt;none&amp;gt;
Selector:               run=goslings
Type:                   NodePort
IP:                     10.0.0.69
Port:                   &amp;lt;unset&amp;gt; 8080/TCP
NodePort:               &amp;lt;unset&amp;gt; 32406/TCP
Endpoints:              172.17.0.2:8080
Session Affinity:       None
Events:                 &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;run=goslings&lt;/code&gt;というSelectorがServiceに紐づいている。
つまり、ServiceとPodが、&lt;code&gt;run=goslings&lt;/code&gt;で紐づいているというわけだ。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Labelはクエリ時のフィルタとかにも使える。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl get po -l run=goslings
NAME                        READY     STATUS    RESTARTS   AGE
goslings-1210510689-6w5tf   1/1       Running   0          1h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;後からラベル付けることもできる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl label pod goslings-1210510689-6w5tf ver=1.2.3
pod &amp;quot;goslings-1210510689-6w5tf&amp;quot; labeled
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;kubernetes-basics-モジュール-5-アプリをスケールする&#34;&gt;Kubernetes Basics - モジュール 5: アプリをスケールする&lt;/h2&gt;

&lt;p&gt;アプリのスケールアウト・スケールインを学ぶモジュール。&lt;/p&gt;

&lt;p&gt;Deploymentの定義でPodのレプリカ数を変えると、その数に合わせてKubernetesがPodを起動したり止めたりしてくれてスケールできる仕組み。
レプリカを作っておくとローリングアップデートできるのも利点。
&lt;a href=&#34;http://kubernetes.io/docs/user-guide/horizontal-pod-autoscaling/&#34;&gt;オートスケール機能&lt;/a&gt;もあるけど、それはチュートリアルでは扱われない。&lt;/p&gt;

&lt;p&gt;複数のPodで負荷分散するということなので、Serviceでロードバランシングするのが前提。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;現在のDeploymentの状態をみる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl get deploy
NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
goslings   1         1         1            1           1h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Podのレプリカ数は、期待してる(DESIRED)のが1で、今(CURRENT)も1。&lt;/p&gt;

&lt;p&gt;スケールアウトするには、&lt;code&gt;kubectl scale&lt;/code&gt;コマンドでレプリカ数を増やしてやる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl scale deploy/goslings --replicas=3
deployment &amp;quot;goslings&amp;quot; scaled

C:\Users\kaitoy&amp;gt;kubectl get deploy
NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
goslings   3         3         3            3           1h

C:\Users\kaitoy&amp;gt;kubectl get po -o wide
NAME                       READY     STATUS    RESTARTS   AGE       IP           NODE
goslings-442066424-jn1lw   1/1       Running   0          1h        172.17.0.2   minikube
goslings-442066424-rdw4k   1/1       Running   0          1m        172.17.0.3   minikube
goslings-442066424-rwwjw   1/1       Running   0          1m        172.17.0.4   minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;レプリカが3個になった。&lt;/p&gt;

&lt;p&gt;スケールインするには、&lt;code&gt;kubectl scale&lt;/code&gt;コマンドでレプリカ数を減らす。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl scale deploy/goslings --replicas=2
deployment &amp;quot;goslings&amp;quot; scaled

C:\Users\kaitoy&amp;gt;kubectl get po
NAME                       READY     STATUS        RESTARTS   AGE
goslings-442066424-0mv4x   1/1       Terminating   0          1m
goslings-442066424-34h1f   1/1       Running       0          1m
goslings-442066424-kmn3p   1/1       Running       0          17m

C:\Users\kaitoy&amp;gt;kubectl get po
NAME                       READY     STATUS    RESTARTS   AGE
goslings-442066424-34h1f   1/1       Running   0          1m
goslings-442066424-kmn3p   1/1       Running   0          17m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;kubectl scale&lt;/code&gt;直後の&lt;code&gt;kubectl get po&lt;/code&gt;では、一つのPodを停止している最中の様子が見えていて、再度の&lt;code&gt;kubectl get po&lt;/code&gt;ではレプリカが2個になったのが確認できた。&lt;/p&gt;

&lt;p&gt;この状態がKubernetes Basicsで作るクラスタの最終形で、図にすると以下の感じ。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/goslings-on-kubernetes-cont/objects.png&#34; alt=&#34;objects&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-basics-モジュール-6-アプリをアップデートする&#34;&gt;Kubernetes Basics - モジュール 6: アプリをアップデートする&lt;/h2&gt;

&lt;p&gt;デプロイしたアプリのアップデート(i.e. コンテナイメージの変更)を学ぶモジュール。&lt;/p&gt;

&lt;p&gt;Deploymentの定義をいじってコンテナイメージを変えてやると、その中のPodを新しいイメージで順次(デフォルトだと一つ一つ)起動しなおしてくれる。&lt;/p&gt;

&lt;p&gt;アプリのアップデートはバージョン管理もされて、ロールバックもできる。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;コンテナイメージを変更するには、&lt;code&gt;kubectl set image&lt;/code&gt;コマンドを使う。
&lt;code&gt;goslings&lt;/code&gt;という名のDeployment内の、&lt;code&gt;goslings&lt;/code&gt;という名のContainerのイメージを&lt;code&gt;kaitoy/goslings:hoge&lt;/code&gt;に変更するコマンドは以下。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl set image deploy/goslings goslings=kaitoy/goslings:hoge
deployment &amp;quot;goslings&amp;quot; image updated
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;実際には&lt;code&gt;kaitoy/goslings:hoge&lt;/code&gt;というイメージはないので、イメージのPullに失敗したというエラー(ErrImagePull)になる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl get po
NAME                       READY     STATUS         RESTARTS   AGE
goslings-274047280-jxmmh   0/1       ErrImagePull   0          9s
goslings-274047280-rgg2v   0/1       ErrImagePull   0          8s
goslings-442066424-34h1f   1/1       Terminating    0          1h
goslings-442066424-kmn3p   1/1       Running        0          1h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;イメージ変更前に戻すには、&lt;code&gt;kubectl rollout undo&lt;/code&gt;する。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl rollout undo deploy/goslings
deployment &amp;quot;goslings&amp;quot; rolled back

C:\Users\kaitoy&amp;gt;kubectl rollout status deploy/goslings
deployment &amp;quot;goslings&amp;quot; successfully rolled out

C:\Users\kaitoy&amp;gt;kubectl get po
NAME                       READY     STATUS    RESTARTS   AGE
goslings-442066424-kmn3p   1/1       Running   0          1h
goslings-442066424-m3873   1/1       Running   0          5s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;無事に戻った。&lt;/p&gt;

&lt;h2 id=&#34;番外編1-3つのオブジェクト管理手法&#34;&gt;番外編1 - 3つのオブジェクト管理手法&lt;/h2&gt;

&lt;p&gt;Kubernetesオブジェクトを管理する手法は&lt;a href=&#34;https://kubernetes.io/docs/tutorials/object-management-kubectl/object-management/&#34;&gt;大きく3つある&lt;/a&gt;。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;管理手法&lt;/th&gt;
&lt;th&gt;いじる対象&lt;/th&gt;
&lt;th&gt;難易度&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;命令的コマンド&lt;/td&gt;
&lt;td&gt;生のオブジェクト&lt;/td&gt;
&lt;td&gt;簡単&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;命令的オブジェクト設定&lt;/td&gt;
&lt;td&gt;個々のファイル&lt;/td&gt;
&lt;td&gt;普通&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;宣言的オブジェクト設定&lt;/td&gt;
&lt;td&gt;ディレクトリに入ったファイル群&lt;/td&gt;
&lt;td&gt;難しい&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes Basicsでやってた手法は一番上の命令的コマンド。
これは簡単で分かりやすい。
けど、何度も同じようなデプロイするならコマンドを毎回打つのが面倒だし、作成されるオブジェクトは明示的じゃないし、変更管理もできない。
この手法は主に開発中に使う。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;二つ目の手法の命令的オブジェクト設定では、YAML(かJSON)ファイルにオブジェクト定義を書いておいて、kubectlに渡す。
この手法だと、定義ファイルをオブジェクトのテンプレートとして使えるし、Gitとかのリポジトリに入れることでバージョン管理・変更管理できる。
けど、Kubernetesのオブジェクトモデルを理解しないと使えない。
(オブジェクト定義の詳細は&lt;a href=&#34;https://kubernetes.io/docs/api-reference/v1.8/&#34;&gt;APIリファレンス&lt;/a&gt;を参照。)&lt;/p&gt;

&lt;p&gt;命令的オブジェクト設定は以下のような形でやる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl create -f nginx.yaml
$ kubectl delete -f nginx.yaml -f redis.yaml
$ kubectl replace -f nginx.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;三つ目の手法の宣言的オブジェクト設定では、設定フォルダに定義ファイル群を置く。
ユーザは明示的にcreateとかupdateとか指示する必要が無く、kubectlが勝手に判断してくれる。
生のオブジェクトを直接いじった後、同じオブジェクトの設定を設定ファイルで変更しても、
両者の変更が上手くマージされる。&lt;/p&gt;

&lt;p&gt;なんかすごいけど、上手くいかなかったときのデバッグがむずい。&lt;/p&gt;

&lt;p&gt;宣言的オブジェクト設定は以下のような形でやる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ kubectl apply -R -f configs/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;番外編2-命令的オブジェクト設定&#34;&gt;番外編2 - 命令的オブジェクト設定&lt;/h2&gt;

&lt;p&gt;3つの手法の内、命令的オブジェクト設定でGoslingsをMinikubeにデプロイしてみる。&lt;/p&gt;

&lt;p&gt;まず、Kubernetes Basicsで作ったオブジェクトを消すため、MinikubeのVMを作り直す。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;minikube stop
Stopping local Kubernetes cluster...
Machine stopped.

C:\Users\kaitoy&amp;gt;minikube delete
Deleting local Kubernetes cluster...
Machine deleted.

C:\Users\kaitoy&amp;gt;minikube start --vm-driver virtualbox --kubernetes-version v1.7.0
Starting local Kubernetes v1.7.0 cluster...
Starting VM...
Getting VM IP address...
Moving files into cluster...
Setting up certs...
Connecting to cluster...
Setting up kubeconfig...
Starting cluster components...
Kubectl is now configured to use the cluster.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;次に定義ファイルを書いていく。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://v1-7.docs.kubernetes.io/docs/api-reference/v1.7/#deployment-v1beta1-apps&#34;&gt;APIリファレンスのDeploymentのとこ&lt;/a&gt;をみると、Kubernetes Basicsの最終形と同じようなDeploymentを作る定義は以下のように書ける。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: goslings-sample
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: goslings
        ver: latest
    spec:
      containers:
        - name: goslings
          image: kaitoy/goslings:latest
          ports:
            - name: http
              containerPort: 8080
          args:
            - &#39;8080&#39;
            - /tmp
            - https://github.com/kaitoy/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同様に、Serviceは、&lt;a href=&#34;https://v1-7.docs.kubernetes.io/docs/api-reference/v1.7/#service-v1-core&#34;&gt;APIリファレンスのServiceのとこ&lt;/a&gt;みると以下のように書ける。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;kind: Service
apiVersion: v1
metadata:
  name: goslings-sample
spec:
  ports:
    - name: http
      port: 8080
      targetPort: 8080
  selector:
    app: goslings
  type: NodePort
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;で、それぞれのYAMLファイルを&lt;code&gt;kubectl create&lt;/code&gt;に渡してやると、Goslingsデプロイ完了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy\kubeTest&amp;gt;kubectl create -f deploy_goslings.yml
deployment &amp;quot;goslings-sample&amp;quot; created

C:\Users\kaitoy\kubeTest&amp;gt;kubectl create -f service_goslings.yml
service &amp;quot;goslings-sample&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;オブジェクトの種類もパラメータも大量にあるので、使いこなすのは難しそう。&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Kubernetes 1.8が出たので、Minikubeを触ってみる</title>
          <link>https://www.kaitoy.xyz/2017/10/10/goslings-on-kubernetes/</link>
          <pubDate>Tue, 10 Oct 2017 00:10:59 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2017/10/10/goslings-on-kubernetes/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt;1.8のリリースが話題になっていたので、ちょっと触って見たという話。
(1.8を触ったとは言っていない。)&lt;/p&gt;

&lt;p&gt;具体的には、&lt;a href=&#34;https://kubernetes.io/docs/tutorials/kubernetes-basics/&#34;&gt;Kubernetes Basics&lt;/a&gt;というチュートリアルをやりながら、&lt;a href=&#34;https://github.com/kubernetes/minikube&#34;&gt;Minikube&lt;/a&gt;に&lt;a href=&#34;https://kaitoy.github.io/goslings/&#34;&gt;Goslings&lt;/a&gt;をデプロイしたんだけど、この記事ではMinikubeをセットアップしたところまで。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h2 id=&#34;kubernetesとは&#34;&gt;Kubernetesとは&lt;/h2&gt;

&lt;p&gt;KubernetesはOSSのコンテナオーケストレーションツール。
英語だとクーバネティスみたいに発音する。
Googleが自身のコンテナ技術である&lt;a href=&#34;https://research.google.com/pubs/pub43438.html&#34;&gt;Borg&lt;/a&gt;の運用で培ったノウハウを活かして開発したもの。
2014年ころに開発が始まり、2015年夏にv1がリリースされたということで、かなり新しいツール。
よく比べられるものには&lt;a href=&#34;https://docs.docker.com/engine/swarm/&#34;&gt;DockerのSwarmモード&lt;/a&gt;や&lt;a href=&#34;http://mesos.apache.org/&#34;&gt;Apache Mesos&lt;/a&gt;があるが、何が違うのかは調べてないので知らない。
ただ、Dockerコンテナ管理ツールとしてはKubernetesが一番勢いがある雰囲気を感じる。&lt;/p&gt;

&lt;p&gt;(2017/10/18追記: &lt;a href=&#34;http://www.publickey1.jp/blog/17/dockerkubernetesdockercon_eu_2017.html&#34;&gt;DockerがKubernetesとの統合を発表&lt;/a&gt;した。KubernetesはDockerネイティブなツールになり、Dockerとともにインストールされ、Docker ComposeのConposeファイルでデプロイできるようになったりする。Kubernetesの大勝利っぽい。)&lt;/p&gt;

&lt;p&gt;Kubernetesを使うと、複数の物理マシンからなるHAクラスタ(Kubernetesクラスタ)を構成し、その上にコンテナをデプロイして管理できる。
Kubernetesクラスタは、一組のMasterコンポーネント群(a.k.a. Kubernetes Control Plane、または単にMaster)と一つ以上のNode(昔はMinionと呼ばれてたもの)で構成される。
Nodeは、Masterの管理下でコンテナを実行する機能を備えた、一台のVMや物理マシン。
MasterはNode上で動き、クラスタを管理し、コンテナのスケジューリング、状態管理、スケーリング、アップデートなどを担う。&lt;/p&gt;

&lt;p&gt;Kubernetesの&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/architecture.md&#34;&gt;アーキテクチャ&lt;/a&gt;を図にすると以下の感じ。
矢印の向きとかはちょっと間違ってるかも。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/goslings-on-kubernetes/architecture.png&#34; alt=&#34;architecture&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;ごちゃごちゃするので省いたけど、図の下部のNode内のコンポーネントは、他のNode内でも動いている。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Masterには&lt;a href=&#34;https://kubernetes.io/docs/admin/kube-apiserver/&#34;&gt;kube-apiserver&lt;/a&gt;が含まれていて、&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/kubernetes-api/&#34;&gt;Kubernetes API&lt;/a&gt;というREST APIを公開する。
このAPIを通して&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/&#34;&gt;Kubernetesオブジェクト&lt;/a&gt;を定義したりすることで、宣言的にコンテナの管理ができる仕組み。
ユーザは普通、&lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl/&#34;&gt;kubectl&lt;/a&gt;(キューブシーティーエル)というコマンドでkube-apiserverとやり取りする。&lt;/p&gt;

&lt;p&gt;KubernetesオブジェクトはMasterの&lt;a href=&#34;https://github.com/coreos/etcd&#34;&gt;etcd&lt;/a&gt;によって分散キーバリューストアに永続化され、そのストアを&lt;a href=&#34;https://kubernetes.io/docs/admin/kube-controller-manager/&#34;&gt;kube-controller-manager&lt;/a&gt;と&lt;a href=&#34;https://kubernetes.io/docs/admin/kube-scheduler/&#34;&gt;kube-scheduler&lt;/a&gt;がwatchしてて、変更に応じた処理をする。&lt;/p&gt;

&lt;p&gt;kube-controller-managerは、ノードの管理や、オブジェクトのライフサイクルの管理や、コンテナのスケーリングなど、クラスタレベルの機能を実行する。
(よくわからない。)&lt;/p&gt;

&lt;p&gt;kube-schedulerは、コンテナを実行するホストを選出し、コンテナのスケジューリングをする。&lt;/p&gt;

&lt;p&gt;あと、図にはないけど、&lt;a href=&#34;https://kubernetes.io/docs/admin/cloud-controller-manager/&#34;&gt;cloud-controller-manager&lt;/a&gt;というのがMasterで動くこともあって、クラウドプラットフォームとやり取りしてクラウド固有の仕事をするらしい。
クラウドベンダじゃなければ気にしなくて良さそう。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;一方、各Nodeでは、&lt;a href=&#34;https://kubernetes.io/docs/admin/kubelet/&#34;&gt;kubelet&lt;/a&gt;(キューブレット)というMasterのエージェントになるプロセスが動く。&lt;/p&gt;

&lt;p&gt;kubeletはkube-apiserverからの指示で、コンテナイメージを取得してコンテナを起動したり監視したり止めたりする。&lt;/p&gt;

&lt;p&gt;kubeletがコンテナを扱うためのコンテナランタイムは、普通は&lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt;だけど、&lt;a href=&#34;https://coreos.com/rkt/&#34;&gt;rkt&lt;/a&gt;とか&lt;a href=&#34;https://github.com/kubernetes-incubator/cri-o&#34;&gt;cri-o&lt;/a&gt;とか&lt;a href=&#34;https://github.com/kubernetes/frakti&#34;&gt;frakti&lt;/a&gt;とかも使える。&lt;a href=&#34;https://github.com/opencontainers/runc&#34;&gt;runc&lt;/a&gt;や&lt;a href=&#34;https://github.com/oracle/railcar&#34;&gt;RailCar&lt;/a&gt;はどうなんだろう。&lt;/p&gt;

&lt;p&gt;コンテナはデフォルトではクラスタ内のプライベートネットワークにつながるので、そこで動いているアプリにユーザからアクセスするには、何らかの形でトラフィックを中継してやる必要がある。
これをするのが&lt;a href=&#34;https://kubernetes.io/docs/admin/kube-proxy/&#34;&gt;kube-proxy&lt;/a&gt;。
ロードバランシングもしてくれる。&lt;/p&gt;

&lt;h2 id=&#34;kubernetesオブジェクトとは&#34;&gt;Kubernetesオブジェクトとは&lt;/h2&gt;

&lt;p&gt;Kubernetesオブジェクトは、Kubernetesクラスタ上で機能する構成要素を表現するもの。
オブジェクトは&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/#object-spec-and-status&#34;&gt;specとstatus&lt;/a&gt;を持ち、オブジェクトに期待する状態やふるまい(spec)を定義しておくと、Kubernetesが実際の状態(status)をそれに合わせてくれる。
宣言的。&lt;/p&gt;

&lt;p&gt;オブジェクトには以下のようなものがある。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/&#34;&gt;Pod&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;デプロイの最小単位。
一つ(またはリソースを共有する複数)のコンテナと、ストレージ、ネットワークなどを内包する。
一つのPodには一つのIPアドレスが付く。&lt;/p&gt;

&lt;p&gt;kubeletはPodの定義に従ってコンテナを起動する。&lt;/p&gt;

&lt;p&gt;因みに、etcd以外のMasterコンポーネントもPodとしてデプロイされる。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/&#34;&gt;Service&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Podの論理グループ。
PodのIPアドレスは外部に公開されないので、外とのやり取りをするためにServiceがある。
kube-proxyはこいつの定義に従って働く。&lt;/p&gt;

&lt;p&gt;Serviceには複数のEndpoint(i.e. Pod等)が紐づき、外部からのトラフィックをラウンドロビンでルーティングするので、冗長化やロードバランサ的な働きもする。
ServiceはPodを抽象化するので、Podが死んだり入れ替わったりしても外に影響が見えにくくなる。&lt;/p&gt;

&lt;p&gt;Serviceには以下のtypeがある。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ClusterIP (デフォルト): Kubernetesクラスタ内からだけアクセスできる内部IPアドレスだけをもつ。&lt;/li&gt;
&lt;li&gt;NodePort: ClusterIPの拡張。内部IPアドレスに加え、クラスタ外からアクセスできるポートを一つ持つ。&lt;/li&gt;
&lt;li&gt;LoadBalancer: NodePortの拡張。外部ロードバランサを作って、固定の外部IPアドレスを付けて、内部IPアドレスへルーティングする。&lt;/li&gt;
&lt;li&gt;ExternalName: 抽象名をもつサービス。Kubernetes DNS serverで名前解決する。&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/&#34;&gt;詳細&lt;/a&gt;は読んでないので知らない。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/&#34;&gt;Volume&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;永続化やPod内でのファイル共有のためのオブジェクト。
Podとともに作られ、Podとともに破棄される。
実態はファイルシステム上のディレクトリ。
hostPathとか、nfsとか、awsElasticBlockStoreとかの種類があるらしい。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/&#34;&gt;Namespace&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;仮想クラスタを表すオブジェクト。
これを定義すると、ひとつの物理クラスタを複数の仮想クラスタに分割できる。
大規模ユーザ・プロジェクト向け機能。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Controller&lt;/p&gt;

&lt;p&gt;Podを管理するオブジェクト。レプリケーションしたり、スケーリングや自動再起動したり。&lt;/p&gt;

&lt;p&gt;以下のようなものがある。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/deployment/&#34;&gt;Deployment&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Podのデプロイを管理するオブジェクト。
PodとReplicaSetの宣言的な生成・更新を実現する。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/&#34;&gt;ReplicaSet&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;指定した数のPodのレプリカを維持してくれる。
基本はDeploymentから作られて、Podの作成・削除・更新をオーケストレイトする。
&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/&#34;&gt;ReplicationController&lt;/a&gt;というのもあるけど、今はReplicaSetが推奨。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/&#34;&gt;StatefulSet&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ステートフルなアプリを管理するオブジェクト。
現時点でのKubernetes最新版の1.8でまだベータ。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/&#34;&gt;DaemonSet&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;全てのノードで動くアプリを実現するオブジェクト。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/&#34;&gt;Job&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ジョブを表すオブジェクト。
指定された回数、Podを成功で完了させる。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;オブジェクトには&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/&#34;&gt;ラベル&lt;/a&gt;というキーバリューな属性を付けることができ、PodとServiceの紐づけや、オブジェクト検索時のフィルタとかに使える。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;今回Goslingsを動かすのに使ったのは、Pod、Deployment、ReplicaSet、Service (NodePort)。&lt;/p&gt;

&lt;h2 id=&#34;podネットワーク&#34;&gt;Podネットワーク&lt;/h2&gt;

&lt;p&gt;ちょっと細かい話だけど、Pod間の通信はどうなっているかという話についてちょっと調べたのでざっくり書いておく。&lt;/p&gt;

&lt;p&gt;普通の&lt;a href=&#34;https://www.kaitoy.xyz/2015/07/25/how-to-capture-packets-on-a-local-network-with-pcap4j-container/#docker-network&#34;&gt;Dockerネットワーク&lt;/a&gt;だと、コンテナはdocker0という仮想ブリッジ上のプライベートネットワークで動くため、同じホスト上のコンテナ間は通信できるけど、別のホスト上のコンテナ通信させたい場合は、ホストのIPアドレスのポートを割り当ててやらなければいけない。&lt;/p&gt;

&lt;p&gt;これはめんどいので、Kubernetesは、各Podに一意なIPアドレスを与え、Podがどのホストにいるかにかかわらず、NAT無しで相互に通信できる&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/networking/&#34;&gt;ネットワーク&lt;/a&gt;を提供する。
これがPodネットワークとか呼ばれ、その仕様は&lt;a href=&#34;https://github.com/containernetworking/cni&#34;&gt;CNI&lt;/a&gt;でオープンに定められていて、以下のような実装がある。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/kubeadm/&#34;&gt;Calico&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/projectcalico/canal/tree/master/k8s-install&#34;&gt;Canal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/coreos/flannel&#34;&gt;Flannel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cloudnativelabs/kube-router/blob/master/Documentation/kubeadm.md&#34;&gt;Kube-router&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/romana/romana/tree/master/containerize#using-kubeadm&#34;&gt;Romana&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.weave.works/docs/net/latest/kube-addon/&#34;&gt;Weave Net&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;minikubeとは&#34;&gt;Minikubeとは&lt;/h2&gt;

&lt;p&gt;Kubernetesクラスタを構築する方法は&lt;a href=&#34;https://kubernetes.io/docs/setup/pick-right-solution/&#34;&gt;いくつかある&lt;/a&gt;が、中でももっとも簡単な方法がMinikube。&lt;/p&gt;

&lt;p&gt;Minikubeは、単一NodeのKubernetesクラスタを詰めたVMをダウンロードして起動して、ローカルのkubectlから使えるようにしてくれるツール。
Linux、Windows、OS Xで動き、開発やテスト用途のKubernetes環境として使われる。&lt;/p&gt;

&lt;p&gt;ちょっと&lt;a href=&#34;https://www.vagrantup.com/&#34;&gt;Vagrant&lt;/a&gt;っぽい感じ。Kubernetes専用の。&lt;/p&gt;

&lt;h2 id=&#34;minikubeインストール&#34;&gt;Minikubeインストール&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-minikube/&#34;&gt;Kubernetesのドキュメント&lt;/a&gt;にしたがって、Minikubeをインストールする。
環境はWindows 10 Home x64。&lt;/p&gt;

&lt;p&gt;まず、MinikubeのVMを動かす仮想化ツールを入れる。
今のところMinikubeがサポートしてるのは、Windowsだと&lt;a href=&#34;https://www.virtualbox.org/&#34;&gt;VirtualBox&lt;/a&gt;か&lt;a href=&#34;https://docs.microsoft.com/ja-jp/virtualization/hyper-v-on-windows/quick-start/enable-hyper-v&#34;&gt;Hyper-V&lt;/a&gt;。
Windows 10 HomeだとHyper-Vが使えないので、VirtualBox一択。
VirtualBoxは、適当にVT-xを有効にして(してあった)、インストーラダウンロードしてインストールしただけ。
バージョンは5.1.28。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;次に、minikubeコマンドを入れる。
このコマンドはGoで書かれていて、各プラットフォーム用にビルドされたバイナリがGitHubのプロジェクトページの&lt;a href=&#34;https://github.com/kubernetes/minikube/releases&#34;&gt;Releases&lt;/a&gt;に上がってるので、ダウンロードしてPathの通ったとこに置くだけ。
今回ダウンロードしたのはv0.22.2のminikube-windows-amd64で、これをminikube.exeにリネームして配置した。&lt;/p&gt;

&lt;p&gt;で、minikubeがサポートしているKubernetesのバージョンを調べると、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;minikube get-k8s-versions
The following Kubernetes versions are available:
        - v1.7.5
        - v1.7.4
        - v1.7.3
        - v1.7.2
        - v1.7.0
        (snip)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1.8はまだサポートされていない…&lt;/p&gt;

&lt;p&gt;1.7.5が最新なのでそれでやることにする。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;ということで、kubectlの1.7.5をインストールする。
kubectlもGoで書かれているので、以下のアドレスからWindowsバイナリをダウンロードしてPathの通ったところに置くだけ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;https://storage.googleapis.com/kubernetes-release/release/v1.7.5/bin/windows/amd64/kubectl.exe
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;以上でMinikubeの環境ができた。
簡単。&lt;/p&gt;

&lt;h2 id=&#34;minikube起動&#34;&gt;Minikube起動&lt;/h2&gt;

&lt;p&gt;Minikubeは、&lt;code&gt;minikube start&lt;/code&gt;で起動することができ、Minikubeが起動したらすぐにKubernetesをいじれるようになる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;minikube start --vm-driver virtualbox --kubernetes-version v1.7.5
Starting local Kubernetes v1.7.5 cluster...
Starting VM...
Downloading Minikube ISO
 106.37 MB / 106.37 MB [============================================] 100.00% 0s
Getting VM IP address...
Moving files into cluster...
Setting up certs...
Connecting to cluster...
Setting up kubeconfig...
Starting cluster components...
Kubectl is now configured to use the cluster.

C:\Users\kaitoy&amp;gt;minikube status
minikube: Running
cluster: Running
kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;起動した。
VirtualBoxのGUIを見ると、minikubeというVMが起動しているのが分かる。
この中でKubernetesクラスタが動いているはずだ。&lt;/p&gt;

&lt;p&gt;このVMには、&lt;code&gt;minikube ssh&lt;/code&gt;でログインできる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;minikube ssh
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/&#39;)  _   _ | |_      __
/&#39; _ ` _ `\| |/&#39; _ `\| || , &amp;lt;  ( ) ( )| &#39;_`\  /&#39;__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/&#39;(_,__/&#39;`\____)

$ uname -a
Linux minikube 4.9.13 #1 SMP Fri Sep 15 23:35:16 UTC 2017 x86_64 GNU/Linux
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;すごくVagrantっぽい。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Minikubeを起動すると、kubectlのコンテキストがminikubeというものに設定され、kubectlコマンドの接続先がMinikubeのKubernetesになる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl config current-context
minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;で、kubectlでクラスタの状態とかを見てみようと思ったら、なんか様子が変。
なしのつぶて。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;kubectl get nodes
Unable to connect to the server: dial tcp 192.168.99.100:8443: connectex: No connection could be made because the target machine actively refused it.

C:\Users\kaitoy&amp;gt;kubectl cluster-info dump
Unable to connect to the server: dial tcp 192.168.99.100:8443: connectex: No connection could be made because the target machine actively refused it.

C:\Users\kaitoy&amp;gt;minikube dashboard
Could not find finalized endpoint being pointed to by kubernetes-dashboard: Error validating service: Error getting service kubernetes-dashboard: Get https://192.168.99.100:8443/api/v1/namespaces/kube-system/services/kubernetes-dashboard: dial tcp 192.168.99.100:8443: connectex: No connection could be made because the target machine actively refused it.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再度&lt;code&gt;minikube status&lt;/code&gt;してみたら、クラスタが落ちていた。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;minikube status
minikube: Running
cluster: Stopped
kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;minikube logs&lt;/code&gt;でログを見てみると、エラーがたくさん出ていた。
以下のようなログが最初のほうに出てたので、認証系がだめで、サービス間でやり取りができなかったんじゃないかという感じ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Oct 04 23:08:43 minikube localkube[2783]: W1004 23:08:43.599396    2783 authentication.go:368] AnonymousAuth is not allowed with the AllowAll authorizer.  Resetting AnonymousAuth to false. You should use a different authorizer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;エラーの原因はよくわからないので、Kubernetesのバージョンをちょっと古いの(1.7.0)変えてみる。&lt;/p&gt;

&lt;p&gt;kubectlの1.7.0をPathに置いて、Minikubeを1.7.0で再起動する。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;minikube stop
Stopping local Kubernetes cluster...
Machine stopped.

C:\Users\kaitoy&amp;gt;minikube start --vm-driver virtualbox --kubernetes-version v1.7.0
Starting local Kubernetes v1.7.0 cluster...
Starting VM...
Getting VM IP address...
Kubernetes version downgrade is not supported. Using version: v1.7.5
Moving files into cluster...
Setting up certs...
Connecting to cluster...
Setting up kubeconfig...
Starting cluster components...
Kubectl is now configured to use the cluster.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Kubernetesのダウングレードはサポートされてないと言われた。
ので一回VMを消してからやりなおす。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;minikube stop
Stopping local Kubernetes cluster...
Machine stopped.

C:\Users\kaitoy&amp;gt;minikube delete
Deleting local Kubernetes cluster...
Machine deleted.

C:\Users\kaitoy&amp;gt;minikube start --vm-driver virtualbox --kubernetes-version v1.7.0
Starting local Kubernetes v1.7.0 cluster...
Starting VM...
Getting VM IP address...
Moving files into cluster...
Setting up certs...
Connecting to cluster...
Setting up kubeconfig...
Starting cluster components...
Kubectl is now configured to use the cluster.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1.7.0で動いた。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;様子はどうか。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy\Desktop\bin\pleiades\workspace\blog&amp;gt;minikube status
minikube: Running
cluster: Running
kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100

C:\Users\kaitoy\Desktop\bin\pleiades\workspace\blog&amp;gt;kubectl get nodes
NAME       STATUS    AGE       VERSION
minikube   Ready     22s       v1.7.0

C:\Users\kaitoy\Desktop\bin\pleiades\workspace\blog&amp;gt;kubectl version
Client Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;7&amp;quot;, GitVersion:&amp;quot;v1.7.0&amp;quot;, GitCommit:&amp;quot;d3ada0119e776222f11ec7945e6d860061339aad&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2017-06-29T23:15:59Z&amp;quot;, GoVersion:&amp;quot;go1.8.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;windows/amd64&amp;quot;}
Server Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;7&amp;quot;, GitVersion:&amp;quot;v1.7.0&amp;quot;, GitCommit:&amp;quot;d3ada0119e776222f11ec7945e6d860061339aad&amp;quot;, GitTreeState:&amp;quot;dirty&amp;quot;, BuildDate:&amp;quot;2017-10-04T09:25:40Z&amp;quot;, GoVersion:&amp;quot;go1.8.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ちゃんと動いているっぽい。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;ダッシュボードだけはなぜか相変わらず開けないけどまあいいか。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\kaitoy&amp;gt;minikube dashboard
Could not find finalized endpoint being pointed to by kubernetes-dashboard: Error validating service: Error getting service kubernetes-dashboard: services &amp;quot;kubernetes-dashboard&amp;quot; not found
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;続きはまた&lt;a href=&#34;https://www.kaitoy.xyz/2017/10/11/goslings-on-kubernetes-cont/&#34;&gt;別の記事&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;因みに、ベーシック認証ありのプロキシ環境でMinikube on Windowsする場合は、まず以下の環境変数を設定:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;NO_PROXY&lt;/code&gt;: 192.168.99.100&lt;/li&gt;
&lt;li&gt;&lt;code&gt;http_proxy&lt;/code&gt;: username:password@domain.com:port&lt;/li&gt;
&lt;li&gt;&lt;code&gt;https_proxy&lt;/code&gt;: username:password@domain.com:port&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;NO_PROXY&lt;/code&gt;の値は&lt;code&gt;minikube ip&lt;/code&gt;の値。
で、&lt;/p&gt;

&lt;p&gt;&lt;code&gt;minikube start --docker-env HTTP_PROXY=http://%http_proxy% --docker-env HTTPS_PROXY=https://%https_proxy% --docker-env NO_PROXY=%NO_PROXY%&lt;/code&gt;みたいにすればできる。&lt;/p&gt;

&lt;p&gt;はず。(参考: &lt;a href=&#34;https://github.com/kubernetes/minikube/issues/530&#34;&gt;https://github.com/kubernetes/minikube/issues/530&lt;/a&gt;)&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>WebdriverIOとChromeのヘッドレスモードで自動ブラウザテストするDockerイメージ: webdriverio-chrome</title>
          <link>https://www.kaitoy.xyz/2017/08/14/webdriverio-chrome/</link>
          <pubDate>Mon, 14 Aug 2017 10:53:17 JST</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2017/08/14/webdriverio-chrome/</guid>
          <description>

&lt;p&gt;「&lt;a href=&#34;https://www.kaitoy.xyz/2017/08/04/browser-test-framework/&#34;&gt;2017年夏、ブラウザテストフレームワーク&lt;/a&gt;」の続き。
&lt;a href=&#34;https://www.servicenow.com/ja&#34;&gt;ServiceNow&lt;/a&gt;アプリケーションのブラウザテストをしたくて色々調べている。
前回は、フレームワークに&lt;a href=&#34;http://webdriver.io/&#34;&gt;WebdriverIO&lt;/a&gt;を使うと決めたところまで書いた。&lt;/p&gt;

&lt;p&gt;今回、最終的に、&lt;a href=&#34;http://webdriver.io/&#34;&gt;WebdriverIO&lt;/a&gt;、&lt;a href=&#34;http://webdriver.io/guide/testrunner/gettingstarted.html&#34;&gt;WDIO&lt;/a&gt;、&lt;a href=&#34;https://github.com/vvo/selenium-standalone&#34;&gt;selenium-standalone&lt;/a&gt;、&lt;a href=&#34;https://jasmine.github.io/&#34;&gt;Jasmine&lt;/a&gt;と、Chromeのヘッドレスモードを使って、Dockerコンテナ(&lt;a href=&#34;https://alpinelinux.org/&#34;&gt;Alpine Linux&lt;/a&gt;)上でテストスクリプトを実行して、ServiceNowのログイン画面のスクリーンショットが取れるところまでできた。&lt;/p&gt;

&lt;p&gt;そのコンテナイメージのDockerfileは&lt;a href=&#34;https://github.com/kaitoy/webdriverio-chrome&#34;&gt;GitHubに置いた&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h2 id=&#34;とりあえずalpine-linux&#34;&gt;とりあえずAlpine Linux&lt;/h2&gt;

&lt;p&gt;テスト環境の作成は自宅でやってるけど、DockerイメージにしてDocker Hubとかに上げておけば、社内でダウンロードしてそのまま再現できる。
ダウンロードに係る社内手続きも、Dockerイメージだけに対してやればいいので、中に何を詰め込んでも、後でライブラリとか追加しても、一回こっきりで済む。&lt;/p&gt;

&lt;p&gt;というわけでWebdriverIO環境をDockerコンテナとしてつくることにする。
とりあえず、自PC(Windows 10 Home x64)に入ってるVMware Workstation Player 12.5.5でCentOS 7 x64のVMを作り、そこにDockerをインストールした。&lt;/p&gt;

&lt;p&gt;次に、そのDockerを使って、WebdriverIO環境のベースにするAlpine Linuxをpullする。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;$ docker pull alpine:edge
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Alpine Linuxは&lt;a href=&#34;https://busybox.net/&#34;&gt;BusyBox&lt;/a&gt;と&lt;a href=&#34;https://www.musl-libc.org/&#34;&gt;musl libc&lt;/a&gt;で構成された軽量な Linuxディストリビューション。
2016年2月に&lt;a href=&#34;https://www.brianchristner.io/docker-is-moving-to-alpine-linux/&#34;&gt;すべてのオフィシャルDockerイメージがAlpine Linuxベースになる&lt;/a&gt;というアナウンスがあったし、他にそれっぽいものもなかったので、これをベースに環境を作ることにした。
&lt;a href=&#34;https://www.gnu.org/software/libc/&#34;&gt;glibc&lt;/a&gt;じゃないのがちょっと気になるけど、まあ問題ないか。&lt;/p&gt;

&lt;p&gt;現在、&lt;a href=&#34;https://pkgs.alpinelinux.org/package/edge/community/x86_64/chromium&#34;&gt;Chrome 59&lt;/a&gt;のAlpine Linuxパッケージはedgeブランチ(i.e. 開発ブランチ)でしか作られていない。
pullするタグをedgeにしたのはそのため。
(因みに現時点でAlpine Linuxのlatestは3.6。)&lt;/p&gt;

&lt;p&gt;で、起動。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;$ docker run -it alpine:edge sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;chrome-chromium-インストール&#34;&gt;Chrome(Chromium)インストール&lt;/h2&gt;

&lt;p&gt;まずはChrome(がAlpine Linuxパッケージにはないので、実際にはChromium)と、ついでにChromeDriverをインストールする。
Alpine Linux独自のパッケージマネージャーである&lt;a href=&#34;https://wiki.alpinelinux.org/wiki/Alpine_Linux_package_management&#34;&gt;apk&lt;/a&gt;を使う。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;/ # apk add --update chromium chromium-chromedriver
/ # chromium-browser -version
Chromium 59.0.3071.115
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;無事インストールできた。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://bufferings.hatenablog.com/entry/2017/05/03/181713&#34;&gt;この記事&lt;/a&gt;を参考にヘッドレスモードで実行してみる。
ヘッドレスモードにするために&lt;code&gt;--headless&lt;/code&gt;を付けて、一時的な制限事項で&lt;code&gt;--disable-gpu&lt;/code&gt;を付ける必要があって、コンテナの権限不足を回避するために&lt;code&gt;--no-sandbox&lt;/code&gt;を付ける。
(コンテナの権限不足回避には他に、&lt;code&gt;docker run&lt;/code&gt;に&lt;code&gt;--privileged&lt;/code&gt;や&lt;code&gt;--cap-add=SYS_ADMIN&lt;/code&gt;付ける&lt;a href=&#34;https://github.com/yukinying/chrome-headless-browser-docker&#34;&gt;方法がある&lt;/a&gt;。)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;/ # chromium-browser --headless --no-sandbox --disable-gpu https://example.com/
[0811/145902.894023:WARNING:dns_config_service_posix.cc(326)] Failed to read DnsConfig.
[0811/145902.906137:FATAL:udev_loader.cc(38)] Check failed: false.
Received signal 6
  r8: 0000000000000061  r9: 00007fe3fe01c066 r10: 0000000000000008 r11: 0000000000000246
 r12: 00007fe3fe01bed0 r13: 00007fe3fe01be80 r14: 0000000000000000 r15: 0000000000000000
  di: 0000000000000002  si: 00007fe3fe01bda0  bp: 00007fe3fe01bda0  bx: 0000000000000006
  dx: 0000000000000000  ax: 0000000000000000  cx: ffffffffffffffff  sp: 00007fe3fe01bd88
  ip: 00007fe412a2f769 efl: 0000000000000246 cgf: 0000000000000033 erf: 0000000000000000
 trp: 0000000000000000 msk: 0000000000000000 cr2: 0000000000000000
[end of stack trace]
Calling _exit(1). Core file will not be generated.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;エラーになった。&lt;/p&gt;

&lt;p&gt;最初のWARNINGは置いといて、FATALのほうは、udev_loader.ccというのでエラーになってる。&lt;/p&gt;

&lt;p&gt;エラーメッセージでググったら、&lt;a href=&#34;http://qiita.com/dd511805/items/dfe03c5486bf1421875a&#34;&gt;Qiitaに同じエラーを解決している記事&lt;/a&gt;が。
よくわからないが、&lt;a href=&#34;https://pkgs.alpinelinux.org/package/v3.5/main/x86_64/udev&#34;&gt;udev&lt;/a&gt;と&lt;a href=&#34;https://pkgs.alpinelinux.org/package/v3.6/main/x86_64/ttf-freefont&#34;&gt;ttf-freefont&lt;/a&gt;を入れればいいらしい。
深く考えずにそれに従うことにする。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;/ # apk add udev ttf-freefont
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;で、再度実行。
(ちゃんと動いてるか分かりやすくするために&lt;code&gt;--dump-dom&lt;/code&gt;も付けた。)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;/ # chromium-browser --headless --no-sandbox --disable-gpu --dump-dom https://example.com/
[0811/151303.698629:WARNING:dns_config_service_posix.cc(326)] Failed to read DnsConfig.
&amp;lt;body&amp;gt;
&amp;lt;div&amp;gt;
    &amp;lt;h1&amp;gt;Example Domain&amp;lt;/h1&amp;gt;
    &amp;lt;p&amp;gt;This domain is established to be used for illustrative examples in documents. You may use this
    domain in examples without prior coordination or asking for permission.&amp;lt;/p&amp;gt;
    &amp;lt;p&amp;gt;&amp;lt;a href=&amp;quot;http://www.iana.org/domains/example&amp;quot;&amp;gt;More information...&amp;lt;/a&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;


&amp;lt;/body&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;動いた!&lt;/p&gt;

&lt;h2 id=&#34;フォント追加&#34;&gt;フォント追加&lt;/h2&gt;

&lt;p&gt;前節で参考にした&lt;a href=&#34;http://qiita.com/dd511805/items/dfe03c5486bf1421875a&#34;&gt;Qiitaの記事&lt;/a&gt;に、文字化け対策としてフォントを追加する手順も書いてあったのでそれもやる。&lt;/p&gt;

&lt;p&gt;まず試しに、何もしないでスクリーンショットを撮ってみる。
&lt;code&gt;--screenshot&lt;/code&gt;オプションで。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;/ # chromium-browser --headless --no-sandbox --disable-gpu --screenshot https://www.google.co.jp/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;するとやはり文字化けしている。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/webdriverio-chrome/garblings.png&#34; alt=&#34;garblings.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.google.com/get/noto/&#34;&gt;Google Noto Fonts&lt;/a&gt;を入れて対応する。
(因みにNotoはNo Tofuの略で、文字化けした時に出る、クエスチョンマークが乗った豆腐の撲滅を目指して開発されたフォント。)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;/ # apk add curl
/ # cd /tmp/
/tmp # curl https://noto-website.storage.googleapis.com/pkgs/NotoSansCJKjp-hinte
/tmp # unzip NotoSansCJKjp-hinted.zip
/tmp # mkdir -p /usr/share/fonts/noto
/tmp # cp *.otf /usr/share/fonts/noto
/tmp # chmod 644 -R /usr/share/fonts/noto/
/tmp # fc-cache -fv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;後半に実行してるコマンドの詳細はよくわからないが、文字化けは直った。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/webdriverio-chrome/garblings_fixed.png&#34; alt=&#34;garblings_fixed.png&#34; /&gt;
&lt;/p&gt;

&lt;h2 id=&#34;webdriverioインストール&#34;&gt;WebdriverIOインストール&lt;/h2&gt;

&lt;p&gt;次にWebdriverIOをインストールする。
&lt;a href=&#34;https://yarnpkg.com/lang/en/&#34;&gt;Yarn&lt;/a&gt;でインストールして&lt;a href=&#34;https://nodejs.org/ja/&#34;&gt;Node.js&lt;/a&gt;で動かすので、まずそれらをapkで入れる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;/tmp # apk add nodejs yarn
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;で、プロジェクトを作ってWebdriverIOを追加。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;/tmp # mkdir /root/webdriverio-chrome
/tmp # cd /root/webdriverio-chrome
~/webdriverio-chrome # yarn init
~/webdriverio-chrome # yarn add webdriverio --dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;package.jsonのscriptsを編集して、WebdriverIO付属のテストランナであるWDIOを使えるようにする。&lt;/p&gt;

&lt;p&gt;package.json:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;name&amp;quot;: &amp;quot;webdriverio-chrome&amp;quot;,
  &amp;quot;version&amp;quot;: &amp;quot;0.0.1&amp;quot;,
  &amp;quot;description&amp;quot;: &amp;quot;Browser test stack with WebdriverIO and headless Chrome&amp;quot;,
  &amp;quot;scripts&amp;quot;: {
    &amp;quot;test&amp;quot;: &amp;quot;wdio&amp;quot;
  },
  &amp;quot;author&amp;quot;: &amp;quot;Kaito Yamada&amp;quot;,
  &amp;quot;license&amp;quot;: &amp;quot;MIT&amp;quot;,
  &amp;quot;devDependencies&amp;quot;: {
    &amp;quot;webdriverio&amp;quot;: &amp;quot;^4.8.0&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;wdioの設定ファイル生成&#34;&gt;WDIOの設定ファイル生成&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://webdriver.io/guide/testrunner/gettingstarted.html&#34;&gt;WDIOのconfigコマンド&lt;/a&gt;でWDIO Configuration Helperを起動し、設定ファイルwdio.conf.jsをインタラクティブに生成する。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;~/webdriverio-chrome # yarn test -- config
yarn test v0.27.5
$ wdio &amp;quot;config&amp;quot;

=========================
WDIO Configuration Helper
=========================

? Where do you want to execute your tests? On my local machine
? Which framework do you want to use? jasmine
? Shall I install the framework adapter for you? No
? Where are your test specs located? ./test/specs/**/*.js
? Which reporter do you want to use?  spec - https://github.com/webdriverio/wdio-spec-reporter
? Shall I install the reporter library for you? No
? Do you want to add a service to your test setup?  selenium-standalone - https://github.com/webdriverio/wdio-selenium-standalone-service
? Shall I install the services for you? No
? Level of logging verbosity verbose
? In which directory should screenshots gets saved if a command fails? ./errorShots/
? What is the base url? http://localhost

Configuration file was created successfully!
To run your tests, execute:

$ wdio wdio.conf.js

Done in 53.58s.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;WDIO Configuration Helperで、テストフレームワークは&lt;a href=&#34;https://mochajs.org/&#34;&gt;Mocha&lt;/a&gt;か&lt;a href=&#34;https://jasmine.github.io/&#34;&gt;Jasmine&lt;/a&gt;か&lt;a href=&#34;https://cucumber.io/&#34;&gt;Cucumber&lt;/a&gt;から&lt;a href=&#34;http://webdriver.io/guide/testrunner/frameworks.html&#34;&gt;選択できる&lt;/a&gt;。
ServiceNowのテストフレームワーク(ATF)がJasmine使ってるので、一応それに合わせてJasmineにした。
ATFは今のところ使うつもりはないけど。&lt;/p&gt;

&lt;p&gt;レポータ(標準出力へテスト結果を出力するコンポーネント)は妙に色々ある中から、雰囲気で&lt;a href=&#34;http://webdriver.io/guide/reporters/spec.html&#34;&gt;spec&lt;/a&gt;を選択。&lt;/p&gt;

&lt;p&gt;サービス(テスト実行に必要な準備などをしてくれるコンポーネント)には&lt;a href=&#34;http://webdriver.io/guide/services/selenium-standalone.html&#34;&gt;selenium-standalone&lt;/a&gt;を選択。
こいつは、テスト実行前に、npmパッケージの&lt;a href=&#34;https://www.npmjs.com/package/selenium-standalone&#34;&gt;selenium-standalone&lt;/a&gt;を使って&lt;a href=&#34;http://docs.seleniumhq.org/download/&#34;&gt;Selenium Server&lt;/a&gt;をダウンロードして起動したり、WebDriverをセットアップしてくれたりする。&lt;/p&gt;

&lt;p&gt;因みにサービスには他に、&lt;a href=&#34;http://webdriver.io/guide/services/browserstack.html&#34;&gt;browserstack&lt;/a&gt;とか&lt;a href=&#34;http://webdriver.io/guide/services/appium.html&#34;&gt;appium&lt;/a&gt;とか&lt;a href=&#34;http://webdriver.io/guide/services/phantomjs.html&#34;&gt;phantomjs&lt;/a&gt;とかがある。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Shall I install …&lt;/code&gt;という質問には全てnoで回答した。
でないとWDIOがnpm installしようとして、npmが無くて以下のようなエラーになるので。
(apkでは、npmは&lt;a href=&#34;https://pkgs.alpinelinux.org/package/edge/main/x86_64/nodejs&#34;&gt;nodejs&lt;/a&gt;パッケージに入ってなくて、&lt;a href=&#34;https://pkgs.alpinelinux.org/package/edge/main/x86_64/nodejs-npm&#34;&gt;nodejs-npm&lt;/a&gt;に入ってる。)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Installing wdio packages:
/root/webdriverio-chrome/node_modules/webdriverio/build/lib/cli.js:278
                    throw err;
                    ^

Error: Command failed: npm i -D wdio-jasmine-framework
/bin/sh: npm: not found

    at ChildProcess.exithandler (child_process.js:204:12)
    at emitTwo (events.js:106:13)
    at ChildProcess.emit (events.js:191:7)
    at maybeClose (internal/child_process.js:891:16)
    at Socket.&amp;lt;anonymous&amp;gt; (internal/child_process.js:342:11)
    at emitOne (events.js:96:13)
    at Socket.emit (events.js:188:7)
    at Pipe._handle.close [as _onclose] (net.js:497:12)
error Command failed with exit code 1.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;生成された設定ファイルは以下の感じ。(コメントは省略してる。)&lt;/p&gt;

&lt;p&gt;wdio.conf.js:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;exports.config = {
    specs: [
        &#39;./test/specs/**/*.js&#39;
    ],
    exclude: [
        // &#39;path/to/excluded/files&#39;
    ],
    maxInstances: 10,
    capabilities: [{
        maxInstances: 5,
        browserName: &#39;firefox&#39;,
    }],

    sync: true,
    logLevel: &#39;verbose&#39;,
    coloredLogs: true,
    bail: 0,
    screenshotPath: &#39;./errorShots/&#39;,
    baseUrl: &#39;http://localhost&#39;,
    waitforTimeout: 10000,
    connectionRetryTimeout: 90000,
    connectionRetryCount: 3,

    services: [&#39;selenium-standalone&#39;],

    framework: &#39;jasmine&#39;,
    reporters: [&#39;spec&#39;],
    jasmineNodeOpts: {
        defaultTimeoutInterval: 50000,
        expectationResultHandler: function(passed, assertion) {
            // do something
        }
    },
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;npmパッケージとjava追加&#34;&gt;npmパッケージとJava追加&lt;/h2&gt;

&lt;p&gt;WDIO Configuration Helperの&lt;code&gt;Shall I install …&lt;/code&gt;でnoした分は自分でインストールしておく。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;~/webdriverio-chrome # yarn add wdio-jasmine-framework wdio-spec-reporter wdio-selenium-standalone-service selenium-standalone --dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;したらエラー。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;error /root/webdriverio-chrome/node_modules/fibers: Command failed.
Exit code: 127
Command: sh
Arguments: -c node build.js || nodejs build.js
Directory: /root/webdriverio-chrome/node_modules/fibers
Output:
`linux-x64-48` exists; testing
Problem with the binary; manual build incoming
node-gyp not found! Please ensure node-gyp is in your PATH--
Try running: `sudo npm install -g node-gyp`
sh: nodejs: not found
spawn node-gyp ENOENT
info Visit https://yarnpkg.com/en/docs/cli/add for documentation about this command.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/nodejs/node-gyp&#34;&gt;node-gyp&lt;/a&gt;が無いと。
では追加する。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;~/webdriverio-chrome # yarn global add node-gyp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;node-gypのREADME.md読むと、PythonとmakeとC/C++コンパイラが要るとあるので、それも入れる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;~/webdriverio-chrome # apk add python make gcc g++
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;で、再度、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;~/webdriverio-chrome # yarn add wdio-jasmine-framework wdio-spec-reporter wdio-selenium-standalone-service selenium-standalone --dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;したら入った。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;あと、Selenium ServerがJavaで動くので、Javaも入れておく。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;~/webdriverio-chrome # apk add openjdk8
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;wdio-conf-jsの修正&#34;&gt;wdio.conf.jsの修正&lt;/h2&gt;

&lt;p&gt;生成されたwdio.conf.jsはFirefoxを使うようになっているなどの問題があるので修正する。
参考にしたのは&lt;a href=&#34;https://stackoverflow.com/questions/42303119/selenium-webdriverio-chrome-headless&#34;&gt;Stack Overflowの回答&lt;/a&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;     capabilities: [{
     maxInstances: 5,
-        browserName: &#39;firefox&#39;
+        browserName: &#39;chrome&#39;,
+        chromeOptions: {
+            binary: &#39;/usr/bin/chromium-browser&#39;,
+            args: [
+                &#39;headless&#39;,
+                &#39;disable-gpu&#39;,
+                &#39;no-sandbox&#39;,
+            ],
+        },
     }],
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;browserName&lt;/code&gt;を&lt;code&gt;firefox&lt;/code&gt;から&lt;code&gt;chrome&lt;/code&gt;に変えて、ヘッドレスモードで動かすためのオプションを指定している。
また、普通のChromeとは実行ファイルの名前が違うので、&lt;code&gt;binary&lt;/code&gt;で指定している。&lt;/p&gt;

&lt;h2 id=&#34;テスト作成と実行&#34;&gt;テスト作成と実行&lt;/h2&gt;

&lt;p&gt;テストはとりあえず&lt;a href=&#34;http://blog.asial.co.jp/1484&#34;&gt;この記事&lt;/a&gt;を参考に以下のようなものを書いた。&lt;/p&gt;

&lt;p&gt;test-sample.js:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;describe(&#39;Sample&#39;, function() {
    it(&amp;quot;takes a screenshot of www.google.co.jp&amp;quot;, function() {
        browser.url(&#39;https://www.google.co.jp/&#39;);
        browser.saveScreenshot(&#39;./screenshots/google.png&#39;);
    });
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これを実行すると、&lt;code&gt;https://www.google.co.jp/&lt;/code&gt;をブラウザで開いて、スクリーンショットを撮る。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;これを&lt;code&gt;~/webdriverio-chrome/test/specs/&lt;/code&gt;において、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;~/webdriverio-chrome # yarn test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;でテスト実行。
したらエラー。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;~/webdriverio-chrome # yarn test
yarn test v0.27.5
$ wdio
[06:43:04]  COMMAND     POST     &amp;quot;/wd/hub/session&amp;quot;
[06:43:04]  DATA                {&amp;quot;desiredCapabilities&amp;quot;:{&amp;quot;javascriptEnabled&amp;quot;:true,&amp;quot;locationContextEnabled&amp;quot;:true,&amp;quot;handlesAlerts&amp;quot;:true,&amp;quot;rotatable&amp;quot;:true,&amp;quot;maxInstances&amp;quot;:5,&amp;quot;browserName&amp;quot;:&amp;quot;chrome&amp;quot;,&amp;quot;chromeOptions&amp;quot;:{&amp;quot;binary&amp;quot;:&amp;quot;/usr/bin/chromium-browser&amp;quot;,&amp;quot;args&amp;quot;:[&amp;quot;headless&amp;quot;,&amp;quot;disable-gpu&amp;quot;,&amp;quot;no-sandbox&amp;quot;]},&amp;quot;loggingPrefs&amp;quot;:{&amp;quot;browser&amp;quot;:&amp;quot;ALL&amp;quot;,&amp;quot;driver&amp;quot;:&amp;quot;ALL&amp;quot;},&amp;quot;requestOrigins&amp;quot;:{&amp;quot;url&amp;quot;:&amp;quot;http://webdriver.io&amp;quot;,&amp;quot;version&amp;quot;:&amp;quot;4.6.2&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;webdriverio&amp;quot;}}}
ERROR: An unknown server-side error occurred while processing the command. (UnknownError:13)
chrome
Error: An unknown server-side error occurred while processing the command. (UnknownError:13)

error Command failed with exit code 1.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;サーバサイドでよくわからないエラーが起きたとのこと。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;試しに手動でSelenium Serverを起動してみる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;~/webdriverio-chrome # node ./node_modules/.bin/selenium-standalone start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;正常に起動する。&lt;/p&gt;

&lt;p&gt;ChromeDriverはどうか。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;~/webdriverio-chrome # /usr/bin/chromedriver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これも起動する。はて。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/webdriverio/wdio-selenium-standalone-service/blob/v0.0.9/lib/launcher.js&#34;&gt;wdio-selenium-standalone-serviceのソース&lt;/a&gt;を見てみたら、selenium-standaloneの&lt;code&gt;install&lt;/code&gt;を呼んでいた。
これはSelenium ServerとChromeDriverをダウンロードする機能だ。
コンテナ内を確認したら、&lt;code&gt;node_modules/selenium-standalone/.selenium/chromedrive
r/2.31-x64-chromedriver&lt;/code&gt;というのが出来てた。
これがselenium-standaloneがダウンロードしたChromeDriverだろうが、Apline Linux用ではないので、&lt;code&gt;ldd&lt;/code&gt;してやるとたくさんエラーが出る。
selenium-standaloneがこれを実行しようとしたせいでテスト実行がエラーになったんだろう。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.com/@jlchereau/how-to-configure-webdrivier-io-with-selenium-standalone-and-additional-browsers-9369d38bc4d1&#34;&gt;Mediumの記事&lt;/a&gt;などを参考にして、wdio.conf.jsを以下のように修正して、ChromeDriverのバイナリを指定してやったら動いた。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;     services: [&#39;selenium-standalone&#39;],
+    seleniumArgs: {
+        javaArgs: [
+            &#39;-Dwebdriver.chrome.driver=/usr/bin/chromedriver&#39;
+        ]
+    },
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;プロキシ対策&#34;&gt;プロキシ対策&lt;/h2&gt;

&lt;p&gt;社内で使うには、ベーシック認証付きのプロキシを突破しないといけない。&lt;/p&gt;

&lt;p&gt;今回作った環境をクールな図にするとこんな↓感じ。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/webdriverio-chrome/internet_accesses.png&#34; alt=&#34;internet_accesses.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;なので、二か所あるインターネッツアクセスをプロキシ対応させる必要がある。
図の左のアクセスについては、&lt;a href=&#34;https://github.com/webdriverio/wdio-selenium-standalone-service/blob/master/lib/launcher.js&#34;&gt;wdio-selenium-standalone-serviceのソース&lt;/a&gt;を見たりして、wdio.conf.jsを次のように修正すればいいことが分かった。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;     services: [&#39;selenium-standalone&#39;],
     seleniumArgs: {
         javaArgs: [
             &#39;-Dwebdriver.chrome.driver=/usr/bin/chromedriver&#39;,
         ],
     },
+    seleniumInstallArgs: {
+        proxy: &#39;http://userId:password@proxy.com:8080&#39;,
+    },
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;図の右のアクセスについては、プロキシのベーシック認証のクレデンシャルを指定するオプションがChromeにないので、&lt;a href=&#34;https://github.com/sjitech/proxy-login-automator&#34;&gt;proxy-login-automator&lt;/a&gt;を使うことにして、wdio.conf.jsには次のように追記しておく。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;         chromeOptions: {
             binary: &#39;/usr/bin/chromium-browser&#39;,
             args: [
                 &#39;headless&#39;,
                 &#39;disable-gpu&#39;,
                 &#39;no-sandbox&#39;,
+                &#39;proxy-server=localhost:18080&#39;,
             ],
         },
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;これで、テスト実行前に、以下みたいにproxy-login-automatorを起動しておけばいい。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;~/webdriverio-chrome # node node_modules/.bin/proxy-login-automator.js -local_port 18080 -remote_host proxy.com -remote_port 8080 -usr userId -pwd password`
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;

&lt;p&gt;以上の操作をまとめたDockerfileが以下。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dockerfile&#34;&gt;From alpine:edge

ADD package.json wdio.conf.js yarn.lock test-sample.js /root/webdriverio-chrome/

RUN apk add --update --no-cache \
            udev \
            ttf-freefont \
            chromium \
            chromium-chromedriver \
            openjdk8 \
            nodejs \
            yarn \
            make gcc g++ python \
            curl &amp;amp;&amp;amp; \
    cd /tmp &amp;amp;&amp;amp; \
    curl https://noto-website.storage.googleapis.com/pkgs/NotoSansCJKjp-hinted.zip -O &amp;amp;&amp;amp; \
    unzip NotoSansCJKjp-hinted.zip &amp;amp;&amp;amp; \
    mkdir -p /usr/share/fonts/noto &amp;amp;&amp;amp; \
    cp *.otf /usr/share/fonts/noto &amp;amp;&amp;amp; \
    chmod 644 -R /usr/share/fonts/noto/ &amp;amp;&amp;amp; \
    fc-cache -fv &amp;amp;&amp;amp; \
    cd /root/webdriverio-chrome/ &amp;amp;&amp;amp; \
    yarn global add node-gyp &amp;amp;&amp;amp; \
    yarn &amp;amp;&amp;amp; \
    mkdir -p test/specs &amp;amp;&amp;amp; \
    mv test-sample.js test/specs/ &amp;amp;&amp;amp; \
    mkdir screenshots &amp;amp;&amp;amp; \
    yarn global remove node-gyp &amp;amp;&amp;amp; \
    rm -rf /root/.node-gyp &amp;amp;&amp;amp; \
    rm -rf /tmp/* &amp;amp;&amp;amp; \
    yarn cache clean &amp;amp;&amp;amp; \
    apk del --purge make gcc g++ python curl

WORKDIR /root/webdriverio-chrome
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;できるイメージを小さくするため、レイヤを減らしたり、ビルド用パッケージを消したりしてる。
&lt;a href=&#34;http://qiita.com/minamijoyo/items/711704e85b45ff5d6405&#34;&gt;Multi-Stage build&lt;/a&gt;が&lt;a href=&#34;https://hub.docker.com/&#34;&gt;Docker Hub&lt;/a&gt;のAutomated Buildで&lt;a href=&#34;https://github.com/docker/hub-feedback/issues/1039&#34;&gt;もうすぐサポートされる&lt;/a&gt;ので、そしたらもう少しきれいに書き直せるはず。&lt;/p&gt;

&lt;p&gt;(後日書き直して&lt;a href=&#34;https://github.com/kaitoy/webdriverio-chrome/blob/master/Dockerfile&#34;&gt;きれいになった&lt;/a&gt;。)&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;最終的なpackage.jsonは&lt;a href=&#34;https://github.com/kaitoy/webdriverio-chrome/blob/v0.0.3/package.json&#34;&gt;これ&lt;/a&gt;。
wdio.conf.jsは&lt;a href=&#34;https://github.com/kaitoy/webdriverio-chrome/blob/v0.0.3/wdio.conf.js&#34;&gt;これ&lt;/a&gt;。&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Pcap4J on Nano Server on Hyper-V Containers on Windows 10 on VMware Playerにトライ</title>
          <link>https://www.kaitoy.xyz/2016/09/15/pcap4j-on-hyper-v-container-on-win10/</link>
          <pubDate>Thu, 15 Sep 2016 13:56:35 -0600</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2016/09/15/pcap4j-on-hyper-v-container-on-win10/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;https://github.com/kaitoy/pcap4j&#34;&gt;Pcap4J&lt;/a&gt;が動くHyper-VコンテナをWindows 10上でビルドしようとしたけど3合目あたりで息絶えた話。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h2 id=&#34;hyper-v-containersとは&#34;&gt;Hyper-V Containersとは&lt;/h2&gt;

&lt;p&gt;Hyper-V Containersは、MicrosoftによるWindowsネイティブなコンテナ技術である&lt;a href=&#34;https://msdn.microsoft.com/en-us/virtualization/windowscontainers/about/about_overview&#34;&gt;Windows Containers&lt;/a&gt;の一種で、これによるコンテナは、同じくWindows Containersの一種であるWindows Server Containersのものに比べて、より厳密に隔離されている分、起動コストが高い。&lt;/p&gt;

&lt;p&gt;実体は&lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt;そのもので、コンテナイメージは&lt;a href=&#34;https://hub.docker.com/&#34;&gt;Docker Hub&lt;/a&gt;からpullできるし、コンテナの操作や管理はdockerコマンドでやる。(昔はコンテナ操作用PowerShellコマンドレットもあったが、不評だったので廃止したようだ。)
&lt;a href=&#34;https://github.com/docker/docker&#34;&gt;ソース&lt;/a&gt;もLinuxとWindowsで一本化されている。&lt;/p&gt;

&lt;p&gt;Windows 10の&lt;a href=&#34;https://blogs.windows.com/japan/2016/08/03/how-to-get-the-windows-10-anniversary-update/#eFCYhK68sDp1V0F7.97&#34;&gt;Anniversary Update&lt;/a&gt;で正式にリリースされたが、なんだかあまり注目されていない気がする。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.kaitoy.xyz/2016/07/31/docker-for-windows/#docker-for-windows%E3%81%A8%E3%81%AF&#34;&gt;Docker for Windows&lt;/a&gt;とは全く別物なので注意。&lt;/p&gt;

&lt;h2 id=&#34;hyper-v-containersのインストール-on-vmware-player&#34;&gt;Hyper-V Containersのインストール (on VMware Player)&lt;/h2&gt;

&lt;p&gt;自前のPCが5年前に買った&lt;a href=&#34;https://dynabook.com/&#34;&gt;dynabook&lt;/a&gt;でWindows 10をサポートしていないので、VMware PlayerのVM上のWindows 10にHyper-V Containersをインストールしてみる。&lt;/p&gt;

&lt;p&gt;VMは、Windows 7に入れたVMware Workstation 11.1.0 build-2496824に付属の VMware Player 7.1.0 build-2496824で作ったもの。
VMのバージョンは11.0。
2CPUでメモリは2.5GB。
ネットワークインターフェースはNAT。
このVMを、&lt;a href=&#34;https://www.kaitoy.xyz/2016/07/31/docker-for-windows/#vmware-player%E3%81%AEvm%E3%81%A7hyper-v%E3%82%92%E4%BD%BF%E3%81%86%E3%81%9F%E3%82%81%E3%81%AE%E8%A8%AD%E5%AE%9A&#34;&gt;Hyper-Vが使えるように設定しておく&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://ascii.jp/elem/000/001/216/1216220/&#34;&gt;この記事&lt;/a&gt;にしたがい、Windows 10の評価版をダウンロード。
今公開されている評価版はAnniversary Update適用済みのバージョン1607で、Hyper-V Containersをサポートしている。&lt;/p&gt;

&lt;p&gt;これをさっき作ったVMにインストール。&lt;/p&gt;

&lt;p&gt;Windows 10を起動し、以下、&lt;a href=&#34;https://msdn.microsoft.com/en-us/virtualization/windowscontainers/quick_start/quick_start_windows_10&#34;&gt;Windows Containers on Windows 10&lt;/a&gt;に従って進める。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;containers機能有効化&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.thewindowsclub.com/how-to-open-an-elevated-powershell-prompt-in-windows-10&#34;&gt;PowerShellプロンプトを管理者権限でひらき&lt;/a&gt;、以下のコマンドで&lt;code&gt;containers&lt;/code&gt;機能を有効化。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;PS C:\Windows\system32&amp;gt;Enable-WindowsOptionalFeature -Online -FeatureName containers -All
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1分程度経つと再起動を促されるので再起動。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hyper-V機能有効化&lt;/p&gt;

&lt;p&gt;再度PowerShellプロンプトを管理者権限で開いて、以下のコマンドでHyper-Vを有効化。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;PS C:\Windows\system32&amp;gt;Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1分程度経つと再起動を促されるので再起動。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;OpLocks無効化&lt;/p&gt;

&lt;p&gt;現在のHyper-Vコンテナは、安定性を上げるためにOpLocksという機能を無効にすべきらしい。
再度PowerShellプロンプトを管理者権限で開いて、以下のコマンドを実行する。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;PS C:\Windows\system32&amp;gt;Set-ItemProperty -Path &#39;HKLM:SOFTWARE\Microsoft\Windows NT\CurrentVersion\Virtualization\Containers&#39; -Name VSmbDisableOplocks -Type DWord -Value 1 -Force
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Dockerインストール&lt;/p&gt;

&lt;p&gt;同じPowerShellプロンプトで以下のコマンドを実行してDocker(EngineとClient)のアーカイブをダウンロード。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;PS C:\Windows\system32&amp;gt;Invoke-WebRequest &amp;quot;https://master.dockerproject.org/windows/amd64/docker-1.13.0-dev.zip&amp;quot; -OutFile &amp;quot;$env:TEMP\docker-1.13.0-dev.zip&amp;quot; -UseBasicParsing
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ダウンロードしたアーカイブを解凍。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;PS C:\Windows\system32&amp;gt;Expand-Archive -Path &amp;quot;$env:TEMP\docker-1.13.0-dev.zip&amp;quot; -DestinationPath $env:ProgramFiles
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここまででDockerが&lt;code&gt;C:\Program Files\docker\&lt;/code&gt;に入るので、このパスを環境変数&lt;code&gt;PATH&lt;/code&gt;に追加。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;PATH&lt;/code&gt;の変更を反映させるために再度PowerShellプロンプトを管理者権限で開いて、以下のコマンドでDockerデーモンをサービスに登録。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;PS C:\Windows\system32&amp;gt;dockerd --register-service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Dockerサービスを起動。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;PS C:\Windows\system32&amp;gt;Start-Service Docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(Dockerサービスは自動起動に設定されているので、OS再起動時は上記&lt;code&gt;Start-Service&lt;/code&gt;は不要。)&lt;/p&gt;

&lt;p&gt;これでDockerが使えるようになった。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;PS C:\Windows\system32&amp;gt;docker version
Client:
 Version:      1.13.0-dev
 API version:  1.25
 Go version:   go1.7.1
 Git commit:   130db0a
 Built:        Sat Sep 10 13:25:48 2016
 OS/Arch:      windows/amd64


Server:
 Version:      1.13.0-dev
 API version:  1.25
 Go version:   go1.7.1
 Git commit:   130db0a
 Built:        Sat Sep 10 13:25:48 2016
 OS/Arch:      windows/amd64
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;コンテナイメージダウンロード&lt;/p&gt;

&lt;p&gt;どうもDockerコマンドの実行には管理者権限が必要なようなので、このまま管理者権限のPowerShellプロンプトで続ける。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;docker pull&lt;/code&gt;でNano Serverのコンテナイメージをダウンロード。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;PS C:\Windows\system32&amp;gt;docker pull microsoft/nanoserver
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;docker images&lt;/code&gt;で確認。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;PS C:\Windows\system32&amp;gt;docker images
REPOSITORY             TAG                 IMAGE ID            CREATED             SIZE
microsoft/nanoserver   latest              3a703c6e97a2        12 weeks ago        970 MB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;試しにコンテナ起動。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;PS C:\Windows\system32&amp;gt;docker run -it microsoft/nanoserver cmd&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;起動はかなり遅い。1分近くかかった。ともあれちゃんと起動した。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/pcap4j-on-hyper-v-container-on-win10/test_container.png&#34; alt=&#34;test_container.png&#34; /&gt;
&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;pcap4jコンテナのビルド&#34;&gt;Pcap4Jコンテナのビルド&lt;/h2&gt;

&lt;p&gt;Pcap4Jコンテナを、&lt;code&gt;docker build&lt;/code&gt;でビルドしてみる。
Dockerfileはとりあえず&lt;a href=&#34;https://www.kaitoy.xyz/2016/07/11/windows_containers_on_tp5/#pcap4j%E3%82%B3%E3%83%B3%E3%83%86%E3%83%8A%E3%82%A4%E3%83%A1%E3%83%BC%E3%82%B8%E3%81%AE%E3%83%93%E3%83%AB%E3%83%89&#34;&gt;以前のもの&lt;/a&gt;をちょっと書き変えただけのものを試す。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dockerfile&#34;&gt;# escape=`

#
# Dockerfile for Pcap4J on Windows Nano Server
#

FROM microsoft/nanoserver
MAINTAINER Kaito Yamada &amp;lt;kaitoy@pcap4j.org&amp;gt;

# Install Chocolatey.
RUN mkdir C:\pcap4j
WORKDIR C:\\pcap4j
ADD https://chocolatey.org/install.ps1 install.ps1
RUN powershell .\install.ps1

# Install dependencies.
RUN choco install -y nmap jdk7 &amp;amp;&amp;amp; `
    choco install -y maven -version 3.2.5

# Build Pcap4J.
RUN powershell -Command Invoke-WebRequest https://github.com/kaitoy/pcap4j/archive/v1.zip -OutFile pcap4j.zip &amp;amp;&amp;amp; `
    powershell -Command Expand-Archive -Path pcap4j.zip -DestinationPath .
WORKDIR pcap4j-1
RUN powershell -Command &amp;quot;mvn -P distribution-assembly install 2&amp;gt;&amp;amp;1 | Add-Content -Path build.log -PassThru&amp;quot;

# Collect libraries.
RUN mkdir bin &amp;amp;&amp;amp; `
    cd pcap4j-packetfactory-static &amp;amp;&amp;amp; `
    mvn -DoutputDirectory=..\bin -Dmdep.stripVersion=true -DincludeScope=compile dependency:copy-dependencies &amp;amp;&amp;amp; `
    mvn -DoutputDirectory=..\bin -Dmdep.stripVersion=true -DincludeGroupIds=ch.qos.logback dependency:copy-dependencies &amp;amp;&amp;amp; `
    cd ../pcap4j-distribution &amp;amp;&amp;amp; `
    mvn -DoutputDirectory=..\bin -Dmdep.stripVersion=true -DincludeArtifactIds=pcap4j-packetfactory-static,pcap4j-sample dependency:copy-dependencies

# Generate sample script. (C:\pcap4j\pcap4j-1\bin\capture.bat)
RUN echo @echo off &amp;gt; bin\capture.bat &amp;amp;&amp;amp; `
    echo &amp;quot;%JAVA_HOME%\bin\java&amp;quot; -cp C:\pcap4j\pcap4j-1\bin\pcap4j-core.jar;C:\pcap4j\pcap4j-1\bin\pcap4j-packetfactory-static.jar;C:\pcap4j\pcap4j-1\bin\pcap4j-sample.jar;C:\pcap4j\pcap4j-1\bin\jna.jar;C:\pcap4j\pcap4j-1\bin\slf4j-api.jar;C:\pcap4j\pcap4j-1\bin\logback-classic.jar;C:\pcap4j\pcap4j-1\bin\logback-core.jar org.pcap4j.sample.GetNextPacketEx &amp;gt;&amp;gt; bin\capture.bat
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/engine/reference/builder/#/escape&#34;&gt;escapeディレクティブ&lt;/a&gt;が使えるようになっていたので使うようにしている。
(というか以前Windows Server 2016 TP5で試した時はescapeディレクティブをDockerfileの先頭に書かなかったのがだめだったってだけかもしれない。)
&lt;code&gt;WORKDIR&lt;/code&gt;のパスの区切りにはescapeディレクティブは利かない変な仕様。&lt;/p&gt;

&lt;h4 id=&#34;nano-serverでsystem-net-webclient使えない問題&#34;&gt;Nano ServerでSystem.Net.WebClient使えない問題&lt;/h4&gt;

&lt;p&gt;このDockerfileでビルドしたら、&lt;a href=&#34;https://chocolatey.org/&#34;&gt;Chocolatey&lt;/a&gt;のダウンロード・インストールスクリプトを実行する&lt;code&gt;RUN powershell .\install.ps1&lt;/code&gt;のステップで&lt;code&gt;System.Net.WebClient&lt;/code&gt;が見つからないというエラー。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;new-object : Cannot find type [System.Net.WebClient]: verify that the assembly
containing this type is loaded.
At C:\pcap4j\install.ps1:84 char:17
+   $downloader = new-object System.Net.WebClient
+                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : InvalidType: (:) [New-Object], PSArgumentExcepti
   on
    + FullyQualifiedErrorId : TypeNotFound,Microsoft.PowerShell.Commands.NewOb
   jectCommand
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nano Serverに入っているPowerShellは&lt;a href=&#34;https://technet.microsoft.com/en-us/windows-server-docs/compute/nano-server/powershell-on-nano-server&#34;&gt;Core Editionなる機能限定版&lt;/a&gt;で、System.Net.WebClientだけでなく、&lt;a href=&#34;http://serverfault.com/questions/788949/download-a-file-with-powershell-on-nano-server&#34;&gt;WebアクセスのためのAPIがいろいろ欠けているもよう&lt;/a&gt;。&lt;/p&gt;

&lt;h4 id=&#34;hyper-v-containersでserver-core使えない問題&#34;&gt;Hyper-V ContainersでServer Core使えない問題&lt;/h4&gt;

&lt;p&gt;Nano Serverめんどくさそうなので、Server Coreをpullする。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PS C:\Windows\system32&amp;gt;docker pull microsoft/windowsservercore
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Dockerfileの&lt;code&gt;FROM&lt;/code&gt;を&lt;code&gt;microsoft/windowsservercore&lt;/code&gt;に書き変えてビルドしたら、最初の&lt;code&gt;RUN&lt;/code&gt;で以下のエラー。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;container 4bc8d8d38993426fa7a3c76e4aabbe6a229cbd025754723ff396aec04ffbfa1d encountered an error during Start failed in Win32: The operating system of the container does not match the operating system of the host. (0xc0370101)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;調べたら、Hyper-V Containersはまだ&lt;a href=&#34;https://social.msdn.microsoft.com/Forums/en-US/9eea93ac-18de-4953-bc7c-efd76a155526/are-microsoftwindowsservercore-containers-working-on-windows-10?forum=windowscontainers&#34;&gt;Nano Serverしかサポートしていない&lt;/a&gt;ようだ。&lt;/p&gt;

&lt;h4 id=&#34;unzip難しい問題&#34;&gt;unzip難しい問題&lt;/h4&gt;

&lt;p&gt;Chocolateyのダウンロード・インストールスクリプトを実行するのはあきらめて、&lt;a href=&#34;https://chocolatey.org/install#download-powershell-method&#34;&gt;アーカイブを自分でダウンロードする方法&lt;/a&gt;を試す。&lt;/p&gt;

&lt;p&gt;これは&lt;code&gt;https://chocolatey.org/api/v2/package/chocolatey/&lt;/code&gt;というWeb APIをたたいてアーカイブをダウンロードする方法だけど、このURLを&lt;code&gt;ADD&lt;/code&gt;に渡してもうまくいかなかったので、このWeb APIが最終的に呼ぶ&lt;code&gt;https://packages.chocolatey.org/chocolatey.0.10.0.nupkg&lt;/code&gt;を&lt;code&gt;ADD&lt;/code&gt;するようにした。
これでダウンロードできる&lt;code&gt;chocolatey.0.10.0.nupkg&lt;/code&gt;はzipファイルで、unzipするとインストールスクリプトが出てくる。&lt;/p&gt;

&lt;p&gt;しかしこのunzipが曲者で、&lt;a href=&#34;https://www.kaitoy.xyz/2016/09/12/unzip-on-nanoserver/&#34;&gt;妙に苦労した話&lt;/a&gt;を最近書いた。&lt;/p&gt;

&lt;p&gt;で、苦労して取り出したインストールスクリプトを実行したら、エラーがわんさと出ただけだった。
そんなこったろうと思ってはいたが。&lt;/p&gt;

&lt;p&gt;どうせChocolateyをインストールできても、パッケージのインストールスクリプトがまた動かないんだろうから、もうChocolateyはあきらめる。&lt;/p&gt;

&lt;h4 id=&#34;wow64サポートしてない問題&#34;&gt;WoW64サポートしてない問題&lt;/h4&gt;

&lt;p&gt;Chocolateyを使わないようにDockerfileの前半を以下の様に書き変えた。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dockerfile&#34;&gt;(snip)

FROM michaeltlombardi/nanoserveropenjdk
MAINTAINER Kaito Yamada &amp;lt;kaitoy@pcap4j.org&amp;gt;

RUN mkdir C:\pcap4j
WORKDIR C:\\pcap4j

# Install Maven
ADD http://archive.apache.org/dist/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.zip maven.zip
RUN jar xf maven.zip
RUN powershell -command $env:path += &#39;;C:\pcap4j\apache-maven-3.3.9\bin&#39;; setx PATH $env:path /M

# Install Npcap
ADD https://github.com/nmap/npcap/releases/download/v0.08-r7/npcap-0.08-r7.exe npcap.exe
RUN npcap.exe /S

# Build Pcap4J.

(snip)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(因みにこの時点で、&lt;code&gt;PATH&lt;/code&gt;を設定するのに&lt;code&gt;GetEnvironmentVariable&lt;/code&gt;と&lt;code&gt;SetEnvironmentVariable&lt;/code&gt;がうまく使えない問題を乗り越えている。&lt;code&gt;Cannot find an overload for &amp;quot;GetEnvironmentVariable&amp;quot; and the argument count: &amp;quot;2&amp;quot;.&lt;/code&gt;というエラーが出て、PowerShell Desktop Editionのものと仕様がちょっと違うようだったので、&lt;code&gt;GetEnvironmentVariable&lt;/code&gt;も&lt;code&gt;SetEnvironmentVariable&lt;/code&gt;も使わないようにした。)&lt;/p&gt;

&lt;p&gt;このDockerfileでビルドしたら、&lt;code&gt;RUN npcap.exe /S&lt;/code&gt;で以下のエラー。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;The subsystem needed to support the image type is not present.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このsubsystemというのはどうも&lt;a href=&#34;https://ja.wikipedia.org/wiki/WOW64&#34;&gt;WoW64&lt;/a&gt;を指しているようで、&lt;a href=&#34;https://blogs.technet.microsoft.com/windowsserver/2016/02/10/exploring-nano-server-for-windows-server-2016/&#34;&gt;Nano ServerがWoW64をサポートしていない&lt;/a&gt;のにnpcap.exeが32bitバイナリであることが問題のようであった。&lt;/p&gt;

&lt;p&gt;ついでに&lt;a href=&#34;https://ja.wikipedia.org/wiki/Microsoft_Windows_Installer&#34;&gt;MSI&lt;/a&gt;もサポートされていないことがわかった。大丈夫かこれ。&lt;/p&gt;

&lt;h4 id=&#34;nano-serverパッケージプロバイダバグってる問題&#34;&gt;Nano Serverパッケージプロバイダバグってる問題&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#34;https://technet.microsoft.com/en-us/windows-server-docs/compute/nano-server/getting-started-with-nano-server#a-namebkmkonlineainstalling-roles-and-features-online&#34;&gt;Nano Serverにもロールや機能の追加ができる&lt;/a&gt;らしいので、ひょっとしてこれで何か改善できないかと思って試した。&lt;/p&gt;

&lt;p&gt;Nano Serverへのロール・機能の追加は、Windowsのパッケージマネジメントシステムである&lt;a href=&#34;https://github.com/OneGet/oneget&#34;&gt;PackageManagement (a.k.a. OneGet)&lt;/a&gt;を使ってやる。PowerShellで&lt;code&gt;Install-PackageProvider NanoServerPackage&lt;/code&gt;と&lt;code&gt;Import-PackageProvider NanoServerPackage&lt;/code&gt;を実行するとNano Serverのパッケージプロバイダが使えるようになり、&lt;code&gt;Find-NanoServerPackage&lt;/code&gt;で利用できるパッケージの一覧が見れる。&lt;/p&gt;

&lt;p&gt;はずなんだけど、&lt;code&gt;Find-NanoServerPackage&lt;/code&gt;でエラー。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;C:\pcap4j&amp;gt;powershell -command Find-NanoServerPackage
DownloadFile : Save-HTTPItem: Bits Transfer failed. Job State:  ExitCode = 255
At C:\Program Files\WindowsPowerShell\Modules\NanoServerPackage\0.1.1.0\NanoServerPackage.psm1:1294 char:9
+         DownloadFile -downloadURL $fullUrl `
+         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : InvalidOperation: (https://nanoser...ServerIndex.txt:String) [DownloadFile], RuntimeException
    + FullyQualifiedErrorId : FailedToDownload,DownloadFile

Get-Content : Cannot find drive. A drive with the name &#39;CleanUp&#39; does not exist.
At C:\Program Files\WindowsPowerShell\Modules\NanoServerPackage\0.1.1.0\NanoServerPackage.psm1:674 char:26
+     $searchFileContent = Get-Content $searchFile
+                          ~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : ObjectNotFound: (CleanUp:String) [Get-Content], DriveNotFoundException
    + FullyQualifiedErrorId : DriveNotFound,Microsoft.PowerShell.Commands.GetContentCommand

Get-Content : Cannot find path
&#39;C:\Users\ContainerAdministrator\AppData\Local\NanoServerPackageProvider\searchNanoPackageIndex.txt&#39; because it does not exist.
At C:\Program Files\WindowsPowerShell\Modules\NanoServerPackage\0.1.1.0\NanoServerPackage.psm1:674 char:26
+     $searchFileContent = Get-Content $searchFile
+                          ~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : ObjectNotFound: (C:\Users\Contai...ackageIndex.txt:String) [Get-Content], ItemNotFoundException
    + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.GetContentCommand
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/OneGet/NanoServerPackage/issues/4&#34;&gt;NanoServerPackageのIssues&lt;/a&gt;にこのエラーが登録されていた。1か月放置されてる。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;パトラッシュ、僕はもう疲れたよ。&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Hyper-Vコンテナ(Nano Server)でunzipしたいならjarを使え</title>
          <link>https://www.kaitoy.xyz/2016/09/12/unzip-on-nanoserver/</link>
          <pubDate>Mon, 12 Sep 2016 16:46:54 -0600</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2016/09/12/unzip-on-nanoserver/</guid>
          <description>

&lt;p&gt;Nano Serverでunzipしたかっただけだったのに、妙に苦労した話。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h2 id=&#34;nano-serverとは&#34;&gt;Nano Serverとは&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://technet.microsoft.com/en-us/windows-server-docs/compute/nano-server/getting-started-with-nano-server&#34;&gt;Nano Server&lt;/a&gt;は、Windows Server 2016で追加されるWindows Serverの新たなインストール形式で、&lt;a href=&#34;https://en.wikipedia.org/wiki/Server_Core&#34;&gt;Server Core&lt;/a&gt;よりさらに機能を絞り、リモートで管理するクラウドホストやWebサーバ向けにに特化したもの。&lt;/p&gt;

&lt;p&gt;Server Coreが数GBくらいなのに対し、Nano Serverは数百MBととても軽量で、それゆえ起動が速くセキュア。&lt;/p&gt;

&lt;h2 id=&#34;unzipとは&#34;&gt;unzipとは&lt;/h2&gt;

&lt;p&gt;unzipとは、&lt;a href=&#34;https://ja.wikipedia.org/wiki/ZIP_(%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%83%95%E3%82%A9%E3%83%BC%E3%83%9E%E3%83%83%E3%83%88&#34;&gt;zip&lt;/a&gt;ファイルを解凍する、ただそれだけのこと。&lt;/p&gt;

&lt;p&gt;ただそれだけのことで、基本的な機能だと思うのだが、Windowsはこれを&lt;a href=&#34;https://technet.microsoft.com/en-us/library/dn841359.aspx&#34;&gt;コマンドラインで実行する方法&lt;/a&gt;をつい最近まで正式に提供していなかった。&lt;/p&gt;

&lt;h2 id=&#34;nano-serverでunzip&#34;&gt;Nano Serverでunzip&lt;/h2&gt;

&lt;p&gt;Windows 10の&lt;a href=&#34;https://www.kaitoy.xyz/2016/01/22/pcap4j-meets-windows-containers/#windows-containers%E3%81%A8%E3%81%AF&#34;&gt;Hyper-V Containers&lt;/a&gt;の上で&lt;a href=&#34;https://github.com/kaitoy/pcap4j&#34;&gt;Pcap4J&lt;/a&gt;のビルドとテストをするDockerイメージをビルドしたくて、そのための依存ライブラリなどをインストールする処理をDockerfileに書いていて、&lt;code&gt;ADD&lt;/code&gt;でzipをダウンロードしたところまではいいんだけど、このzipどうやって解凍してやろうかとなった。
(Dockerホストに置いたものをコンテナに&lt;code&gt;ADD&lt;/code&gt;するのはなんか格好悪いから無しで。Dockerfile裸一貫で実現したい。)&lt;/p&gt;

&lt;p&gt;Windows 10のHyper-V Containersは、&lt;a href=&#34;https://social.msdn.microsoft.com/Forums/en-US/9eea93ac-18de-4953-bc7c-efd76a155526/are-microsoftwindowsservercore-containers-working-on-windows-10?forum=windowscontainers&#34;&gt;現時点でNano Serverしかサポートしていない&lt;/a&gt;のが厳しい点。Server Coreだったら楽だったのに。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;以下、いろいろ試したことを書く。&lt;/p&gt;

&lt;h4 id=&#34;正攻法-expand-archive&#34;&gt;正攻法: Expand-Archive&lt;/h4&gt;

&lt;p&gt;PowerShellの v5 で実装されたExpand-Archiveというコマンドレットでzipを解凍できる。
Nano ServerのPowerShellのバージョンを確認したら 5.1 だったのでこれでいけるかと思った。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\&amp;gt;powershell -command &amp;quot;$PSVersionTable.PSVersion&amp;quot;

Major  Minor  Build  Revision
-----  -----  -----  --------
5      1      14284  1000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;したらこのエラー。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;Add-Type : Cannot find path &#39;C:\System.IO.Compression.FileSystem.dll&#39; because it does not exist.
At
C:\windows\system32\windowspowershell\v1.0\Modules\Microsoft.PowerShell.Archive\Microsoft.PowerShell.Archive.psm1:914
char:5
+     Add-Type -AssemblyName System.IO.Compression.FileSystem
+     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : ObjectNotFound: (C:\System.IO.Compression.FileSystem.dll:String) [Add-Type], ItemNotFoun
   dException
    + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.AddTypeCommand
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;どうもPowerShellの 5.1 以降には、.NET FrameworkベースのDesktop Editionと、そこから機能を絞った.NET CoreベースのCore Editionがあり、&lt;a href=&#34;https://technet.microsoft.com/en-us/windows-server-docs/compute/nano-server/powershell-on-nano-server&#34;&gt;Nano ServerのはCore Editionなんだそうな&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;Expand-ArchiveはSystem.IO.Compression.FileSystem.dllの中のZipFileクラスに依存しているんだけど、.NET CoreにはSystem.IO.Compression.FileSystem.dllが含まれていないっぽい。&lt;/p&gt;

&lt;h4 id=&#34;shellオブジェクトのcopyhere&#34;&gt;ShellオブジェクトのCopyHere&lt;/h4&gt;

&lt;p&gt;PowerShellでのunzip方法を調べたらStack Overflowに&lt;a href=&#34;http://stackoverflow.com/questions/27768303/how-to-unzip-a-file-in-powershell&#34;&gt;いくつか載っていた&lt;/a&gt;。
Expand-Archiveと、System.IO.Compression.ZipFileを直接使う方法と、Shellオブジェクト(COMオブジェクト)のCopyHereメソッドを使う方法。&lt;/p&gt;

&lt;p&gt;最初の2つはCore Editionでは使えないことが分かっているので、3つめにトライ。
こんなの↓&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;$shell = New-Object -ComObject shell.application
$zip = $shell.NameSpace(&amp;quot;C:\a.zip&amp;quot;)
MkDir(&amp;quot;C:\a&amp;quot;)
foreach ($item in $zip.items()) {
  $shell.Namespace(&amp;quot;C:\a&amp;quot;).CopyHere($item)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;調べたらこの方法は&lt;a href=&#34;https://support.microsoft.com/ja-jp/kb/2679832&#34;&gt;Microsoftから非推奨にされている&lt;/a&gt;ことが分かったんだけど、一応やってみる。&lt;/p&gt;

&lt;p&gt;したら以下のエラー。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;new-object : Retrieving the COM class factory for component with CLSID {00000000-0000-0000-0000-000000000000} failed
due to the following error: 80040154 Class not registered (Exception from HRESULT: 0x80040154 (REGDB_E_CLASSNOTREG)).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この方法で利用しようとしているのは&lt;a href=&#34;https://en.wikipedia.org/wiki/Windows_shell&#34;&gt;Windows shell&lt;/a&gt;、つまり&lt;a href=&#34;https://ja.wikipedia.org/wiki/Windows_Explorer&#34;&gt;Windows Explorer&lt;/a&gt;の機能らしく、そうであればまあGUIがないNano Serverで動かないのも当然か。&lt;/p&gt;

&lt;h4 id=&#34;サードパーティツールに頼る&#34;&gt;サードパーティツールに頼る&lt;/h4&gt;

&lt;p&gt;Stack Overflowの&lt;a href=&#34;http://stackoverflow.com/questions/1021557/how-to-unzip-a-file-using-the-command-line&#34;&gt;別の質問&lt;/a&gt;にサードパーティツールに頼る方法がいくつか紹介されていた。&lt;/p&gt;

&lt;p&gt;ここで挙げられていたもののうち、&lt;a href=&#34;http://www.7-zip.org/download.html&#34;&gt;7-Zip&lt;/a&gt;、&lt;a href=&#34;http://www.freebyte.com/fbzip/&#34;&gt;Freebyte Zip&lt;/a&gt;、&lt;a href=&#34;http://infozip.sourceforge.net/&#34;&gt;Info-ZIP&lt;/a&gt;は、配布形式がダメ。&lt;/p&gt;

&lt;p&gt;7-Zipのインストーラをコンテナで実行してみたら、「The subsystem needed to support the image type is not present.」というエラー。7-Zipにはzipで配布されているものもあるんだけど、皮肉としか思えない。&lt;/p&gt;

&lt;p&gt;Freebyte ZipやInfo-ZIPの自己解凍ファイルもコンテナ内では動いてくれない。&lt;/p&gt;

&lt;p&gt;一方、&lt;a href=&#34;http://membrane.com/synapse/library/pkunzip.html&#34;&gt;pkunzip&lt;/a&gt;はコマンドが裸で配布されているので行けるかと思ったが、実行したら「The system cannot execute the specified program.」なるエラー。
よく見たらこれ16bitアプリケーション。Nano Serverは32bitアプリすら実行できないというのに。&lt;/p&gt;

&lt;h4 id=&#34;jarに託された最後の希望&#34;&gt;jarに託された最後の希望&lt;/h4&gt;

&lt;p&gt;上に貼ったStack Overflowの質問には&lt;a href=&#34;https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jar.html&#34;&gt;jarコマンド&lt;/a&gt;を使う方法も挙げられていたが、JDKなんてどうせインストールできないとあきらめていた。&lt;/p&gt;

&lt;p&gt;が、ふと思い立って&lt;a href=&#34;https://hub.docker.com/&#34;&gt;Docker Hub&lt;/a&gt;を検索してみたら、&lt;a href=&#34;https://hub.docker.com/r/michaeltlombardi/nanoserveropenjdk/&#34;&gt;OpenJDK入りのNano Server&lt;/a&gt;をアップしてくれている人がいた。&lt;/p&gt;

&lt;p&gt;pullしてrunしてみたら確かにJDKがインストールされていた。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\&amp;gt;java -version
openjdk version &amp;quot;1.8.0_102-1-ojdkbuild&amp;quot;
OpenJDK Runtime Environment (build 1.8.0_102-1-ojdkbuild-b14)
OpenJDK 64-Bit Server VM (build 25.102-b14, mixed mode)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;で、無事&lt;code&gt;&amp;quot;C:\Program Files\Java\bin\jar.exe&amp;quot; xf hoge.zip&lt;/code&gt;のようにしてunzipできた。&lt;/p&gt;

&lt;p&gt;ここまで1日かかった。&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Docker for Windowsがコレジャナかった</title>
          <link>https://www.kaitoy.xyz/2016/07/31/docker-for-windows/</link>
          <pubDate>Sun, 31 Jul 2016 14:34:16 -0600</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2016/07/31/docker-for-windows/</guid>
          <description>

&lt;p&gt;7/28にDocker for Winodws(とDocker for Mac)の正式版リリースの&lt;a href=&#34;https://blog.docker.com/2016/07/docker-for-mac-and-windows-production-ready/&#34;&gt;アナウンス&lt;/a&gt;があったので試してみたけど、期待していたものと違ったしなんだか上手く動かなかった話。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h2 id=&#34;docker-for-windowsとは&#34;&gt;Docker for Windowsとは&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/docker-for-windows/&#34;&gt;Docker for Windows&lt;/a&gt;は&lt;a href=&#34;https://www.docker.com/products/docker-toolbox&#34;&gt;Docker Toolbox&lt;/a&gt;の後継製品。(多分。)&lt;/p&gt;

&lt;p&gt;Docker ToolboxはWindowsやMacでDockerを使うための製品で、以下のコンポーネントからなる。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.docker.com/products/docker-engine&#34;&gt;Docker Engine&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;コンテナランタイム。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/compose/&#34;&gt;Docker Compose&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;複数のコンテナを組み合わせたアプリケーション/サービスの構築/管理ツール。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/machine/&#34;&gt;Docker Machine&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Docker仮想ホストのプロビジョニング/管理ツール。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://kitematic.com/&#34;&gt;Kitematic&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Dockerコンテナを管理するGUIを提供する製品。
Docker Machineと連携してローカルマシンへのDocker仮想ホストのプロビジョニングもしてくれる。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Docker Toolboxを使うと、&lt;a href=&#34;https://ja.wikipedia.org/wiki/VirtualBox&#34;&gt;VirtualBox&lt;/a&gt;のLinux VMをWindows/Mac上にプロビジョニングして、そのVMにDockerをインストールして、Windows/Macから利用できる。&lt;/p&gt;

&lt;p&gt;Docker for Windowsもだいたい同じで、Docker EngineとDocker ComposeとDocker MachineをWinodwsで利用するための製品。
&lt;a href=&#34;http://electron.atom.io/&#34;&gt;Electron&lt;/a&gt;ベースでOracleのVirtualBox依存なKitematicの代わりに、ネイティブなインストーラがWindows内蔵の&lt;a href=&#34;https://ja.wikipedia.org/wiki/Hyper-V&#34;&gt;Hyper-V&lt;/a&gt;を使ってDockerをセットアップしてくれる。
Hyper-Vを使うため、VirtualBoxより速くて高信頼らしい。
KitematicはDocker for Windowsには付属しないが、別途ダウンロードすればコンテナ管理に使える。Docker for WindowsとDocker Toolboxとは共存はできない。&lt;/p&gt;

&lt;p&gt;私は勝手にDocker for Windowsは&lt;a href=&#34;https://www.kaitoy.xyz/2016/01/22/pcap4j-meets-windows-containers/#windows-containersとは&#34;&gt;Hyper-V Containers&lt;/a&gt;のデスクトップOS版のようなものかと勘違いしていて、Windowsのコンテナが使えるようになったのかと期待したが違った。
Docker for Windowsは単にDocker ToolboxのVirtualBoxがHyper-Vになっただけのもので、結局Linux VMの中でDockerを使うだけのものだということにセットアップ中に気付いた。&lt;/p&gt;

&lt;p&gt;(2017/9/12追記: &lt;a href=&#34;https://blogs.msdn.microsoft.com/webdev/2017/09/07/getting-started-with-windows-containers/&#34;&gt;これ&lt;/a&gt;とか&lt;a href=&#34;https://docs.docker.com/docker-for-windows/install/#about-windows-containers-and-windows-server-2016&#34;&gt;これ&lt;/a&gt;とかを見るに、いまではDocker for Winodwsは、Hyper-V ContainersやWindows Server Containersのフロントエンドでもあるようだ。)&lt;/p&gt;

&lt;p&gt;コレジャナイ感がすごかった。&lt;/p&gt;

&lt;p&gt;ともあれ、やった作業を以下に記す。&lt;/p&gt;

&lt;h2 id=&#34;docker-for-windows-on-vmware-player&#34;&gt;Docker for Windows on VMware Player&lt;/h2&gt;

&lt;p&gt;現時点ではDocker for WindowsはホストとしてWindows 10 x64 Pro/Enterprise/Education (Version 1511 Build 10586 以降)しかサポートしていない。
自前のPCが5年前に買った&lt;a href=&#34;https://dynabook.com/&#34;&gt;dynabook&lt;/a&gt;でWindows 10をサポートしていないので、VMware PlayerのVM上のWindows 10にDocker for Windowsをインストールしてみる。&lt;/p&gt;

&lt;h4 id=&#34;vmware-playerのvmでhyper-vを使うための設定&#34;&gt;VMware PlayerのVMでHyper-Vを使うための設定&lt;/h4&gt;

&lt;p&gt;VMware PlayerのVMでは通常Hyper-Vは使えないので、&lt;a href=&#34;http://social.technet.microsoft.com/wiki/contents/articles/22283.how-to-install-hyper-v-on-vmware-workstation-10.aspx&#34;&gt;How to Install Hyper-V on vmware Workstation 10 ?&lt;/a&gt;を参考にしてVMの設定をいじる。
この記事はVMware Workstationについてのものだが、VMware Playerでも全く同じ方法でいける。&lt;/p&gt;

&lt;p&gt;いじるのは、dynabookのWindows 7に入れたVMware Workstation 11.1.0 build-2496824に付属の
VMware Player 7.1.0 build-2496824で作ったWindows 10 Pro x64 (Version 1511 Build 10586.494)のVM。
VMのバージョンは11.0。2CPUでメモリは2GB。ネットワークインターフェースはNAT。&lt;/p&gt;

&lt;p&gt;このVMの.vmxファイルをテキストエディタで開いて以下を追記。意味は不明。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hypervisor.cpuid.v0 = &amp;quot;FALSE&amp;quot;
mce.enable = &amp;quot;TRUE&amp;quot;
vhu.enable = &amp;quot;TRUE&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次いで、VMware PlayerのGUIからVMのCPU設定を開き、&lt;code&gt;Intel VT-x/EPTまたはAMD-V/RVIを仮想化&lt;/code&gt;と&lt;code&gt;CPUパフォーマンスカウンタを仮想化&lt;/code&gt;にチェックを付ける。意味はなんとなくしかわからない。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/docker-for-windows/vm.jpg&#34; alt=&#34;vm.jpg&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;これだけ。&lt;/p&gt;

&lt;p&gt;Hyper-VはDocker for Windowsのインストーラが有効化してくれるのでここでは何もしなくていい。&lt;/p&gt;

&lt;h4 id=&#34;docker-for-windowsインストール&#34;&gt;Docker for Windowsインストール&lt;/h4&gt;

&lt;p&gt;VMを起動して、&lt;a href=&#34;https://docs.docker.com/docker-for-windows/&#34;&gt;Getting Started with Docker for Windows&lt;/a&gt;に従ってDocker for Windowsをインストールする。&lt;/p&gt;

&lt;p&gt;まず、&lt;a href=&#34;https://download.docker.com/win/stable/InstallDocker.msi&#34;&gt;上記サイト内のリンク&lt;/a&gt;からインストーラをダウンロード。stableの方。&lt;/p&gt;

&lt;p&gt;ダウンロードした&lt;code&gt;InstallDocker.msi&lt;/code&gt;をVM上で実行してウィザードに従えばインストール完了。
ウィザードの最後で&lt;code&gt;Launch Docker&lt;/code&gt;にチェックが付いた状態で&lt;code&gt;Finish&lt;/code&gt;するとDockerを起動してくれる。
この起動中にHyper-Vを有効化してくれる。(OS再起動有り。)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/docker-for-windows/hyper-v.jpg&#34; alt=&#34;hyper-v.jpg&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;OS再起動後、「Failed to create Switch &amp;ldquo;DockerNAT&amp;rdquo;: Hyper-V was unable to find a virtual switch with name &amp;ldquo;DockerNAT&amp;rdquo;」というエラー出た。&lt;code&gt;DockerNAT&lt;/code&gt;が見つからない?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/docker-for-windows/error.jpg&#34; alt=&#34;error.jpg&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;code&gt;DockerNAT&lt;/code&gt;はDocker for Windowsがインストール中に作るHyper-Vの仮想スイッチ。&lt;/p&gt;

&lt;p&gt;以前に&lt;code&gt;hosts&lt;/code&gt;に変なエントリを書いてしまっていたのでそれを一応消して、VMware PlayerのVMのアダプタの設定もちょっといじってしまっていたので一応もとにもどして、再度Docker for Windowsをクリーンインストールしたら上記エラーは出なくなった。
なんだったんだろう。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Dockerの起動中に今度はメモリ系のエラー: 「Failed to create VM &amp;ldquo;MobyLinuxVM&amp;rdquo;: Failed to modify device &amp;lsquo;Memory&amp;rsquo;」。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/docker-for-windows/error2.jpg&#34; alt=&#34;error2.jpg&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;code&gt;MobyLinuxVM&lt;/code&gt;はDockerを動かすHyper-V VMの名前。このVMに割り当てるメモリはホストOSのメモリ量から決められるようで、これが少なすぎるとダメな模様。&lt;/p&gt;

&lt;p&gt;VMware PlayerのVMのメモリを2Gから3.3Gに増やしたらこのエラーもなくなったけど、今度はIPアドレスのエラー: 「Failed to start VM &amp;ldquo;MobyLinuxVM&amp;rdquo;: The VM couldn&amp;rsquo;t get an IP address after 60 tries」。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/docker-for-windows/error3.jpg&#34; alt=&#34;error3.jpg&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;フォーラムを見たら
&lt;a href=&#34;https://forums.docker.com/t/vm-mobylinuxvm-the-vm-couldnt-get-an-ip-address-after-60-tries/8505/11&#34;&gt;このエラーが載っていた&lt;/a&gt;。そこには以下の様な解決方法が挙がっていた。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Docker for Windowsをクリーンインストールしなおす。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;vEthernet (DockerNAT)&lt;/code&gt;のアダプタのオプションでIPv6を無効にする。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;タスクトレイの鯨アイコンから開けるDockerのSettingsで&lt;code&gt;Reset to factory defaults...&lt;/code&gt;を実行。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/docker-for-windows/docker_settings.jpg&#34; alt=&#34;docker_settings.jpg&#34; /&gt;
&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;どれもだめだった。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;MobyLinuxVM&lt;/code&gt;がちゃんと起動しなくて、Dockerデーモンに接続できない。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Windows\System32&amp;gt;docker version
Client:
 Version:      1.12.0
 API version:  1.24
 Go version:   go1.6.3
 Git commit:   8eab29e
 Built:        Thu Jul 28 21:15:28 2016
 OS/Arch:      windows/amd64
An error occurred trying to connect: Get http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/version: open //./pipe/docker_engine: The system cannot find the file specified.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因みにちゃんと起動すると以下の感じになるらしい。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;PS C:\Users\samstevens&amp;gt; docker version
Client:
Version:      1.12.0-rc2
API version:  1.24
Go version:   go1.6.2
Git commit:   906eacd
Built:        Fri Jun 17 20:35:33 2016
OS/Arch:      windows/amd64
Experimental: true

Server:
Version:      1.12.0-rc2
API version:  1.24
Go version:   go1.6.2
Git commit:   a7119de
Built:        Fri Jun 17 22:09:20 2016
OS/Arch:      linux/amd64
Experimental: true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;もうあきらめる。
どうせWindowsコンテナが使えないならあまり面白くないし。&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Windows Server 2016 TP5でWindows Containersにリトライ</title>
          <link>https://www.kaitoy.xyz/2016/07/11/windows_containers_on_tp5/</link>
          <pubDate>Mon, 11 Jul 2016 00:30:33 -0600</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2016/07/11/windows_containers_on_tp5/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;https://www.microsoft.com/ja-jp/evalcenter/evaluate-windows-server-technical-preview&#34;&gt;Windows Server 2016のTechnical Preview 5(TP5)が公開されていた&lt;/a&gt;ので、
&lt;a href=&#34;https://www.kaitoy.xyz/2016/01/22/pcap4j-meets-windows-containers/&#34;&gt;TP4でバグに阻まれて挫折した&lt;/a&gt;、Windows Containersで&lt;a href=&#34;https://github.com/kaitoy/pcap4j&#34;&gt;Pcap4J&lt;/a&gt;を使ってパケットキャプチャする試みにリトライした話。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h2 id=&#34;osセットアップ&#34;&gt;OSセットアップ&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.kaitoy.xyz/2016/01/22/pcap4j-meets-windows-containers/#windows-containersセットアップ&#34;&gt;TP4のとき&lt;/a&gt;と同じ環境。&lt;/p&gt;

&lt;p&gt;以降は&lt;a href=&#34;https://msdn.microsoft.com/en-us/virtualization/windowscontainers/quick_start/quick_start_windows_server&#34;&gt;Windows Server Containersのクイックスタートガイド&lt;/a&gt;に沿ってセットアップを進める。
&lt;a href=&#34;https://www.kaitoy.xyz/2016/01/22/pcap4j-meets-windows-containers/#windows-containersセットアップ&#34;&gt;TP4&lt;/a&gt;からは大分変わっていて、単一のPowershellスクリプトを実行する形式から、Powershellのコマンドレットを逐次手動実行する形式になっている。
面倒だけど何やってるかわかりやすくて好き。&lt;/p&gt;

&lt;h2 id=&#34;コンテナ機能のインストール&#34;&gt;コンテナ機能のインストール&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;管理者権限のパワーシェルウィンドウを開く&lt;/p&gt;

&lt;p&gt;コマンドプロンプトから以下のコマンドを実行。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;powershell start-process powershell -Verb runas
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;コンテナ機能のインストール&lt;/p&gt;

&lt;p&gt;開いた青いパワーシェルウィンドウで以下のコマンドを実行するとコンテナ機能がインストールされる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;Install-WindowsFeature containers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数分で終わる。&lt;/p&gt;

&lt;p&gt;インストールされたのはHyper-V ContainersじゃなくてWindows Server Containersの方。
クイックスタートガイドをみると、前者がWindows 10向け、後者がWindows Server向けというように住み分けされているっぽい。TP4では両方ともWindows Serverで使えたんだけど。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;再起動&lt;/p&gt;

&lt;p&gt;変更を有効にするために再起動が必要。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;Restart-Computer -Force
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;dockerインストール&#34;&gt;Dockerインストール&lt;/h2&gt;

&lt;p&gt;Dockerは、コンテナイメージの管理やコンテナの起動などもろもろの機能を提供するDockerデーモンと、その機能を利用するためのCLIを提供するDockerクライアントからなる。この節ではそれら両方をインストールする。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Dockerインストールフォルダ作成&lt;/p&gt;

&lt;p&gt;管理者権限のパワーシェルウィンドウを開いて、以下のコマンドでDockerインストールフォルダを作成。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;New-Item -Type Directory -Path &#39;C:\Program Files\docker\&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Dockerデーモンインストール&lt;/p&gt;

&lt;p&gt;まずはデーモンの方をインストール。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;Invoke-WebRequest https://aka.ms/tp5/b/dockerd -OutFile $env:ProgramFiles\docker\dockerd.exe -UseBasicParsing
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数分。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Dockerクライアントインストール&lt;/p&gt;

&lt;p&gt;次にクライアント。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;Invoke-WebRequest https://aka.ms/tp5/b/docker -OutFile $env:ProgramFiles\docker\docker.exe -UseBasicParsing
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数十秒。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;パスの設定&lt;/p&gt;

&lt;p&gt;さっき作ったDockerインストールフォルダにパスを通す。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;[Environment]::SetEnvironmentVariable(&amp;quot;Path&amp;quot;, $env:Path + &amp;quot;;C:\Program Files\Docker&amp;quot;, [EnvironmentVariableTarget]::Machine)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Dockerデーモンをサービスに登録&lt;/p&gt;

&lt;p&gt;パスの設定を反映するためにいったんパワーシェルウィンドウとコマンドプロンプトを閉じて、
また管理者権限でパワーシェルウィンドウ開いて、以下のコマンドでDockerデーモンをサービスに登録する。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;dockerd --register-service
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Dockerデーモン起動&lt;/p&gt;

&lt;p&gt;Dockerデーモンは以下のコマンドで起動できる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;Start-Service docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数秒で立ち上がる。
デフォルトではOS再起動時にはDockerデーモンは自動起動しないので、そのつどこのコマンドを実行する必要がある。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;これでDockerインストール完了。
この時点ではコンテナイメージは何もない。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\Administrator&amp;gt;docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因みにインストールされたDockerのバージョンは1.12開発版。現時点での最新版だ。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\Administrator&amp;gt;docker -v
Docker version 1.12.0-dev, build 8e92415
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;コンテナイメージのインストール&#34;&gt;コンテナイメージのインストール&lt;/h2&gt;

&lt;p&gt;次に、コンテナイメージをインストールする。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;コンテナイメージのパッケージプロバイダをインストール&lt;/p&gt;

&lt;p&gt;いまいち何なのかはよくわからないが、
コンテナイメージのパッケージプロバイダというのをインストールする。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;Install-PackageProvider ContainerImage -Force
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;数十秒。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Windows Server Coreのイメージをインストール&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;Install-ContainerImage -Name WindowsServerCore
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;9GB以上もあるファイルをダウンロードして処理するのでかなり時間がかかる。
50分くらいかかった。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Dockerデーモン再起動&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;Restart-Service docker
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;無事Windows Server Coreイメージがインストールされた。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;PS C:\Users\Administrator&amp;gt; docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
windowsservercore   10.0.14300.1000     5bc36a335344        8 weeks ago         9.354 GB
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;pcap4jコンテナイメージのビルド&#34;&gt;Pcap4Jコンテナイメージのビルド&lt;/h2&gt;

&lt;p&gt;以下を&lt;code&gt;C:\Users\Administrator\Desktop\pcap4j\Dockerfile&lt;/code&gt;に書いて、&lt;code&gt;cd C:\Users\Administrator\Desktop\pcap4j&lt;/code&gt;して、&lt;code&gt;docker build -t pcap4j .&lt;/code&gt;を実行。
(Notepad使ったので、拡張子を表示する設定にして&lt;code&gt;Dockerfile&lt;/code&gt;の&lt;code&gt;.txt&lt;/code&gt;を消さないといけない罠があった。)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dockerfile&#34;&gt;#
# Dockerfile for Pcap4J on Windows
#

FROM windowsservercore:10.0.14300.1000
MAINTAINER Kaito Yamada &amp;lt;kaitoy@pcap4j.org&amp;gt;

# Install Chocolatey.
RUN mkdir C:\pcap4j
WORKDIR c:\\pcap4j
ADD https://chocolatey.org/install.ps1 install.ps1
RUN powershell .\install.ps1

# Install dependencies.
RUN choco install -y nmap jdk7 &amp;amp;&amp;amp; \
    choco install -y maven -version 3.2.5

# Build Pcap4J.
RUN powershell -Command Invoke-WebRequest https://github.com/kaitoy/pcap4j/archive/v1.zip -OutFile pcap4j.zip &amp;amp;&amp;amp; \
    powershell -Command Expand-Archive -Path pcap4j.zip -DestinationPath .
WORKDIR pcap4j-1
RUN powershell -Command &amp;quot;mvn -P distribution-assembly install 2&amp;gt;&amp;amp;1 | Add-Content -Path build.log -PassThru&amp;quot;

# Collect libraries.
RUN mkdir bin &amp;amp;&amp;amp; \
    cd pcap4j-packetfactory-static &amp;amp;&amp;amp; \
    mvn -DoutputDirectory=..\bin -Dmdep.stripVersion=true -DincludeScope=compile dependency:copy-dependencies &amp;amp;&amp;amp; \
    mvn -DoutputDirectory=..\bin -Dmdep.stripVersion=true -DincludeGroupIds=ch.qos.logback dependency:copy-dependencies &amp;amp;&amp;amp; \
    cd ../pcap4j-distribution &amp;amp;&amp;amp; \
    mvn -DoutputDirectory=..\bin -Dmdep.stripVersion=true -DincludeArtifactIds=pcap4j-packetfactory-static,pcap4j-sample dependency:copy-dependencies

# Generate sample script. (C:\pcap4j\pcap4j-1\bin\capture.bat)
RUN echo @echo off &amp;gt; bin\capture.bat &amp;amp;&amp;amp; \
    echo &amp;quot;%JAVA_HOME%\bin\java&amp;quot; -cp C:\pcap4j\pcap4j-1\bin\pcap4j-core.jar;C:\pcap4j\pcap4j-1\bin\pcap4j-packetfactory-static.jar;C:\pcap4j\pcap4j-1\bin\pcap4j-sample.jar;C:\pcap4j\pcap4j-1\bin\jna.jar;C:\pcap4j\pcap4j-1\bin\slf4j-api.jar;C:\pcap4j\pcap4j-1\bin\logback-classic.jar;C:\pcap4j\pcap4j-1\bin\logback-core.jar org.pcap4j.sample.GetNextPacketEx &amp;gt;&amp;gt; bin\capture.bat
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Dockerfileに書いた処理内容は&lt;a href=&#34;https://www.kaitoy.xyz/2016/01/22/pcap4j-meets-windows-containers/#pcap4j-on-windows-container&#34;&gt;TP4のとき&lt;/a&gt;とだいたい同じ。
以下、Dockerfile書いているときに気付いたこと。&lt;/p&gt;

&lt;h4 id=&#34;tp4からのアップデート&#34;&gt;TP4からのアップデート&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;WORKDIR や ENV や COPY でパスの区切りは \ 一つだと消えちゃうので \ か / を使わないといけない。
&lt;div style=&#34;font-size: 0.5em; text-align: right;&#34;&gt;&lt;cite&gt;引用元: &lt;a href=&#34;https://www.kaitoy.xyz/2016/01/22/pcap4j-meets-windows-containers/#pcap4j-on-windows-container&#34;&gt;TP4のときのエントリ&lt;/a&gt;&lt;/cite&gt;&lt;/div&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&#34;https://msdn.microsoft.com/en-us/virtualization/windowscontainers/docker/manage_windows_dockerfile&#34;&gt;このページ&lt;/a&gt;の各コマンドの&lt;strong&gt;Windows Considerations&lt;/strong&gt;に、&lt;code&gt;WORKDIR&lt;/code&gt;のパスの区切りのバックスラッシュはエスケープしないといけないとか、&lt;code&gt;ADD&lt;/code&gt;のパスの区切りはスラッシュじゃないといけないとか書いてある。
TP4のときはなかったような。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;WORKDIR や COPY のコンテナ内のパスに絶対パスを指定したい場合、C:\hoge、C:/hoge、C:\hoge、いずれもダメ。 以下の様なエラーが出る。
&lt;div style=&#34;font-size: 0.5em; text-align: right;&#34;&gt;&lt;cite&gt;引用元: &lt;a href=&#34;https://www.kaitoy.xyz/2016/01/22/pcap4j-meets-windows-containers/#pcap4j-on-windows-container&#34;&gt;TP4のときのエントリ&lt;/a&gt;&lt;/cite&gt;&lt;/div&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;これは直った。&lt;code&gt;WORKDIR c:\\pcap4j&lt;/code&gt;で行ける。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;install.ps1の中でChocolateyのインストーラをHTTPSで取ってこようとしてエラー
&lt;div style=&#34;font-size: 0.5em; text-align: right;&#34;&gt;&lt;cite&gt;引用元: &lt;a href=&#34;https://www.kaitoy.xyz/2016/01/22/pcap4j-meets-windows-containers/#pcap4j-on-windows-container&#34;&gt;TP4のときのエントリ&lt;/a&gt;&lt;/cite&gt;&lt;/div&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;普通に&lt;code&gt;choco install&lt;/code&gt;できたので、HTTPSが使えない制限は消えた模様。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ビルドしてみると、各ステップの実行(多分レイヤの作成)がすごく遅い。
&lt;div style=&#34;font-size: 0.5em; text-align: right;&#34;&gt;&lt;cite&gt;引用元: &lt;a href=&#34;https://www.kaitoy.xyz/2016/01/22/pcap4j-meets-windows-containers/#pcap4j-on-windows-container&#34;&gt;TP4のときのエントリ&lt;/a&gt;&lt;/cite&gt;&lt;/div&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;各ステップの実行は相変わらず重い。特にファイル変更が多いときはすごく重い。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;コンテナの起動は非常に遅い。30秒以上かかる。
&lt;div style=&#34;font-size: 0.5em; text-align: right;&#34;&gt;&lt;cite&gt;引用元: &lt;a href=&#34;https://www.kaitoy.xyz/2016/01/22/pcap4j-meets-windows-containers/#windows-server-containers味見&#34;&gt;TP4のときのエントリ&lt;/a&gt;&lt;/cite&gt;&lt;/div&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;コンテナ起動は早くなったけどまだ5秒くらいかかる。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;WORKDIR や ENV で環境変数が展開されない。
&lt;div style=&#34;font-size: 0.5em; text-align: right;&#34;&gt;&lt;cite&gt;引用元: &lt;a href=&#34;https://www.kaitoy.xyz/2016/01/22/pcap4j-meets-windows-containers/#pcap4j-on-windows-container&#34;&gt;TP4のときのエントリ&lt;/a&gt;&lt;/cite&gt;&lt;/div&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;これはまだ直っていない。&lt;code&gt;%tmp%&lt;/code&gt;、&lt;code&gt;%TMP%&lt;/code&gt;、&lt;code&gt;$TMP&lt;/code&gt;、&lt;code&gt;${TMP}&lt;/code&gt;、どれもだめ。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;コンテナ内で C:\ 直下に . で始まる名前のフォルダ作ると次のステップで消えてる。
&lt;div style=&#34;font-size: 0.5em; text-align: right;&#34;&gt;&lt;cite&gt;引用元: &lt;a href=&#34;https://www.kaitoy.xyz/2016/01/22/pcap4j-meets-windows-containers/#pcap4j-on-windows-container&#34;&gt;TP4のときのエントリ&lt;/a&gt;&lt;/cite&gt;&lt;/div&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;これは再現しなかった。以前のも勘違いだったのかもしれない。
なんにせよデフォルトの.m2フォルダのパスが&lt;code&gt;C:\Users\ContainerAdministrator\.m2&lt;/code&gt;になったので気にしなくてよくなった。&lt;/p&gt;

&lt;h4 id=&#34;ビルドエラー-hcsshim-importlayer-failed-in-win32-the-filename-or-extension-is-too-long-0xce&#34;&gt;ビルドエラー: hcsshim::ImportLayer failed in Win32: The filename or extension is too long. (0xce)&lt;/h4&gt;

&lt;p&gt;&lt;code&gt;choco install&lt;/code&gt;の後で以下のエラーが出た。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;re-exec error: exit status 1: output: time=&amp;quot;2016-07-09T19:57:22-07:00&amp;quot; level=error msg=&amp;quot;hcsshim::ImportLayer failed in Win32: The filename or extension is too long. (0xce) layerId=\\\\?\\C:\\ProgramData\\docker\\windowsfilter\\103de6bf1358c506510ad67990f09ec3e2f10f9e866e846df5a88c04f5edf7aa flavour=1 folder=C:\\Windows\\TEMP\\hcs719016711&amp;quot;
hcsshim::ImportLayer failed in Win32: The filename or extension is too long. (0xce) layerId=\\?\C:\ProgramData\docker\windowsfilter\103de6bf1358c506510ad67990f09ec3e2f10f9e866e846df5a88c04f5edf7aa flavour=1 folder=C:\Windows\TEMP\hcs719016711
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;調べたら&lt;a href=&#34;https://github.com/docker/docker/issues/22449&#34;&gt;DockerのGitHub Issues&lt;/a&gt;に登録されていた。
ここのコメントを参考に以下のコマンドでコンテナホストのアップデートをしたら発生しなくなった。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;Invoke-WebRequest https://aka.ms/tp5/Update-Container-Host -OutFile update-containerhost.ps1
.\update-containerhost.ps1
Restart-Computer -Force
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;git-cloneできない&#34;&gt;git cloneできない&lt;/h4&gt;

&lt;p&gt;Pcap4Jのソースをダウンロードしたかったんだけど、なぜか&lt;code&gt;git clone&lt;/code&gt;がHTTPSでもGITプロトコルでもエラーを返す。
原因を調べるのが面倒で結局zipでダウンロードするようにした。&lt;/p&gt;

&lt;h4 id=&#34;未実装の機能&#34;&gt;未実装の機能&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/engine/reference/builder/&#34;&gt;Dockerfileのリファレンス&lt;/a&gt;に載っていて、Windows向けのサンプルも書いてあるのに、&lt;a href=&#34;https://docs.docker.com/engine/reference/builder/#/escape&#34;&gt;escapeディレクティブ&lt;/a&gt;と&lt;a href=&#34;https://docs.docker.com/engine/reference/builder/#/shell&#34;&gt;SHELLコマンド&lt;/a&gt;
が使えなかった。&lt;/p&gt;

&lt;h2 id=&#34;コンテナ起動&#34;&gt;コンテナ起動&lt;/h2&gt;

&lt;p&gt;とりあえず上記DockerfileでPcap4Jコンテナイメージのビルドはできた。&lt;/p&gt;

&lt;p&gt;以下のコマンドでそのイメージからコンテナを起動。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\Administrator&amp;gt;docker run -it pcap4j cmd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;コンテナ内で&lt;code&gt;ipconfig&lt;/code&gt;すると&lt;code&gt;vEthernet (Temp Nic Name)&lt;/code&gt;という名のネットワークインターフェースがあることがわかる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\pcap4j\pcap4j-1\bin&amp;gt;ipconfig

Windows IP Configuration


Ethernet adapter vEthernet (Temp Nic Name):

   Connection-specific DNS Suffix  . : localdomain
   Link-local IPv6 Address . . . . . : fe80::59cf:1491:6f8e:30c8%18
   IPv4 Address. . . . . . . . . . . : 172.23.71.6
   Subnet Mask . . . . . . . . . . . : 255.240.0.0
   Default Gateway . . . . . . . . . : 172.16.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;けどPcap4Jからは見えなかった。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\pcap4j\pcap4j-1\bin&amp;gt;capture.bat
org.pcap4j.sample.GetNextPacketEx.count: 5
org.pcap4j.sample.GetNextPacketEx.readTimeout: 10
org.pcap4j.sample.GetNextPacketEx.snaplen: 65536


18:49:00.582 [main] INFO  org.pcap4j.core.Pcaps - No NIF was found.
java.io.IOException: No NIF to capture.
        at org.pcap4j.sample.GetNextPacketEx.main(GetNextPacketEx.java:45)java:44)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;コンテナには&lt;code&gt;ContainerAdministrator&lt;/code&gt;というユーザでログインしていて、これの権限が弱いせいなんじゃないかと。
コンテナ内にも&lt;code&gt;Administrator&lt;/code&gt;というユーザがあるようだったので、こっちでコマンド実行するよう奮闘した。&lt;/p&gt;

&lt;h2 id=&#34;コンテナ内でadministratorでコマンド実行したい&#34;&gt;コンテナ内でAdministratorでコマンド実行したい&lt;/h2&gt;

&lt;h4 id=&#34;userコマンド&#34;&gt;USERコマンド&lt;/h4&gt;

&lt;p&gt;Dockerfileのコマンドに&lt;a href=&#34;https://docs.docker.com/engine/reference/builder/#/user&#34;&gt;USER&lt;/a&gt;というのがあるので、&lt;code&gt;USER Administrator&lt;/code&gt;をDockerfileの末尾に追加してみたら以下のエラー。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;The daemon on this platform does not support the command &#39;user&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;userオプション&#34;&gt;&amp;ndash;userオプション&lt;/h4&gt;

&lt;p&gt;docker runコマンドに&lt;a href=&#34;https://docs.docker.com/compose/reference/run/&#34;&gt;&amp;ndash;user&lt;/a&gt;というオプションがあるので以下のように試してみたところ、オプションは無視されて&lt;code&gt;ContainerAdministrator&lt;/code&gt;でコンテナに入った。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;docker run -it --user Administrator pcap4j cmd
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;runas&#34;&gt;runas&lt;/h4&gt;

&lt;p&gt;ちょっと発想の転換をして、&lt;code&gt;ContainerAdministrator&lt;/code&gt;でコンテナに入った後sudoみたいなことをすればいいかと思い、&lt;a href=&#34;https://technet.microsoft.com/en-us/library/bb490994.aspx&#34;&gt;runas&lt;/a&gt;コマンドを試したけどだめだった。
よく分からないエラーがでるし、そもそも&lt;code&gt;Administrator&lt;/code&gt;のパスワードがわからない。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\pcap4j\pcap4j-1\bin&amp;gt;runas /user:Administrator cmd
Enter the password for Administrator:
Attempting to start cmd as user &amp;quot;92EC7B3B09B4\Administrator&amp;quot; ...
RUNAS ERROR: Unable to run - cmd
1326: The user name or password is incorrect.
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\pcap4j\pcap4j-1\bin&amp;gt;runas /user:&amp;quot;User Manager\Administrator&amp;quot; capture.bat
Enter the password for User Manager\Administrator:
RUNAS ERROR: Unable to acquire user password
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;enter-pssession&#34;&gt;Enter-PSSession&lt;/h4&gt;

&lt;p&gt;フォーラムに行ったら&lt;a href=&#34;https://social.msdn.microsoft.com/Forums/en-US/0b6bd405-a235-4608-a06b-a09b9ba08b2e/runas-administrator?forum=windowscontainers&#34;&gt;Enter-PSSession&lt;/a&gt;を使う方法が書いてあった。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://technet.microsoft.com/en-us/library/hh849707.aspx&#34;&gt;Enter-PSSession&lt;/a&gt;はリモートシステムに接続するコマンドレットで、&lt;code&gt;-ContainerName&lt;/code&gt;オプションを使えばコンテナにも接続できる。&lt;/p&gt;

&lt;p&gt;試したら、コンテナが見つからないというエラーが出た。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\Administrator&amp;gt;powershell -command Enter-PSSession -ContainerName amazing_archimedes -RunAsAdministrator
Enter-PSSession : The input ContainerName amazing_archimedes does not exist, or the corresponding container is not running.
At line:1 char:1
+ Enter-PSSession -ContainerName amazing_archimedes -RunAsAdministrator
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : InvalidOperation: (:) [Enter-PSSession], PSInvalidOperationException
    + FullyQualifiedErrorId : CreateRemoteRunspaceForContainerFailed,Microsoft.PowerShell.Commands.EnterPSSessionCommand
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://technet.microsoft.com/en-us/library/hh849719.aspx&#34;&gt;Invoke-Command&lt;/a&gt;もコンテナをターゲットにできるので試してみたけど、同様のエラー。&lt;/p&gt;

&lt;p&gt;どうもパワーシェルで扱うコンテナやコンテナイメージが、dockerコマンドが扱うものとは別になっているせいっぽい。
そんなことがTP4のときに見たドキュメントに書いてあったのを思い出した。(このドキュメントは消えてた。)&lt;/p&gt;

&lt;p&gt;実際、&lt;code&gt;docker ps&lt;/code&gt;では見えているコンテナが、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\Administrator&amp;gt;docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
a711497f29d8        pcap4j              &amp;quot;cmd&amp;quot;               13 minutes ago      Up 12 minutes                           amazing_archimedes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;コマンドレットからだと見えない。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\Administrator&amp;gt;powershell -command Get-Container
WARNING: Based on customer feedback, we are updating the Containers PowerShell module to better align with Docker. As part of that some cmdlet and parameter names may change in future releases. To learn more about these changes as well as to join in the design process or provide usage feedback please refer to http://aka.ms/windowscontainers/powershell
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;そうなると、パワーシェルのコマンドレットには&lt;code&gt;docker build&lt;/code&gt;にあたるものがないのでもうどうしようもない。&lt;/p&gt;

&lt;p&gt;そもそも、TP4の頃のコマンドレットは&lt;a href=&#34;https://msdn.microsoft.com/en-us/virtualization/windowscontainers/management/docker-powershell&#34;&gt;廃止になって&lt;/a&gt;、&lt;a href=&#34;https://github.com/Microsoft/Docker-PowerShell/&#34;&gt;新しいコマンドレット&lt;/a&gt;を開発中らしい。やはりdockerコマンドとコマンドレットでコンテナの相互運用ができない仕様にユーザから相当つっこみがあったようだ。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Enter-PSSession&lt;/code&gt;や&lt;code&gt;Invoke-Command&lt;/code&gt;の&lt;code&gt;-ContainerName&lt;/code&gt;オプションもその内修正されるであろう。
それまで待つか。&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Pcap4J Meets Windows Containers</title>
          <link>https://www.kaitoy.xyz/2016/01/22/pcap4j-meets-windows-containers/</link>
          <pubDate>Fri, 22 Jan 2016 17:46:43 -0700</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2016/01/22/pcap4j-meets-windows-containers/</guid>
          <description>

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://msdn.microsoft.com/en-us/virtualization/windowscontainers/about/about_overview&#34;&gt;Windows Containers&lt;/a&gt;&lt;/strong&gt; で &lt;strong&gt;&lt;a href=&#34;https://github.com/kaitoy/pcap4j&#34;&gt;Pcap4J&lt;/a&gt;&lt;/strong&gt; のコンテナをビルドしてみた話。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h2 id=&#34;windows-containersとは&#34;&gt;Windows Containersとは&lt;/h2&gt;

&lt;p&gt;Windows Containersは、Microsoftが&lt;a href=&#34;https://www.docker.com/company&#34;&gt;Docker, Inc&lt;/a&gt;と提携して開発している&lt;a href=&#34;http://www.sophia-it.com/content/%E3%82%B3%E3%83%B3%E3%83%86%E3%83%8A%E6%8A%80%E8%A1%93&#34;&gt;コンテナ技術&lt;/a&gt;で、Windows版&lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt;とも言われる機能。
今年リリースされる &lt;strong&gt;Windows Server 2016&lt;/strong&gt; に実装される予定で、その3つめのテクニカルプレビューである &lt;strong&gt;Windows Server 2016 Technical Preview 3&lt;/strong&gt; (2015/8/19公開)から評価できるようになった。&lt;/p&gt;

&lt;p&gt;Windows Containersには次の二種類がある。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Windows Server Containers&lt;/p&gt;

&lt;p&gt;プロセスと名前空間の分離を実現する機能で、これによるコンテナはカーネルをホストと共有する。
つまり本家Dockerに近い形の機能。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hyper-V Containers&lt;/p&gt;

&lt;p&gt;それぞれのコンテナを軽量化されたHyper-Vの仮想マシンっぽいものの上で動かす機能。
このコンテナの実行にはHyper-Vが必要。
Windows Server Containersよりコンテナ間の分離性が高く、カーネルの共有もしないが、そもそもそれってコンテナなの?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;どちらも同じようなインターフェースで操作でき、このインターフェースには&lt;a href=&#34;https://msdn.microsoft.com/en-us/virtualization/windowscontainers/reference/ps_docker_comparison&#34;&gt;PowershellのコマンドレットとDockerコマンドの二種類がある&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;より詳しくは、&lt;a href=&#34;https://msdn.microsoft.com/en-us/virtualization/windowscontainers/about/about_overview&#34;&gt;Microsoftによる解説&lt;/a&gt;や&lt;a href=&#34;http://www.atmarkit.co.jp/ait/articles/1512/11/news022.html&#34;&gt;@ITのこの記事&lt;/a&gt;がわかりやすい。
また、&lt;a href=&#34;http://qiita.com/Arturias/items/3e82de8328067d0e03a3&#34;&gt;Qiitaのこの記事&lt;/a&gt;がDockerとWindows Server Containersのアーキテクチャを詳細に説明していて面白い。&lt;/p&gt;

&lt;h2 id=&#34;windows-containersセットアップ&#34;&gt;Windows Containersセットアップ&lt;/h2&gt;

&lt;p&gt;まず、Windows 7 x64のノートPCにVMware Player 7.1.0を入れてWindows 10 x64用のVM(CPU2つとメモリ2.5GB)を作り、そこに2015/11/19に公開された &lt;strong&gt;Windows Server 2016 Technical Preview 4&lt;/strong&gt; をインストール。
コマンドでいろいろ設定するの慣れていないのでGUI(Desktop Experience)付きで。
(リモートデスクトップ使えばよかったのかもしれないけど。)
ロケールは英語以外は問題が起きそうなので英語で。&lt;/p&gt;

&lt;p&gt;このVMに、&lt;a href=&#34;https://msdn.microsoft.com/en-us/virtualization/windowscontainers/quick_start/inplace_setup&#34;&gt;Microsoftのセットアップガイド&lt;/a&gt;と&lt;a href=&#34;http://www.atmarkit.co.jp/ait/articles/1512/14/news006.html&#34;&gt;@ITの記事&lt;/a&gt;を参照しながらWindows Containersをセットアップ。&lt;/p&gt;

&lt;p&gt;後者の記事によると、Hyper-V ContainersをVM上にセットアップするには、&lt;a href=&#34;https://msdn.microsoft.com/en-us/virtualization/hyperv_on_windows/user_guide/nesting&#34;&gt;Nested Virtualization&lt;/a&gt;というHyper-VのVMの上でHyper-Vを動かす機能を有効にしたホスト上のHyper-V VMを使わないといけないようなので、Windows Server Containersの方を試すことに。&lt;/p&gt;

&lt;p&gt;Windows Server Containersをセットアップする手順は以下。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;VM上でコマンドプロンプトを開いて &lt;code&gt;powershell start-process powershell -Verb runas&lt;/code&gt; を実行。&lt;/li&gt;
&lt;li&gt;青いパワーシェルウィンドウが開くのでそこで &lt;code&gt;wget -uri https://aka.ms/tp4/Install-ContainerHost -OutFile C:\Install-ContainerHost.ps1&lt;/code&gt; を実行。&lt;code&gt;Install-ContainerHost.ps1&lt;/code&gt; というスクリプトがダウンロードされる。&lt;/li&gt;
&lt;li&gt;青いパワーシェルウィンドウで &lt;code&gt;C:\Install-ContainerHost.ps1&lt;/code&gt; を実行するとWindows Server Containersのインストールが始まる。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://www.kaitoy.xyz/images/pcap4j-meets-windows-containers/install.png&#34; alt=&#34;install.png&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;途中再起動が一回あって、ログインしたらインストール処理が再開した。
全部で2時間以上かかった。&lt;/p&gt;

&lt;p&gt;仮想Ethernetスイッチ接続の追加に失敗したというエラーが出たけどなんなんだろう。
&lt;code&gt;ipconfig&lt;/code&gt; の出力によると &lt;code&gt;vEthernet&lt;/code&gt; というDockerの&lt;a href=&#34;https://www.kaitoy.xyz/2015/07/25/how-to-capture-packets-on-a-local-network-with-pcap4j-container/#docker-network&#34;&gt;virtual Ethernet bridge&lt;/a&gt;にあたるものはちゃんと作られているみたいなんだけど。&lt;/p&gt;

&lt;h2 id=&#34;windows-server-containers味見&#34;&gt;Windows Server Containers味見&lt;/h2&gt;

&lt;p&gt;コマンドプロンプトで &lt;code&gt;docker images&lt;/code&gt; を実行すると、既に &lt;code&gt;windowsservercore&lt;/code&gt; というコンテナイメージが入っていることがわかる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\Administrator&amp;gt;docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
windowsservercore   10.0.10586.0        6801d964fda5        11 weeks ago        0 B
windowsservercore   latest              6801d964fda5        11 weeks ago        0 B
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;docker run -it windowsservercore cmd&lt;/code&gt; を実行すると &lt;code&gt;windowsservercore&lt;/code&gt; からコンテナを起動してその上でコマンドプロンプトを起動できる。
コンテナの起動は非常に遅い。30秒以上かかる。これは今の時点での&lt;a href=&#34;https://msdn.microsoft.com/virtualization/windowscontainers/about/work_in_progress#windows-containers-start-slowly&#34;&gt;制限&lt;/a&gt;らしい。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;docker login --help&lt;/code&gt; するとわかるが、コンテナイメージのリポジトリは &lt;code&gt;https://registry-win-tp3.docker.io/v1/&lt;/code&gt; という仮っぽいサーバにあって、&lt;code&gt;docker search *&lt;/code&gt; を実行するとそこに登録されたイメージのリストが見れる。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;C:\Users\Administrator&amp;gt;docker search *
NAME                 DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED
microsoft/aspnet     ASP.NET 5 framework installed in a Windows...   1         [OK]       [OK]
microsoft/django     Django installed in a Windows Server Core ...   1                    [OK]
microsoft/dotnet35   .NET 3.5 Runtime installed in a Windows Se...   1         [OK]       [OK]
microsoft/golang     Go Programming Language installed in a Win...   1                    [OK]
microsoft/httpd      Apache httpd installed in a Windows Server...   1                    [OK]
microsoft/iis        Internet Information Services (IIS) instal...   1         [OK]       [OK]
microsoft/mongodb    MongoDB installed in a Windows Server Core...   1                    [OK]
microsoft/mysql      MySQL installed in a Windows Server Core b...   1                    [OK]
microsoft/nginx      Nginx installed in a Windows Server Core b...   1                    [OK]
microsoft/node       Node installed in a Windows Server Core ba...   1                    [OK]
microsoft/php        PHP running on Internet Information Servic...   1                    [OK]
microsoft/python     Python installed in a Windows Server Core ...   1                    [OK]
microsoft/rails      Ruby on Rails installed in a Windows Serve...   1                    [OK]
microsoft/redis      Redis installed in a Windows Server Core b...   1                    [OK]
microsoft/ruby       Ruby installed in a Windows Server Core ba...   1                    [OK]
microsoft/sqlite     SQLite installed in a Windows Server Core ...   1                    [OK]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これらはちゃんと &lt;code&gt;docker pull&lt;/code&gt; して使える。
けど多分 &lt;code&gt;docker push&lt;/code&gt; はできない。&lt;/p&gt;

&lt;h2 id=&#34;pcap4j-on-windows-container&#34;&gt;Pcap4J on Windows Container&lt;/h2&gt;

&lt;p&gt;結論から言うと、以下の &lt;code&gt;Dockerfile&lt;/code&gt; を書いて &lt;code&gt;docker build&lt;/code&gt; してPcap4Jをコンテナ上でビルドするところまではできたが、それを実行してもNIFが全く検出できず、よってパケットキャプチャも実行できなかった。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#
# Dockerfile for Pcap4J on Windows
#

FROM windowsservercore:latest
MAINTAINER Kaito Yamada &amp;lt;kaitoy@pcap4j.org&amp;gt;

# Install Chocolatey.
RUN mkdir C:\pcap4j
WORKDIR /pcap4j
ADD https://chocolatey.org/install.ps1 install.ps1
RUN powershell .\install.ps1

# Install dependencies.
RUN choco install -y nmap maven git jdk7

# Build Pcap4J.
RUN git clone git://github.com/kaitoy/pcap4j.git
WORKDIR pcap4j
RUN powershell -NoProfile -ExecutionPolicy Bypass -Command &amp;quot;mvn &#39;-Dmaven.repo.local=C:\pcap4j\repo&#39; -P distribution-assembly install 2&amp;gt;&amp;amp;1 | add-content -Path build.log -pass

# Collect libraries.
RUN mkdir bin &amp;amp;&amp;amp; \
    cd pcap4j-packetfactory-static &amp;amp;&amp;amp; \
    mvn -Dmaven.repo.local=C:\pcap4j\repo -DoutputDirectory=..\bin -Dmdep.stripVersion=true -DincludeScope=compile dependency:copy-dependencies &amp;amp;&amp;amp; \
    mvn -Dmaven.repo.local=C:\pcap4j\repo -DoutputDirectory=..\bin -Dmdep.stripVersion=true -DincludeGroupIds=ch.qos.logback dependency:copy-dependencies &amp;amp;&amp;amp; \
    cd ../pcap4j-distribution &amp;amp;&amp;amp; \
    mvn -Dmaven.repo.local=C:\pcap4j\repo -DoutputDirectory=..\bin -Dmdep.stripVersion=true -DincludeArtifactIds=pcap4j-packetfactory-static,pcap4j-sample dependency:copy-dependencies

# Generate sample script. (C:\pcap4j\pcap4j\bin\capture.bat)
RUN echo @echo off &amp;gt; bin\capture.bat &amp;amp;&amp;amp; \
    echo &amp;quot;%JAVA_HOME%\bin\java&amp;quot; -cp C:\pcap4j\pcap4j\bin\pcap4j-core.jar;C:\pcap4j\pcap4j\bin\pcap4j-packetfactory-static.jar;C:\pcap4j\pcap4j\bin\pcap4j-sample.jar;C:\pcap4j\pcap4j\bin\jna.jar;C:\pcap4j\pcap4j\bin\slf4j-api.jar;C:\pcap4j\pcap4j\bin\logback-classic.jar;C:\pcap4j\pcap4j\bin\logback-core.jar org.pcap4j.sample.GetNextPacketEx &amp;gt;&amp;gt; bin\capture.bat
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この &lt;code&gt;Dockerfile&lt;/code&gt; でやっていることはだいたい以下。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://chocolatey.org/&#34;&gt;Chocolatey&lt;/a&gt;をインストール。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nmap.org/&#34;&gt;Nmap&lt;/a&gt;と&lt;a href=&#34;https://maven.apache.org/&#34;&gt;Maven&lt;/a&gt;と&lt;a href=&#34;https://git-scm.com/&#34;&gt;Git&lt;/a&gt;とJDK7をChocolateyでインストール。&lt;/li&gt;
&lt;li&gt;Pcap4Jのソースを &lt;code&gt;git clone&lt;/code&gt; でダウンロード。&lt;/li&gt;
&lt;li&gt;MavenでPcap4Jのビルドを実行。&lt;/li&gt;
&lt;li&gt;Pcap4Jのサンプルクラスを実行するスクリプトを生成。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;2でNmapは&lt;a href=&#34;http://www.winpcap.org/&#34;&gt;WinPcap&lt;/a&gt;の代わりに入れている。
GUI無しの環境でWinPcapをChocolateyで入れようとしても、エラーが発生したりしなかったりして、しかもどちらにせよ正常に入らない。
これはWinPcapのインストーラがサイレントインストールをサポートしていないから。
Nmapはサイレントインストールできて、インストール処理中にWinPcapを入れてくれるのでありがたい。&lt;/p&gt;

&lt;p&gt;ビルドしてみると、各ステップの実行(多分レイヤの作成)がすごく遅い。
&lt;code&gt;RUN choco install -y nmap maven git jdk7&lt;/code&gt; の後、次のコマンド実行まで30分くらい固まった。&lt;/p&gt;

&lt;p&gt;また、&lt;code&gt;Dockerfile&lt;/code&gt; を書いていて以下のバグに悩まされた。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;WORKDIR&lt;/code&gt; や &lt;code&gt;ENV&lt;/code&gt; で環境変数が展開されない。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;  ENV hoge %tmp%
  RUN echo %hoge%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;とすると &lt;code&gt;%tmp%&lt;/code&gt; と表示される。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;WORKDIR&lt;/code&gt; や &lt;code&gt;ENV&lt;/code&gt; や &lt;code&gt;COPY&lt;/code&gt; でパスの区切りは &lt;code&gt;\&lt;/code&gt; 一つだと消えちゃうので &lt;code&gt;\\&lt;/code&gt; か &lt;code&gt;/&lt;/code&gt; を使わないといけない。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;WORKDIR&lt;/code&gt; や &lt;code&gt;COPY&lt;/code&gt; のコンテナ内のパスに絶対パスを指定したい場合、&lt;code&gt;C:\hoge&lt;/code&gt;、&lt;code&gt;C:/hoge&lt;/code&gt;、&lt;code&gt;C:\\hoge&lt;/code&gt;、いずれもダメ。
以下の様なエラーが出る。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  GetFileAttributesEx \\?\Volume{67df3c84-a0ef-11e5-9a63-000c2976fbc3}\C:: The filename, directory name, or volume label syntax is incorrect.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;UNIX式に &lt;code&gt;/hoge&lt;/code&gt; とするといける。C以外のドライブを指定したいときはどうするんだろう。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;コンテナ内で &lt;code&gt;C:\&lt;/code&gt; 直下に &lt;code&gt;.&lt;/code&gt; で始まる名前のフォルダ作ると次のステップで消えてる。
&lt;code&gt;.&lt;/code&gt; で始まる名前のファイルは &lt;code&gt;C:\&lt;/code&gt; 直下じゃなくても次のステップで消えてる。
Mavenのリポジトリがデフォルトで &lt;code&gt;C:\.m2\&lt;/code&gt; 以下にできるのではまる。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;これらのバグを乗り越えて頑張って &lt;code&gt;Dockerfile&lt;/code&gt; 書いたのに、NIFの検出すらできなかったという哀しい結果。
&lt;code&gt;pcap_lookupdev&lt;/code&gt; が以下のエラーを吐いて &lt;code&gt;NULL&lt;/code&gt; を返してきてたので、なんとなくコンテナのNIFに長すぎる名前がついていて検出失敗しているんじゃないかと。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;PacketGetAdapterNames: The data area passed to a system call is too small. (122)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;因みにコンテナ内から見えるNIFは一つで、以下の構成。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Ethernet adapter vEthernet (Virtual Switch-d206475ce13256766b9a16383540a740fe31da8d20499349fe98693393a8490f-0):

   Connection-specific DNS Suffix  . : localdomain
   Link-local IPv6 Address . . . . . : fe80::4086:d11e:5e6:28fe%26
   IPv4 Address. . . . . . . . . . . : 172.16.0.2
   Subnet Mask . . . . . . . . . . . : 255.240.0.0
   Default Gateway . . . . . . . . . : 172.16.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;コンテナ内から &lt;code&gt;www.google.com&lt;/code&gt; とかにping届いたので、このNIFはちゃんと働いていはずなんだけどPcap4Jから見えない。&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;

&lt;p&gt;後日上記 &lt;code&gt;Dockerfile&lt;/code&gt; でビルドしてみたら、&lt;code&gt;RUN powershell .\install.ps1&lt;/code&gt; で以下のエラーが出るようになった。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;The request was aborted: Could not create SSL/TLS secure channel.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;install.ps1の中でChocolateyのインストーラをHTTPSで取ってこようとしてエラーになっている模様。
Windows Containersの&lt;a href=&#34;https://msdn.microsoft.com/en-us/virtualization/windowscontainers/about/work_in_progress#https-and-tls-are-not-supported&#34;&gt;ドキュメント&lt;/a&gt;や&lt;a href=&#34;https://social.msdn.microsoft.com/Forums/en-US/c0d93dda-37b7-4a2c-9a78-55e4ba0b88f5/https-support-in-windowsservercore-image?forum=windowscontainers&#34;&gt;フォーラム&lt;/a&gt;にHTTPSが使えないという制限が載っているけどこのせい?
ちょっと前にやったときは同じ &lt;code&gt;Dockerfile&lt;/code&gt; でビルドできたはずなんだけど。&lt;/p&gt;

&lt;p&gt;試しに以下の処理を挟んでChocolateyのインストーラをHTTPで取ってくるようにしたらChocolateyのインストールまではできた。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;RUN powershell $(Get-Content install.ps1) -replace \&amp;quot;https\&amp;quot;,\&amp;quot;http\&amp;quot; &amp;gt; install.mod.ps1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;けど &lt;code&gt;choco install&lt;/code&gt; がHTTPS使うので結局駄目だった。&lt;/p&gt;

&lt;p&gt;もう面倒なのでHTTPSの制限がとれるのをまとう。&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Another way to capture LAN packets with pcap4j container</title>
          <link>https://www.kaitoy.xyz/2015/07/27/another-way-to-capture-lan-packets-with-pcap4j-container/</link>
          <pubDate>Mon, 27 Jul 2015 23:41:49 -0600</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2015/07/27/another-way-to-capture-lan-packets-with-pcap4j-container/</guid>
          <description>

&lt;p&gt;2 days ago, I posted an article &lt;a href=&#34;https://www.kaitoy.xyz/2015/07/25/how-to-capture-packets-on-a-local-network-with-pcap4j-container/&#34;&gt;How to capture packets on a local network with Pcap4J container&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Today, I was reading &lt;a href=&#34;https://docs.docker.com/reference/run/#network-settings&#34;&gt;Docker Docs&lt;/a&gt; and found another way to do it.
I&amp;rsquo;m writing about it here.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h3 id=&#34;net-option-for-docker-run&#34;&gt;&amp;ndash;net option for docker run&lt;/h3&gt;

&lt;p&gt;When we start a docker container we use &lt;code&gt;docker run&lt;/code&gt; command. It accepts some options.
&lt;code&gt;--net&lt;/code&gt; is one of them, which is to set a network mode for a container.
Network modes &lt;code&gt;--net&lt;/code&gt; takes are &lt;code&gt;bridge&lt;/code&gt;, &lt;code&gt;none&lt;/code&gt;, &lt;code&gt;container:&amp;lt;name|id&amp;gt;&lt;/code&gt;, and &lt;code&gt;host&lt;/code&gt;.
The &lt;code&gt;bridge&lt;/code&gt; is the default mode where containers connect to the virtual Ethernet bridge &lt;code&gt;docker0&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;What I use in this article is &lt;code&gt;host&lt;/code&gt; mode. If it&amp;rsquo;s specified containers use the host network stack,
which means Pcap4J on a container with the &lt;code&gt;host&lt;/code&gt; mode can see network interfaces on its host and sniff network traffic via them directly.&lt;/p&gt;

&lt;p&gt;This sounds easy. And more, according to the Docker Docs, the &lt;code&gt;host&lt;/code&gt; mode gives significantly better networking performance than the &lt;code&gt;bridge&lt;/code&gt; mode. But instead, &lt;code&gt;host&lt;/code&gt; is insecure. (See &lt;a href=&#34;https://docs.docker.com/reference/run/#mode-host&#34;&gt;Docker Docs - Mode: host&lt;/a&gt; for the details.)&lt;/p&gt;

&lt;h3 id=&#34;what-i-did&#34;&gt;What I did&lt;/h3&gt;

&lt;p&gt;In the same environment with &lt;a href=&#34;https://www.kaitoy.xyz/2015/07/25/how-to-capture-packets-on-a-local-network-with-pcap4j-container/#what-i-did:a3622224f79a64f15ba6f2b66e1010d9&#34;&gt;2 days ago&lt;/a&gt;, I did the followings:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Start a Pcap4J container with the network mode set to host&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;  [root@localhost ~]# docker run --name pcap4j-hostnet --net=host kaitoy/pcap4j:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s it.&lt;/p&gt;

&lt;p&gt;The above command create a container named &lt;code&gt;pcap4j-hostnet&lt;/code&gt; from the image &lt;code&gt;kaitoy/pcap4j:latest&lt;/code&gt; and execute &lt;code&gt;/bin/sh /usr/local/src/pcap4j/bin/capture.sh eth0 false&lt;/code&gt; in the container.
  The &lt;code&gt;capture.sh&lt;/code&gt; starts packet capturing on &lt;code&gt;eth0&lt;/code&gt; using Pcap4J.
  This &lt;code&gt;eth0&lt;/code&gt; is the interface of the docker host mashine because the network mode is set to &lt;code&gt;host&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;What a easy way.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
      
    
      
        <item>
          <title>How to capture packets on a local network with Pcap4J container</title>
          <link>https://www.kaitoy.xyz/2015/07/25/how-to-capture-packets-on-a-local-network-with-pcap4j-container/</link>
          <pubDate>Sat, 25 Jul 2015 19:05:06 -0600</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2015/07/25/how-to-capture-packets-on-a-local-network-with-pcap4j-container/</guid>
          <description>

&lt;p&gt;I&amp;rsquo;ll show how to capture packets on a local network with Pcap4J container.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h3 id=&#34;docker-network&#34;&gt;Docker network&lt;/h3&gt;

&lt;p&gt;By default, Docker containers are not connected to a local network.
They are connected only to a virtual network Docker creates as like below:&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;https://www.kaitoy.xyz/images/docker_network.jpg&#34; alt=&#34;Docker network&#34; width=&#34;500&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Refer to &lt;a href=&#34;https://docs.docker.com/articles/networking/&#34;&gt;the Docker doc&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h3 id=&#34;what-s-a-challenge&#34;&gt;What&amp;rsquo;s a challenge&lt;/h3&gt;

&lt;p&gt;In order to let a Pcap4J container capture packets in a local (real) network,
we need to directly connect the container to the local network,
because docker0 forwards only packets the destinations of which are in the virtual network.&lt;/p&gt;

&lt;p&gt;How to do it is explained in some articles.
I referred to one of them, &lt;a href=&#34;http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/&#34;&gt;Four ways to connect a docker container to a local network in Odd Bits blog&lt;/a&gt;, and succeeded in local network capturing using the 4th way.&lt;/p&gt;

&lt;p&gt;What I actually did is as follows.&lt;/p&gt;

&lt;h3 id=&#34;what-i-did&#34;&gt;What I did&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Environment&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;OS: CentOS 7.0 (on VMware Player 7.1.0 on Windows 7)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@localhost ~]# uname -a
Linux localhost.localdomain 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;user: root&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Pcap4J version: 1.5.1-SNAPSHOT&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Docker version: 1.6.2&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Network interfaces:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;  [root@localhost ~]# ip addr show
  1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN
      link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
      inet 127.0.0.1/8 scope host lo
         valid_lft forever preferred_lft forever
      inet6 ::1/128 scope host
         valid_lft forever preferred_lft forever
  2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
      link/ether 00:0c:29:8e:95:27 brd ff:ff:ff:ff:ff:ff
      inet 192.168.1.123/24 brd 192.168.1.255 scope global dynamic eth0
         valid_lft 85975sec preferred_lft 85975sec
      inet6 2601:282:8102:2623:20c:29ff:fe8e:9527/64 scope global dynamic
         valid_lft 221469sec preferred_lft 221469sec
      inet6 fe80::20c:29ff:fe8e:9527/64 scope link
         valid_lft forever preferred_lft forever
  3: docker0: &amp;lt;NO-CARRIER,BROADCAST,MULTICAST,UP&amp;gt; mtu 1500 qdisc noqueue state DOWN
      link/ether 56:84:7a:fe:97:99 brd ff:ff:ff:ff:ff:ff
      inet 172.17.42.1/16 scope global docker0
         valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Prerequisites:

&lt;ul&gt;
&lt;li&gt;Docker is installed and Docker service is started&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://man7.org/linux/man-pages/man1/nsenter.1.html&#34;&gt;nsenter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step by step&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Preparing&lt;/p&gt;

&lt;p&gt;Create a utility script &lt;code&gt;docker-pid&lt;/code&gt; with the following content and place it somewhere in the &lt;code&gt;PATH&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;  #!/bin/sh
  exec docker inspect --format &#39;{{ .State.Pid }}&#39; &amp;quot;$@&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This script show the PID of a docker container by name or ID.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Pull the latest Pcap4J image&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@localhost ~]# docker pull kaitoy/pcap4j
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start a Pcap4J container with wait mode&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@localhost ~]# docker run --name pcap4j-br kaitoy/pcap4j:latest eth1 true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This container (&lt;code&gt;pcap4j-br&lt;/code&gt;) waits for a ping to &lt;code&gt;eth0&lt;/code&gt; on the container before staring capturing packets with &lt;code&gt;eth1&lt;/code&gt; on the container.
After the container starts, you will see messages like below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;17:49:21.196 [main] INFO  org.pcap4j.core.Pcaps - 3 NIF(s) found.
eth0 (null)
IP address: /172.17.0.3
IP address: /fe80:0:0:0:42:acff:fe11:3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The messages say IP address of &lt;code&gt;eth0&lt;/code&gt; is &lt;code&gt;172.17.0.3&lt;/code&gt;. We will use it later.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Configure a bridge interface&lt;/p&gt;

&lt;p&gt;Open another terminal and do the following:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@localhost ~]# ip link add eth1 link eth0 type macvlan mode bridge
[root@localhost ~]# ip link set netns $(docker-pid pcap4j-br) eth1
[root@localhost ~]# nsenter -t $(docker-pid pcap4j-br) -n ip link set eth1 up
[root@localhost ~]# nsenter -t $(docker-pid pcap4j-br) -n ip route del default
[root@localhost ~]# nsenter -t $(docker-pid pcap4j-br) -n ip addr add 192.168.1.200/24 dev eth1
[root@localhost ~]# nsenter -t $(docker-pid pcap4j-br) -n ip route add default via 192.168.1.1 dev eth1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above commands
1) add an interface &lt;code&gt;eth1&lt;/code&gt; bridged to &lt;code&gt;eth0&lt;/code&gt; to the Docker host machine,
2) move the &lt;code&gt;eth1&lt;/code&gt; to the name space of &lt;code&gt;pcap4j-br&lt;/code&gt;,
3) start &lt;code&gt;eth1&lt;/code&gt;,
4) delete the default route in &lt;code&gt;pcap4j-br&lt;/code&gt;,
5) add an IP address &lt;code&gt;192.168.1.200/24&lt;/code&gt; to &lt;code&gt;eth1&lt;/code&gt;,
6) and set the default route in &lt;code&gt;pcap4j-br&lt;/code&gt; to &lt;code&gt;192.168.1.1&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Too much hassle? I agree. Let&amp;rsquo;s use an awesome tool, &lt;a href=&#34;https://github.com/jpetazzo/pipework&#34;&gt;pipework&lt;/a&gt;.
This tool accomplishes the above 6 steps in easier way as shown below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@localhost ~]# git clone https://github.com/jpetazzo/pipework.git
[root@localhost ~]# cd pipework
[root@localhost pipework]# ./pipework eth0 pcap4j-br 192.168.1.200/24@192.168.1.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;pipework uses &lt;code&gt;ip netns exec&lt;/code&gt; command instead of &lt;code&gt;nsenter&lt;/code&gt; to manipulate a container.
Incidentally, &lt;code&gt;docker exec&lt;/code&gt; didn&amp;rsquo;t work for the step 3 due to an error &amp;ldquo;&lt;code&gt;RTNETLINK answers: Operation not permitted&lt;/code&gt;&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;In addition, in my case, because I was doing it on a VMware VM, I needed to enable the promiscuous mode of &lt;code&gt;eth0&lt;/code&gt; (on the docker host machine) as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@localhost ~]# ip link set dev eth0 promisc on
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Try to poke the container&lt;/p&gt;

&lt;p&gt;You can now communicate with &lt;code&gt;pcap4j-br&lt;/code&gt; using &lt;code&gt;eth1&lt;/code&gt; &lt;strong&gt;from another host&lt;/strong&gt;.
I tried some pings from the VM&amp;rsquo;s host to &lt;code&gt;pcap4j-br&lt;/code&gt; and saw replies.&lt;/p&gt;

&lt;p&gt;Note that you can &lt;strong&gt;NOT&lt;/strong&gt; communicate with &lt;code&gt;pcap4j-br&lt;/code&gt; via &lt;code&gt;eth1&lt;/code&gt; from the docker host.
See the &lt;a href=&#34;http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/&#34;&gt;Odd Bits blog&lt;/a&gt; for the details.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Start packet capturing&lt;/p&gt;

&lt;p&gt;Ping to &lt;code&gt;eth0&lt;/code&gt; of &lt;code&gt;pcap4j-br&lt;/code&gt; form the docker host to start packet capturing.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@localhost ~]# ping -c 1 172.17.0.3
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Pcap4J container with runC</title>
          <link>https://www.kaitoy.xyz/2015/07/19/pcap4j-container-with-runc/</link>
          <pubDate>Sun, 19 Jul 2015 16:25:03 -0600</pubDate>
          <author>Kaito Yamada</author>
          <guid>https://www.kaitoy.xyz/2015/07/19/pcap4j-container-with-runc/</guid>
          <description>

&lt;p&gt;I tried to run a &lt;a href=&#34;https://registry.hub.docker.com/u/kaitoy/pcap4j/&#34;&gt;Pcap4J container&lt;/a&gt; with &lt;a href=&#34;https://runc.io/&#34;&gt;runC&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;



&lt;script async src=&#34;//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js&#34;&gt;&lt;/script&gt;
&lt;ins class=&#34;adsbygoogle&#34;
     style=&#34;display:block&#34;
     data-ad-client=&#34;ca-pub-6244473643910448&#34;
     data-ad-slot=&#34;1845600530&#34;
     data-ad-format=&#34;auto&#34;&gt;&lt;/ins&gt;
&lt;script&gt;
(adsbygoogle = window.adsbygoogle || []).push({});
&lt;/script&gt;
&lt;/p&gt;

&lt;h2 id=&#34;what-is-pcap4j&#34;&gt;What is Pcap4J?&lt;/h2&gt;

&lt;p&gt;Pcap4J is a Java library for capturing, crafting, and sending packets.
It&amp;rsquo;s actually a Java wrapper for libpcap/WinPcap plus packet analyzer.
We can see the details in its &lt;a href=&#34;https://github.com/kaitoy/pcap4j&#34;&gt;README&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;what-is-runc&#34;&gt;What is runC?&lt;/h2&gt;

&lt;p&gt;runC is a container runtime developed by Docker and released on June 22, 2015.
With runC, we can start a container from a docker image without the docker service or the docker command.&lt;/p&gt;

&lt;p&gt;That said, as of now, runC cannot directory use docker images.
We need to create a container form a docker image and export its filesystem before executing runC.&lt;/p&gt;

&lt;p&gt;It seems currently it supports only Linux but Windows support is in the roadmap.&lt;/p&gt;

&lt;h2 id=&#34;what-i-did&#34;&gt;What I did&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Environment

&lt;ul&gt;
&lt;li&gt;OS: CentOS 7 (on VMware Player 7.1.0 on Windows 7)&lt;/li&gt;
&lt;li&gt;user: root&lt;/li&gt;
&lt;li&gt;runC version: 0.2&lt;/li&gt;
&lt;li&gt;Pcap4J version: 1.5.1-SNAPSHOT&lt;/li&gt;
&lt;li&gt;Docker version: 1.6.2&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Prerequisites:

&lt;ul&gt;
&lt;li&gt;Docker is installed and Docker service is started&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://golang.org/&#34;&gt;Go&lt;/a&gt; is installed&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Step by step&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Install runC&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@localhost ~]# mkdir -p $GOPATH/src/github.com/opencontainers
[root@localhost ~]# cd $GOPATH/src/github.com/opencontainers
[root@localhost opencontainers]# git clone https://github.com/opencontainers/runc
[root@localhost opencontainers]# cd runc
[root@localhost runc]# make &amp;amp;&amp;amp; make install
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Pull the Pcap4J docker image.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@localhost ~]# docker pull kaitoy/pcap4j
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a container from the image.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@localhost ~]# docker run -d --name pcap4j-tmp kaitoy/pcap4j:latest /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Export the container&amp;rsquo;s file system.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@localhost ~]# mkdir /tmp/pcap4j-test
[root@localhost pcap4j-test]# cd /tmp/pcap4j-test
[root@localhost pcap4j-test]# docker export pcap4j-tmp &amp;gt; pcap4j.tar
[root@localhost pcap4j-test]# tar xf pcap4j.tar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We are now free from Docker. We don&amp;rsquo;t need Docker service, Docker command, Docker images, nor Docker containers anymore.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Generate a container config file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@localhost pcap4j-test]# runc spec | sed -e &#39;s/rootfs/\/root\/Desktop\/pcap4j-container/&#39; -e &#39;s/&amp;quot;readonly&amp;quot;: true/&amp;quot;readonly&amp;quot;: false/&#39; -e &#39;s/&amp;quot;NET_BIND_SERVICE&amp;quot;/&amp;quot;NET_BIND_SERVICE&amp;quot;,&amp;quot;NET_ADMIN&amp;quot;,&amp;quot;NET_RAW&amp;quot;/&#39; &amp;gt; config.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the above command, &lt;code&gt;runc spec&lt;/code&gt; generates a standard container config file and &lt;code&gt;sed&lt;/code&gt; modifies it for Pcap4J.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Run a container.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[root@localhost pcap4j-test]# runc
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In the container, enable lo.&lt;/p&gt;

&lt;p&gt;As far as I saw, lo is the only interface we can use in a container.
So, I used it to capture packets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sh-4.1# ifconfig lo up
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Generate a script to ping localhost and run it background.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sh-4.1# cd /usr/local/src/pcap4j/bin
sh-4.1# echo ping 127.0.0.1 \&amp;gt; /dev/null &amp;gt; pinger.sh
sh-4.1# chmod +x pinger.sh
sh-4.1# ./pinger.sh &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the next step, ICMP packets from this pinger.sh will be captured.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Generate a script to start capturing packets with Pcap4J and run it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sh-4.1# cat runGetNextPacket.sh | sed -e &#39;s/eth0/lo/&#39; &amp;gt; foo.sh
sh-4.1# chmod +x foo.sh
sh-4.1# ./foo.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will see the ICMP packets are dumped on the terminal. That&amp;rsquo;s it!&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
      
    

  </channel>
</rss>
